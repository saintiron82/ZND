{
  "title": "초거대 AI 시대의 도래와 그 이후",
  "summary": "2021년경 '초거대 AI'라는 용어가 기술 및 정책 담론의 중심에 있었으나, 2023년 이후 그 중요성이 감소했습니다. 이는 2021년 구글의 '스위치 트랜스포머'와 오픈AI의 'GPT-3' 발표로 촉발된 대형 모델 경쟁의 변곡점을 나타냅니다. 초거대 AI는 방대한 데이터와 컴퓨팅 인프라를 기반으로 파라미터 규모를 확장하여 범용성과 AGI 가능성을 높였으며, GPT-1부터 시작된 파라미터 확장 경쟁은 GPT-3, 스위치 트랜스포머, 우다오 2.0 등으로 이어졌습니다.",
  "published_at": "2025-12-13T12:42:46+09:00",
  "modified_at": "2025-12-13T12:42:46+09:00",
  "author": [
    "AI타임스"
  ],
  "image": "https://cdn.aitimes.com/news/photo/202512/204771_206121_3731.jpg",
  "text": "2023년 정부 주도로 설립된 '초거대 AI 추진 협의회' (사진=과기정통부) 인공지능(AI) 기술의 발전사를 돌아보면, 특정 시기를 상징하는 키워드가 존재한다. 2010년대 중반을 이끈 ‘딥러닝 혁명’, 2018년 이후 AI 패러다임을 바꾼 ‘트랜스포머 시대’가 그 예다. 그리고 코로나19로 디지털 전환이 가속되던 2021년 전후에는 ‘초거대 AI(Hyper-scale AI)’라는 용어가 기술 분야와 정책 담론의 중심에 자리했다. 정부 보고서와 산업계 발표에서도 “초거대 AI 시대의 도래”라는 표현이 앞다퉈 등장했다. 그러나 2023년 이후 이 용어는 급격히 존재감을 잃기 시작했다. 기술 패러다임 자체가 변곡점을 지난 것이다. 초거대 AI 열풍을 직접 자극한 사건은 2021년 초 구글이 발표한 ‘스위치 트랜스포머(Switch Transformer)’였다. 그 직전 해에는 오픈AI가 'GPT-3'를 공개하며 생성 AI의 대중적 파급력을 처음으로 실감하게 했다. 국내에서도 네이버, LG, KT 그리고 KAIST 등이 2021년 5월 전후 경쟁적으로 초거대 AI 모델을 발표하면서 ‘대형 모델 경쟁’이 급물살을 탔다. 초거대 AI는 방대한 데이터 학습과 고성능 컴퓨팅 인프라를 기반으로, 파라미터(parameter) 규모를 획기적으로 확대한 모델을 의미했다. 파라미터는 모델이 학습과 추론 과정에서 활용하는 핵심 변수로, 그 수가 곧 모델의 크기이자 잠재 성능을 상징했다. 파라미터가 많아질수록 학습 비용과 전력 그리고 시간은 폭증했지만, 성능과 표현 능력 역시 크게 향상될 것이라는 기대가 시장 전반을 지배했다. 다른 한편으로, 바둑만 잘 두던 알파고와 같은 기존 AI와 달리 특정 업무에 국한되지 않고 좀 더 범용적인 범위에 활용할 수 있는 초거대 AI는 인공일반지능(AGI)의 가능성을 높여 주었다. 초거대 AI로 보긴 어렵지만, 시작을 알린 것은 2018년의 'GPT-1'이었다. 이전 회에서 설명한 오픈AI의 알렉 래드포드가 공개한 트랜스포머 기반의 생성 AI인 ‘GPT(Generative Pre-Trained Transformer)’ 시리즈의 첫 모델이었다. 사실 GPT-1은 크게 주목받지는 못했지만, 1억1700만개(117M)의 방대한 파라미터로 학습했다는 부분에서 연구자들의 흥미를 끌었다. 또 모델의 크기를 키우면 성능도 좋아진다는 경험법칙을 만들어 냈고, 본격적인 모델 크기의 확장 경쟁을 불러일으켰다. 2018년 GPT 발표 이래 구글의 'BERT', 페이스북의 'RoBERTa', 엔비디아의 'MegatronLM' 등의 초거대 모델이 잇달아 공개되면서 파라미터의 수가 급격히 증가했다. 오픈AI도 2019년 파라미터 15억개(1.5B)의 'GPT-2'를 발표했는데, 2020년이 되자 마이크로소프트(MS)는 170억개(17B)의 파라미터를 사용한 언어 모델 'Turing-NLG'를 발표했다. 이에 질세라 오픈AI는 같은 해 1750억개(175B)의 파라미터를 사용한 GPT-3를 공개했고, 급기야 구글은 2021년 1월에 1.6조개(1.6T)의 파라미터로 구성된 스위치 트랜스포머를 발표했다. 중국도 경쟁의 대열에 합류했다. 지위안 인공지능연구소(北京智源人工智能研究院)는 2021년 6월, 1.75조개(1.75T)의 파라미터로 구성된 우다오 2.0(悟道 2.0)을 발표했다. 이런 일련의 경쟁은 확실히 ‘Large-Scale AI’ 시대에서 ‘Hyper-Scale AI’ 시대로 넘어왔음을 입증해 줬다. 이는 무어의 법칙을 넘어서는 연산 능력을 요구하는 초거대 AI 모델들의 등장과 경쟁이었다. 여기에서는 GPT-3, 스위치 트랜스포머와 당시 국내에서 공개된 네이버와 LG의 초거대 AI에 대해 간략히 살펴본다. 오픈AI가 2020년 6월에 공개한 GPT-3는 아마 IBM의 딥블루와 구글 딥마인드의 알파고 이후 처음으로 대중의 열렬한 관심을 받은 AI 모델인 듯하다. 모델 자체는 한해 전의 GPT-2와 크게 다르지 않았다. 그러나 100배 이상 증가한 파라미터로 학습하면서, 높은 수준의 문장을 만들어 내면서 많은 사람들을 환호하게 했다. 한때 미국의 SNS에는 GPT-3가 만든 문장으로 넘쳐나기도 했다. 문장 생성만이 아니라 챗봇을 통한 대화와 프로그래밍 코드까지 생성하자, 많은 사람은 GPT-3를 진정한 AI로 가는 기념비적 사건으로 규정하기도 했다. 스위치 트랜스포머는 구글 브레인이 2021년 1월에 발표한 AI 언어 모델이었다. 저장된 파라미터의 수로 비교하자면 GPT-3에 비해 10배 가까이 많은 모델이었지만, 실제 연산량은 훨씬 적었고 대중들의 주목도도 높지 않았다. 그것은 구글이 연구자들에게 논문과 소스코드 형태로만 공개했고, 대중들에게는 직접 실감할 기회가 주지 않았기 때문이었다. 구글은 스위치 트랜스포머의 개발 목적이 기존 대비 시간과 자원을 더욱 효율적으로 쓸 수 있는 기법을 연구해 데이터 처리 규모를 늘리려 한 것이라고 밝혔다. 네이버는 2021년 5월, GPT-3를 뛰어넘는 2040억개(204B)의 파라미터 규모로 개발된 ‘하이퍼클로바(HyperCLOVA)’를 공개했다. 이는 한국어에 최적화된 언어모델을 개발한 것이었다. 그것은 GPT-3의 학습 데이터 중 한국어가 차지하는 비중이 1%에도 미치지 못하는 것에 비해, 하이퍼클로바는 97%에 달했기 때문이다. 이런 한국어 특화 역량은 단순한 번역 성능을 넘어, 한국의 사회적 맥락과 문화를 이해하는 답변을 생성하게 했다. 그래서 국내 사용자들에게 가장 친숙하고 실용적인 AI 서비스를 가능케 했다. 같은 달, LG AI 연구원도 초거대 AI 계획을 발표하고 이후 12월에 구글의 AI 칩인 ‘TPU v4’을 활용해 3000억개(300B) 파라미터 규모로 개발된 ‘엑사원(EXAONE, EXpert Ai for everyONE)’을 공개했다. 엑사원은 IT 기업이 아닌 제조 중심 기업에서 초거대 AI를 구축한 첫 사례였다. 그래서 LG는 AI의 활용을 실질적인 인간의 삶이나 제조 현장에 서비스를 제공하는 기술로서 방향을 설정했다. 엑사원의 멀티모달 능력은 화학 구조식을 보고 그 특성을 텍스트로 설명하거나, 논문의 핵심 내용을 이미지로 시각화하는 등 연구 개발 현장의 효율성을 획기적으로 높일 수 있는 기술적 토대가 됐다. LG AI연구원이 2023년 7월19일 출시한 하이퍼스케일 AI '엑사원 2.0' (사진=LG AI연구원) 그런데, 다른 한편으로 이런 초거대 AI는 경쟁이 일으킬 수 있는 문제점에 대한 논의도 촉발했다. 대표적으로 컴퓨팅 파워에 따른 AI 개발 격차, 알고리즘 편향 및 표절에 따른 윤리적 문제, 에너지 소모에 따른 환경 문제 등이었다. 먼저, 초거대 AI의 등장으로 선도 기업과 후발 기업 간의 격차 심화가 제기됐다. 거대 자본을 가진 선도 기업은 많은 투자를 하며 기술 발전을 앞당겼지만, 후발 기업은 투자 여력과 발전 속도를 따라가기 쉽지 않았다. 특히 수천억 또는 조 단위의 파라미터의 모델을 학습하기 위해서는 막강한 컴퓨팅 파워와 인프라가 필요했다. 대략 추산해도 GPT-3나 스위치 트랜스포머의 학습 비용에만 수십억원에서 수백억원에 이르는 막대한 비용이 들었다. 그동안 딥러닝의 학습 비용이 점차 줄어 왔지만, 여전히 초거대 AI 개발에는 천문학적인 비용이 들어갔다. 이런 비용은 이미 AI 선도 기업인 구글, 페이스북, MS 등에는 투자가 어려운 규모는 아니지만, 후발 AI 기업에는 꽤 무리가 되는 비용이었다. 결국 후발 AI 기업은 소규모의 개별 AI 모델 개발에 집중할 수밖에 없는 환경이 만들어졌다. 막대한 비용이 투입된 초거대 AI의 활용은 자사와 파트너사에만 활용이 제한되는 폐쇄성을 보일 수밖에 없다. 일부 기업들은 초거대 AI 활용에 후발 기업이나 중소업체와 협력한다고 했지만, 이 역시 결국 AI 모델에 대한 종속성을 심화하는 상황이 발생할 수도 있었다. 단일 대규모 모델은 소규모 개별 AI보다 더 나은 성능을 보여줄 것이고, 선도 기업과 후발 기업 간의 격차는 점점 더 커질 수밖에 없는 상황이었다. AI 개발에서 양극화 및 독점화가 심화하는 구조가 예상됐다. 앞선 회에서 설명한 AI의 한계 중 하나인 편향성도 문제였다. 초거대 AI는 온라인의 잘못된 정보와 편견을 상당 부분 흡수해 재생산하고, 지역별, 인종별, 성별 알고리즘 편향을 보일 수도 있는 위험성이 상존했다. 특히 당시의 초거대 AI는 북미 중심 기업이 주도하는 상황이었다. 그런 관점에서 네이버, LG, KT 등의 국내 AI 업계의 발전은 큰 관심을 받게 된 배경이 됐다. 다른 한편으로 초거대 AI가 당면한 문제 중 하나로 에너지 및 자원 소모가 대두됐다. GPT-3는 학습에 엄청난 전력을 필요로 한다. 오픈AI의 보고서에 따르면, GPT-3는 GPT-2 대비 연산량의 증가가 100배 이상에 달한다고 한다. 이는 에너지 등의 자원 소모의 문제와 함께 탄소 발자국 등의 환경 문제로 귀결됐다. 탄소 발자국 문제는 앞선 회에서 이야기된 팀닛 게브루의 해고 사태를 일으킨 논문에서 첫번째로 지적한 문제이기도 했다. 이 문제들이 완전히 해결되지는 않은 가운데, 2023년에는 AI 역사에서 또 다른 패러다임 전환이 일어났다. 모델의 크기라는 지표의 중요성을 바뀐 것이다. GPT-4는 정확한 크기를 공개하지 않았지만, 70억개(7B) 파라미터의 '미스트랄(Mistral)'이나 130억개(13B)의 '라마(Llama)' 같은 상대적으로 작고 효율적인 모델이 훨씬 큰 모델과 경쟁하기 시작했다. 크기 중심의 시대에서 효율성, 데이터 품질, 학습 방법이 더 중요한 지표로 부상하게 된 결정적 사건이었다. 그렇게 효율성은 새로운 경쟁 기준이 됐다. 기업들은 파라미터 경쟁보다는 데이터 품질이나 RLHF 같은 학습 기법, 추론 비용 절감 같은 부분에 더 힘을 기울이기 시작했다. 사실 초거대 모델의 경쟁은 구축이나 운영 비용이 너무 높고 환경적 부담도 커서 지속 불가능한 모델링 형태라는 인식이 확대됐다. 그러면서 AI 산업의 중심도 대형 모델이 아닌 작고 똑똑한 모델로 이동해 갔다. sLM(Small Language Model), 온디바이스 AI, 특화 모델, 경량 모델이 새로운 표준이 제시됐다. 그러면서 AI 시대의 경쟁력은 크기에서 고품질 데이터, 효율적 학습 방식, 저비용 추론과 같은 효율성으로 바뀌었다. 사실 초거대 AI는 기술 용어라기보다 국가나 기업의 홍보 목적의 용어에 가까웠다. 그래서 2023년 이후 업계는 LLM이나 파운데이션 모델(Foundation Model) 같은 좀 더 구체적인 기술 용어를 선호하게 됐다. 초거대 AI라는 키워드는 더 이상 시대 흐름과 맞지 않았다. 구체적인 용어들이 자리를 잡으면서, 추상적이고 과장된 느낌의 용어는 자연스럽게 퇴장하게 됐다. 결과적으로 초거대 AI라는 용어는 AI 산업이 ‘양적 성장’에서 ‘질적 성장’으로 넘어갔음을 보여주는 역사적 흔적이 됐다. 문병성 싸이텍 이사 moonux@gmail.com",
  "url": "https://www.aitimes.com/news/articleView.html?idxno=204771",
  "article_id": "LEGACY_CALL",
  "cached_at": "2025-12-17T15:05:05.964707+00:00",
  "profile_version": "AI_INDUSTRY_NEWS_V6.6.3",
  "tags": [
    "LLM",
    "MODEL_RELEASE",
    "RESEARCH_PAPER"
  ],
  "raw_analysis": {
    "impact_entity": {
      "id": "TIER_Z_GENERAL",
      "value": "1.0",
      "reasoning": "제시된 텍스트에는 OpenAI, Google, Microsoft, Naver, LG 등 여러 기업의 AI 모델 출시가 언급되지만, 이들을 명시적으로 TIER 1, 2, 3으로 분류할 수 있는 정보가 부족합니다. 따라서 가장 일반적인 TIER_Z_GENERAL로 분류합니다."
    },
    "impact_events": [
      {
        "id": "MODEL_RELEASE",
        "value": "1.5",
        "reasoning": "텍스트는 GPT-3, 스위치 트랜스포머, 하이퍼클로바, 엑사원 등 다수의 초거대 AI 모델 출시를 상세히 설명하고 있습니다. 이러한 모델들은 TIER_3 또는 TIER_Z에 해당하므로 'MODEL_RELEASE' 이벤트가 적용됩니다."
      }
    ],
    "penalties": [
      {
        "id": "IRRELEVANT_ENTITY_NOISE",
        "value": "2.0",
        "reasoning": "본문에서 언급된 AI 모델들은 대부분 TIER_Z 또는 TIER_3에 해당하며, '초거대 AI 추진 협의회'와 같은 정부 주도 협의체는 TIER_Z로 간주될 수 있습니다. 따라서 'IRRELEVANT_ENTITY_NOISE' 페널티가 적용됩니다."
      }
    ],
    "credits": []
  },
  "overall_review": "초거대 AI 모델 출시와 경쟁에 대한 역사적 분석입니다. TIER_Z로 분류되어 영향력은 낮게 평가됩니다.",
  "impact_score": 2.5,
  "zero_echo_score": 7.0,
  "evidence": {},
  "impact_evidence": {
    "schema_version": "V0.9-Hybrid"
  },
  "mll_status": "analyzed",
  "analyzed_at": "2025-12-17T15:46:17.758989+00:00",
  "staged_at": "2025-12-17T15:48:44.685462+00:00",
  "staged": true
}