# -*- coding: utf-8 -*-
"""
ìë™í™” íŒŒì´í”„ë¼ì¸ API (Automation Pipeline)
- ìˆ˜ì§‘(Collect), ì¶”ì¶œ(Extract), ë¶„ì„(Analyze), ì¡°íŒ(Stage), ë°œí–‰(Publish)
"""
import os
import json
import asyncio
from datetime import datetime, timezone, timedelta
from flask import Blueprint, request, jsonify

# ê³µìœ  ëª¨ë“ˆ import
from crawler import load_targets, fetch_links
from src.db_client import DBClient
from src.crawler.core import AsyncCrawler
from src.core_logic import (
    load_from_cache as _core_load_from_cache,
    save_to_cache as _core_save_to_cache,
    normalize_field_names as _core_normalize_field_names,
)

automation_bp = Blueprint('automation', __name__)

# ê³µìœ  ì¸ìŠ¤í„´ìŠ¤
db = DBClient()
CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'cache')


def load_from_cache(url):
    """ìºì‹œì—ì„œ URL ë°ì´í„° ë¡œë“œ"""
    return _core_load_from_cache(url)


def save_to_cache(url, content):
    """ìºì‹œì— URL ë°ì´í„° ì €ì¥"""
    return _core_save_to_cache(url, content)


def normalize_field_names(data):
    """í•„ë“œëª… ì •ê·œí™”"""
    return _core_normalize_field_names(data)


@automation_bp.route('/api/automation/collect', methods=['POST'])
def automation_collect():
    """
    1ï¸âƒ£ ë§í¬ ìˆ˜ì§‘: ëª¨ë“  í™œì„± íƒ€ê²Ÿì—ì„œ ìƒˆ ë§í¬ ìˆ˜ì§‘
    - íˆìŠ¤í† ë¦¬ì— ì—†ëŠ” ë§í¬ë§Œ ë°˜í™˜
    """
    try:
        targets = load_targets()
        all_links = []
        
        for target in targets:
            links = fetch_links(target)
            limit = target.get('limit', 5)
            links = links[:limit]
            
            for link in links:
                # íˆìŠ¤í† ë¦¬ ì²´í¬ (ì´ë¯¸ ì²˜ë¦¬ëœ ê²ƒ ì œì™¸)
                if not db.check_history(link):
                    all_links.append({
                        'url': link,
                        'source_id': target['id'],
                        'target_name': target.get('name', target['id'])
                    })
        
        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique_links = []
        for item in all_links:
            if item['url'] not in seen:
                seen.add(item['url'])
                unique_links.append(item)
        
        print(f"ğŸ“¡ [Collect] ìˆ˜ì§‘ ì™„ë£Œ: {len(unique_links)} ìƒˆ ë§í¬")
        return jsonify({
            'success': True,
            'links': unique_links,
            'total': len(unique_links),
            'message': f'{len(unique_links)}ê°œ ìƒˆ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ'
        })
    except Exception as e:
        print(f"âŒ [Collect] Error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@automation_bp.route('/api/automation/extract', methods=['POST'])
def automation_extract():
    """
    2ï¸âƒ£ ì½˜í…ì¸  ì¶”ì¶œ: ìˆ˜ì§‘ëœ ë§í¬ â†’ ìºì‹œ ì €ì¥
    - ì´ë¯¸ ìºì‹œëœ ê²ƒì€ ê±´ë„ˆëœ€
    """
    try:
        data = request.json or {}
        # ë§í¬ ëª©ë¡ì´ ì—†ìœ¼ë©´ ìë™ ìˆ˜ì§‘
        links = data.get('links')
        
        if not links:
            # ìë™ìœ¼ë¡œ collect ë¨¼ì € ì‹¤í–‰
            targets = load_targets()
            links = []
            for target in targets:
                fetched = fetch_links(target)[:target.get('limit', 5)]
                for url in fetched:
                    if not db.check_history(url):
                        links.append({'url': url, 'source_id': target['id']})
        
        extracted_count = 0
        skipped_count = 0
        failed_count = 0
        
        async def extract_all():
            nonlocal extracted_count, skipped_count, failed_count
            crawler = AsyncCrawler(use_playwright=True)
            try:
                await crawler.start()
                for item in links:
                    url = item['url'] if isinstance(item, dict) else item
                    source_id = item.get('source_id', 'unknown') if isinstance(item, dict) else 'unknown'
                    
                    # ìºì‹œ ì²´í¬
                    cached = load_from_cache(url)
                    if cached and cached.get('text'):
                        skipped_count += 1
                        continue
                    
                    try:
                        content = await crawler.process_url(url)
                        if content and len(content.get('text', '')) >= 200:
                            content['source_id'] = source_id
                            save_to_cache(url, content)
                            extracted_count += 1
                        else:
                            failed_count += 1
                    except Exception as e:
                        print(f"âš ï¸ [Extract] Failed: {url[:50]}... - {e}")
                        failed_count += 1
            finally:
                await crawler.close()
        
        asyncio.run(extract_all())
        
        print(f"ğŸ“¥ [Extract] ì¶”ì¶œ: {extracted_count}, ìŠ¤í‚µ: {skipped_count}, ì‹¤íŒ¨: {failed_count}")
        return jsonify({
            'success': True,
            'extracted': extracted_count,
            'skipped': skipped_count,
            'failed': failed_count,
            'message': f'ì¶”ì¶œ {extracted_count}ê°œ ì™„ë£Œ (ìŠ¤í‚µ {skipped_count}, ì‹¤íŒ¨ {failed_count})'
        })
    except Exception as e:
        print(f"âŒ [Extract] Error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@automation_bp.route('/api/automation/analyze', methods=['POST'])
def automation_analyze():
    """
    3ï¸âƒ£ MLL ë¶„ì„: mll_statusê°€ ì—†ëŠ” ìºì‹œë§Œ ë¶„ì„
    """
    try:
        from src.mll_client import MLLClient
        from src.core_logic import get_config
        
        mll = MLLClient()
        today_str = datetime.now().strftime('%Y-%m-%d')
        cache_date_dir = os.path.join(CACHE_DIR, today_str)
        
        analyzed_count = 0
        skipped_count = 0
        failed_count = 0
        
        # ì˜¤ëŠ˜ ìºì‹œ í´ë” ìŠ¤ìº”
        if os.path.exists(cache_date_dir):
            for filename in os.listdir(cache_date_dir):
                if not filename.endswith('.json'):
                    continue
                
                filepath = os.path.join(cache_date_dir, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        cache_data = json.load(f)
                    
                    # ì´ë¯¸ ë¶„ì„ë¨
                    if cache_data.get('mll_status') or cache_data.get('raw_analysis'):
                        skipped_count += 1
                        continue
                    
                    # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                    text = cache_data.get('text', '')
                    if len(text) < 200:
                        skipped_count += 1
                        continue
                    
                    # MLL ë¶„ì„
                    max_text = get_config('crawler', 'max_text_length_for_analysis', default=3000)
                    truncated_text = text[:max_text]
                    
                    mll_result = mll.analyze_text(truncated_text)
                    
                    if mll_result:
                        # ë¶„ì„ ê²°ê³¼ ë³‘í•©
                        mll_result = normalize_field_names(mll_result)
                        cache_data.update(mll_result)
                        cache_data['mll_status'] = 'analyzed'
                        cache_data['analyzed_at'] = datetime.now(timezone.utc).isoformat()
                        
                        with open(filepath, 'w', encoding='utf-8') as f:
                            json.dump(cache_data, f, ensure_ascii=False, indent=2)
                        
                        analyzed_count += 1
                    else:
                        cache_data['mll_status'] = 'failed'
                        with open(filepath, 'w', encoding='utf-8') as f:
                            json.dump(cache_data, f, ensure_ascii=False, indent=2)
                        failed_count += 1
                        
                except Exception as e:
                    print(f"âš ï¸ [Analyze] Error on {filename}: {e}")
                    failed_count += 1
        
        print(f"ğŸ¤– [Analyze] ë¶„ì„: {analyzed_count}, ìŠ¤í‚µ: {skipped_count}, ì‹¤íŒ¨: {failed_count}")
        return jsonify({
            'success': True,
            'analyzed': analyzed_count,
            'skipped': skipped_count,
            'failed': failed_count,
            'message': f'MLL ë¶„ì„ {analyzed_count}ê°œ ì™„ë£Œ'
        })
    except Exception as e:
        print(f"âŒ [Analyze] Error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@automation_bp.route('/api/automation/stage', methods=['POST'])
def automation_stage():
    """
    4ï¸âƒ£ ì¡°íŒ (Staging): ë¶„ì„ ì™„ë£Œëœ ìºì‹œ ì ìˆ˜ ì¬ê²€ì¦ ë° ê³ ë…¸ì´ì¦ˆ í•„í„°ë§
    - ì´ì œ cacheê°€ ì¡°íŒ ì—­í• ì„ ë™ì‹œì— ìˆ˜í–‰ (ë³„ë„ staging í´ë” ì—†ìŒ)
    - ì ìˆ˜ ì¬ê²€ì¦ + ê³ ë…¸ì´ì¦ˆ ìë™ ê±°ë¶€ ì²˜ë¦¬
    """
    try:
        from src.score_engine import process_raw_analysis
        
        staged_count = 0
        skipped_count = 0
        
        # ìµœê·¼ 3ì¼ì¹˜ ìºì‹œ ìŠ¤ìº”
        for i in range(3):
            scan_date = (datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d')
            cache_date_dir = os.path.join(CACHE_DIR, scan_date)
            
            if not os.path.exists(cache_date_dir):
                continue

            print(f"ğŸ•µï¸ [Stage] Scanning cache folder: {scan_date}")

            for filename in os.listdir(cache_date_dir):
                if not filename.endswith('.json'):
                    continue
                
                filepath = os.path.join(cache_date_dir, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        cache_data = json.load(f)
                    
                    # ë¶„ì„ ì•ˆ ëœ ê²ƒì€ ìŠ¤í‚µ
                    is_analyzed = (
                        cache_data.get('mll_status') == 'analyzed' or
                        cache_data.get('raw_analysis') is not None or
                        cache_data.get('zero_echo_score') is not None
                    )
                    if not is_analyzed:
                        skipped_count += 1
                        continue
                    
                    # ì´ë¯¸ processed (staged) ì²˜ë¦¬ëœ ê²ƒì€ ìŠ¤í‚µ
                    if cache_data.get('staged'):
                        skipped_count += 1
                        continue
                    
                    # ì´ë¯¸ ë°œí–‰ëœ ê²ƒì€ ìŠ¤í‚µ
                    if cache_data.get('published'):
                        skipped_count += 1
                        continue

                    # ì ìˆ˜ ì¬ê²€ì¦ (raw_analysis ìˆìœ¼ë©´)
                    if cache_data.get('raw_analysis'):
                        try:
                            scores = process_raw_analysis(cache_data['raw_analysis'])
                            cache_data['zero_echo_score'] = scores.get('zero_echo_score', 5.0)
                            cache_data['impact_score'] = scores.get('impact_score', 0.0)
                        except Exception as e:
                            print(f"âš ï¸ [Stage] Score calc error: {e}")
                    
                    # staged í‘œì‹œ ë° ì €ì¥
                    cache_data['staged'] = True
                    cache_data['staged_at'] = datetime.now(timezone.utc).isoformat()
                    
                    with open(filepath, 'w', encoding='utf-8') as f:
                        json.dump(cache_data, f, ensure_ascii=False, indent=2)
                    
                    staged_count += 1
                    
                except Exception as e:
                    print(f"âš ï¸ [Stage] Error on {filename}: {e}")
        
        print(f"ğŸ“‹ [Stage] ì¡°íŒ: {staged_count}, ìŠ¤í‚µ: {skipped_count}")
        return jsonify({
            'success': True,
            'staged': staged_count,
            'skipped': skipped_count,
            'message': f'ì¡°íŒ {staged_count}ê°œ ì™„ë£Œ (ìŠ¤í‚µ {skipped_count}ê°œ)'
        })
    except Exception as e:
        print(f"âŒ [Stage] Error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@automation_bp.route('/api/automation/publish', methods=['POST'])
def automation_publish():
    """
    5ï¸âƒ£ ë°œí–‰: cache â†’ data í´ë” íŒŒì¼ ìƒì„±
    - rejected ì•„ë‹Œ ê²ƒë§Œ ë°œí–‰
    - ì´ ì‹œì ì— data/ í´ë”ì— ìµœì¢… íŒŒì¼ì´ ìƒì„±ë¨
    """
    try:
        from src.pipeline import save_article
        
        today_str = datetime.now().strftime('%Y-%m-%d')
        cache_date_dir = os.path.join(CACHE_DIR, today_str)
        
        published_count = 0
        skipped_count = 0
        failed_count = 0
        
        if os.path.exists(cache_date_dir):
            for filename in os.listdir(cache_date_dir):
                if not filename.endswith('.json'):
                    continue
                
                filepath = os.path.join(cache_date_dir, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        staging_data = json.load(f)
                    
                    # ì´ë¯¸ ë°œí–‰ë¨
                    if staging_data.get('published'):
                        skipped_count += 1
                        continue
                    
                    # rejectedëŠ” ìŠ¤í‚µ
                    if staging_data.get('rejected'):
                        skipped_count += 1
                        continue
                    
                    # í•„ìˆ˜ í•„ë“œ ì²´í¬
                    required = ['url', 'summary', 'zero_echo_score', 'impact_score']
                    missing = [f for f in required if f not in staging_data]
                    
                    # title í•„ë“œ ê²€ì¦
                    has_title = staging_data.get('title_ko') or staging_data.get('title')
                    if not has_title:
                        missing.append('title_ko or title')
                    
                    if missing:
                        print(f"âš ï¸ [Publish] Missing fields {missing}: {filename}")
                        skipped_count += 1
                        continue
                    
                    # ë°œí–‰ (ë…¸ì´ì¦ˆ í•„í„°ë§ ê±´ë„ˆëœ€)
                    result = save_article(staging_data, source_id=staging_data.get('source_id'), skip_evaluation=True)
                    
                    if result.get('status') == 'saved':
                        # ë°œí–‰ ì™„ë£Œ í‘œì‹œ
                        staging_data['published'] = True
                        staging_data['published_at'] = datetime.now(timezone.utc).isoformat()
                        staging_data['data_file'] = result.get('filename')
                        
                        with open(filepath, 'w', encoding='utf-8') as f:
                            json.dump(staging_data, f, ensure_ascii=False, indent=2)
                        
                        published_count += 1
                    else:
                        failed_count += 1
                        
                except Exception as e:
                    print(f"âš ï¸ [Publish] Error on {filename}: {e}")
                    failed_count += 1
        
        print(f"ğŸš€ [Publish] ë°œí–‰: {published_count}, ìŠ¤í‚µ: {skipped_count}, ì‹¤íŒ¨: {failed_count}")
        return jsonify({
            'success': True,
            'published': published_count,
            'skipped': skipped_count,
            'failed': failed_count,
            'message': f'ë°œí–‰ {published_count}ê°œ ì™„ë£Œ'
        })
    except Exception as e:
        print(f"âŒ [Publish] Error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@automation_bp.route('/api/automation/all', methods=['POST'])
def automation_all():
    """
    âš¡ ALL: 1~4ë‹¨ê³„ ì—°ì† ì‹¤í–‰ (ë°œí–‰ ì œì™¸)
    """
    try:
        from flask import current_app
        
        results = {}
        
        # 1. ìˆ˜ì§‘
        with current_app.test_client() as client:
            resp = client.post('/api/automation/collect')
            results['collect'] = resp.get_json()
        
        # 2. ì¶”ì¶œ
        with current_app.test_client() as client:
            resp = client.post('/api/automation/extract', 
                              json={'links': results['collect'].get('links', [])})
            results['extract'] = resp.get_json()
        
        # 3. ë¶„ì„
        with current_app.test_client() as client:
            resp = client.post('/api/automation/analyze')
            results['analyze'] = resp.get_json()
        
        # 4. ì¡°íŒ
        with current_app.test_client() as client:
            resp = client.post('/api/automation/stage')
            results['stage'] = resp.get_json()
        
        print(f"âš¡ [ALL] íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
        return jsonify({
            'success': True,
            'results': results,
            'message': '1~4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (ë°œí–‰ ëŒ€ê¸°ì¤‘)'
        })
    except Exception as e:
        print(f"âŒ [ALL] Error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@automation_bp.route('/api/desk/recalculate', methods=['POST'])
def automation_stage_recalc():
    """
    âš¡ Cache í´ë”ì˜ ê¸°ì‚¬ ì ìˆ˜ ì¬ê³„ì‚° (ì „ì²´ ë˜ëŠ” ì„ íƒ)
    """
    try:
        from src.score_engine import process_raw_analysis
        
        data = request.json or {}
        date_str = data.get('date') or request.args.get('date', datetime.now().strftime('%Y-%m-%d'))
        target_filenames = data.get('filenames', [])
        schema_version_override = data.get('schema_version')

        cache_date_dir = os.path.join(CACHE_DIR, date_str)
        
        if not os.path.exists(cache_date_dir):
            return jsonify({'success': False, 'error': 'Cache folder not found'}), 404
            
        count = 0
        errors = 0
        
        # íŒŒì¼ ëª©ë¡ ê²°ì •
        if target_filenames:
            files_to_process = target_filenames
        else:
            files_to_process = [f for f in os.listdir(cache_date_dir) if f.endswith('.json')]
            
        for filename in files_to_process:
            filepath = os.path.join(cache_date_dir, filename)
            
            if not os.path.exists(filepath):
                 continue

            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    article_data = json.load(f)
                
                # raw_analysisê°€ ìˆì–´ì•¼ë§Œ ì¬ê³„ì‚° ê°€ëŠ¥
                if 'raw_analysis' in article_data and article_data['raw_analysis']:
                    scores = process_raw_analysis(article_data['raw_analysis'], force_schema_version=schema_version_override)
                    article_data['zero_echo_score'] = scores.get('zero_echo_score', 5.0)
                    article_data['impact_score'] = scores.get('impact_score', 0.0)
                    
                    # ê³„ì‚°ì— ì‚¬ìš©ëœ ìŠ¤í‚¤ë§ˆ ë²„ì „ ê¸°ë¡
                    if 'impact_evidence' not in article_data: 
                        article_data['impact_evidence'] = {}
                    if scores.get('schema_version'):
                        article_data['impact_evidence']['schema_version'] = scores['schema_version']
                    
                    with open(filepath, 'w', encoding='utf-8') as f:
                        json.dump(article_data, f, ensure_ascii=False, indent=2)
                    count += 1
            except Exception as e:
                print(f"âš ï¸ Recalc error {filename}: {e}")
                errors += 1
                
        return jsonify({
            'success': True, 
            'message': f"{count}ê°œ ê¸°ì‚¬ ì ìˆ˜ ì¬ê³„ì‚° ì™„ë£Œ (ì‹¤íŒ¨ {errors}ê±´)"
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500
