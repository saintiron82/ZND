# -*- coding: utf-8 -*-
"""
ZND ìë™ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬
- ìˆ˜ì§‘ â†’ ì¶”ì¶œ â†’ ìºì‹œ ì €ì¥ (MLL ë¶„ì„ ìŠ¤í‚µ)
- index.json ìë™ ê°±ì‹ 
- ìŠ¤ì¼€ì¤„: 06:30, 12:30, 18:30, 00:30 (í•˜ë£¨ 4íšŒ)
"""
import os
import sys
import asyncio
from datetime import datetime, timezone, timedelta
from dotenv import load_dotenv

# Load environment
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
load_dotenv(os.path.join(BASE_DIR, '.env'))

# Import core modules
from crawler import load_targets, fetch_links
from src.db_client import DBClient
from src.crawler.core import AsyncCrawler
from src.core_logic import (
    load_from_cache,
    save_to_cache,
    update_manifest,
)

# Discord ì•Œë¦¼ ëª¨ë“ˆ
import sys
sys.path.insert(0, os.path.join(BASE_DIR, '..', 'crawler'))
try:
    from core.discord_notifier import send_crawl_notification
    DISCORD_ENABLED = True
except ImportError:
    DISCORD_ENABLED = False
    print("âš ï¸ Discord notifier not available")


def serialize_datetime(obj):
    """datetime ê°ì²´ë¥¼ ISO ë¬¸ìì—´ë¡œ ë³€í™˜"""
    if isinstance(obj, datetime):
        return obj.isoformat()
    return obj


def sanitize_content(content: dict) -> dict:
    """datetime ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
    sanitized = {}
    for key, value in content.items():
        if isinstance(value, datetime):
            sanitized[key] = value.isoformat()
        elif isinstance(value, dict):
            sanitized[key] = sanitize_content(value)
        elif isinstance(value, list):
            sanitized[key] = [
                v.isoformat() if isinstance(v, datetime) else v
                for v in value
            ]
        else:
            sanitized[key] = value
    return sanitized


def log(msg: str):
    """íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ ë¡œê·¸ ì¶œë ¥"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"[{timestamp}] {msg}")


async def run_auto_crawl():
    """
    ìë™ í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜
    1. ëª¨ë“  íƒ€ê²Ÿì—ì„œ ìƒˆ ë§í¬ ìˆ˜ì§‘
    2. ì½˜í…ì¸  ì¶”ì¶œ ë° ìºì‹œ ì €ì¥
    3. index.json ê°±ì‹ 
    """
    log("ğŸš€ ìë™ í¬ë¡¤ë§ ì‹œì‘")
    
    db = DBClient()
    targets = load_targets()
    
    collected_count = 0
    extracted_count = 0
    skipped_count = 0
    failed_count = 0
    
    # 1. ë§í¬ ìˆ˜ì§‘
    log("ğŸ“¡ [1ë‹¨ê³„] ë§í¬ ìˆ˜ì§‘ ì¤‘...")
    all_links = []
    
    for target in targets:
        links = fetch_links(target)
        limit = target.get('limit', 5)
        links = links[:limit]
        
        for link in links:
            # íˆìŠ¤í† ë¦¬ ì²´í¬ (ì´ë¯¸ ì²˜ë¦¬ëœ ê²ƒ ì œì™¸)
            if not db.check_history(link):
                all_links.append({
                    'url': link,
                    'source_id': target['id']
                })
    
    # ì¤‘ë³µ ì œê±°
    seen = set()
    unique_links = []
    for item in all_links:
        if item['url'] not in seen:
            seen.add(item['url'])
            unique_links.append(item)
    
    collected_count = len(unique_links)
    log(f"ğŸ“¡ ìˆ˜ì§‘ ì™„ë£Œ: {collected_count}ê°œ ìƒˆ ë§í¬")
    
    if collected_count == 0:
        log("âœ¨ ìƒˆë¡œìš´ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤")
        return
    
    # 2. ì½˜í…ì¸  ì¶”ì¶œ
    log("ğŸ“¥ [2ë‹¨ê³„] ì½˜í…ì¸  ì¶”ì¶œ ì¤‘...")
    
    crawler = AsyncCrawler(use_playwright=True)
    try:
        await crawler.start()
        
        for item in unique_links:
            url = item['url']
            source_id = item['source_id']
            
            # ìºì‹œ ì²´í¬
            cached = load_from_cache(url)
            if cached and cached.get('text'):
                skipped_count += 1
                continue
            
            try:
                content = await crawler.process_url(url)
                if content and len(content.get('text', '')) >= 200:
                    content['source_id'] = source_id
                    content['status'] = 'RAW'  # [MODIFIED] ëª…ì‹œì  ìƒíƒœ: ì›ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ
                    # datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                    content = sanitize_content(content)
                    save_to_cache(url, content)
                    extracted_count += 1
                    log(f"  âœ… ì¶”ì¶œ: {url[:50]}...")
                else:
                    failed_count += 1
                    log(f"  âš ï¸ ì½˜í…ì¸  ë¶€ì¡±: {url[:50]}...")
            except Exception as e:
                failed_count += 1
                log(f"  âŒ ì‹¤íŒ¨: {url[:50]}... - {e}")
                
    finally:
        await crawler.close()
    
    log(f"ğŸ“¥ ì¶”ì¶œ ì™„ë£Œ: ì„±ê³µ {extracted_count}, ìŠ¤í‚µ {skipped_count}, ì‹¤íŒ¨ {failed_count}")
    
    # 3. index.json ê°±ì‹ 
    log("ğŸ“‹ [3ë‹¨ê³„] index.json ê°±ì‹  ì¤‘...")
    today_str = datetime.now().strftime('%Y-%m-%d')
    update_manifest(today_str)
    log(f"ğŸ“‹ index.json ê°±ì‹  ì™„ë£Œ: {today_str}")
    
    # ì™„ë£Œ ìš”ì•½
    log("=" * 50)
    log(f"ğŸ‰ ìë™ í¬ë¡¤ë§ ì™„ë£Œ!")
    log(f"   - ìˆ˜ì§‘: {collected_count}ê°œ")
    log(f"   - ì¶”ì¶œ: {extracted_count}ê°œ")
    log(f"   - ìŠ¤í‚µ: {skipped_count}ê°œ")
    log(f"   - ì‹¤íŒ¨: {failed_count}ê°œ")
    log("=" * 50)
    
    # 4. Discord ì•Œë¦¼ ì „ì†¡
    if DISCORD_ENABLED:
        log("ğŸ“¨ [4ë‹¨ê³„] Discord ì•Œë¦¼ ì „ì†¡ ì¤‘...")
        result = {
            'success': failed_count == 0 or extracted_count > 0,
            'collected': collected_count,
            'extracted': extracted_count,
            'analyzed': 0,  # ìë™ í¬ë¡¤ë§ì€ MLL ë¶„ì„ ìŠ¤í‚µ
            'cached': extracted_count,
            'failed': failed_count,
            'message': f'ìŠ¤í‚µ: {skipped_count}ê°œ (ì´ë¯¸ ìºì‹œë¨)'
        }
        send_crawl_notification(result, "ìë™ í¬ë¡¤ë§")
        log("ğŸ“¨ Discord ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")


def main():
    """ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸"""
    try:
        asyncio.run(run_auto_crawl())
    except KeyboardInterrupt:
        log("âš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        log(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
