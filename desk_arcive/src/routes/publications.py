# -*- coding: utf-8 -*-
"""
Publications API - ë°œí–‰ íšŒì°¨ ê´€ë¦¬, ë¦´ë¦¬ì¦ˆ, ë°œí–‰ ì·¨ì†Œ
"""
import os
import json
import re
from datetime import datetime, timezone
from flask import Blueprint, request, jsonify

from src.core_logic import (
    update_manifest as _core_update_manifest,
    normalize_field_names as _core_normalize_field_names,
    get_article_id
)
from src.db_client import DBClient

publications_bp = Blueprint('publications', __name__)

CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'cache')
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'data')
db = DBClient()


def update_manifest(date_str):
    return _core_update_manifest(date_str)


@publications_bp.route('/api/publications/check')
def publications_check():
    """
    ğŸš€ ìºì‹± ì²´í¬ API - ë³€ê²½ ì—¬ë¶€ë§Œ ë¹ ë¥´ê²Œ í™•ì¸
    Query params: since (ISO format timestamp)
    """
    try:
        from src.pipeline import get_db
        db = get_db()
        
        since = request.args.get('since')
        status_filter = request.args.get('status', 'released')
        
        # ê°€ì¥ ìµœì‹  ë°œí–‰ë³¸ 1ê°œë§Œ ì¡°íšŒ
        issues = db.get_issues_by_date()
        if status_filter:
            issues = [i for i in issues if i.get('status') == status_filter]
        
        if not issues:
            return jsonify({
                'success': True,
                'changed': False,
                'latest_updated_at': None
            })
        
        latest = issues[0]
        latest_updated = latest.get('updated_at') or latest.get('released_at') or latest.get('published_at')
        
        # since íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ë¹„êµ
        if since and latest_updated:
            if latest_updated <= since:
                return jsonify({
                    'success': True,
                    'changed': False,
                    'latest_updated_at': latest_updated
                })
        
        return jsonify({
            'success': True,
            'changed': True,
            'latest_updated_at': latest_updated,
            'latest_issue_id': latest.get('id'),
            'latest_edition_name': latest.get('edition_name')
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@publications_bp.route('/api/publications/list')
def publications_list():
    """ëŒí–‰ íšŒì°¨ ëª©ë¡ ë°˜í™˜ (_meta ë¬¸ì„œì—ì„œ 1 READë¡œ ìµœì í™”)"""
    try:
        from src.pipeline import get_db
        db = get_db()
        
        status_filter = request.args.get('status')
        
        # [OPTIMIZED] _meta ë¬¸ì„œì—ì„œ íšŒì°¨ ëª©ë¡ ì¡°íšŒ (1 READ)
        issues = db.get_issues_from_meta(status_filter=status_filter)
        
        # ìµœì‹  updated_at ë°˜í™˜ (ìºì‹± ë¹„êµìš©)
        latest_updated = None
        if issues:
            latest_updated = issues[0].get('updated_at')
        
        return jsonify({
            'success': True,
            'issues': issues,
            'latest_updated_at': latest_updated
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@publications_bp.route('/api/publications/release', methods=['POST'])
def publications_release():
    """Preview ìƒíƒœì˜ íšŒì°¨ë¥¼ Releasedë¡œ ë³€ê²½ (2ë‹¨ê³„ ë°œí–‰)"""
    try:
        from src.pipeline import get_db
        db = get_db()
        
        data = request.json or {}
        publish_id = data.get('publish_id')
        
        if not publish_id:
            return jsonify({'success': False, 'error': 'publish_id required'}), 400
        
        record = db.get_publication(publish_id)
        if not record:
            return jsonify({'success': False, 'error': 'Publication not found'}), 404
        
        update_data = {
            'status': 'released',
            'released_at': datetime.now(timezone.utc).isoformat()
        }
        
        success = db.update_publication_record(publish_id, update_data)
        
        if success:
            print(f"ğŸ‰ [Release] {record.get('edition_name')} â†’ Released")
            return jsonify({
                'success': True,
                'publish_id': publish_id,
                'edition_name': record.get('edition_name'),
                'message': f"{record.get('edition_name')} ë¦´ë¦¬ì¦ˆ ì™„ë£Œ"
            })
        else:
            return jsonify({'success': False, 'error': 'Update failed'}), 500
            
    except Exception as e:
        print(f"âŒ [Release] Error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@publications_bp.route('/api/publications/view')
def publications_view():
    """íŠ¹ì • ë°œí–‰ íšŒì°¨ì˜ ê¸°ì‚¬ ëª©ë¡ ë°˜í™˜ (ë‚´ì¥ articles ì‚¬ìš©ìœ¼ë¡œ 1 READ ìµœì í™”)"""
    try:
        from src.pipeline import get_db
        db = get_db()
        
        publish_id = request.args.get('publish_id')
        if not publish_id:
            return jsonify({'success': False, 'error': 'publish_id required'}), 400
            
        record = db.get_publication(publish_id)
        if not record:
            return jsonify({'success': False, 'error': 'Publication not found'}), 404
        
        # [OPTIMIZED] ë‚´ì¥ articles ë°°ì—´ ì‚¬ìš© (1 READ, ì¶”ê°€ ì¿¼ë¦¬ ì—†ìŒ)
        full_articles = record.get('articles', [])
        
        # Fallback: articles ë°°ì—´ì´ ë¹„ì–´ìˆìœ¼ë©´ article_idsë¡œ ê°œë³„ ì¡°íšŒ (í•˜ìœ„ í˜¸í™˜)
        if not full_articles:
            article_ids = record.get('article_ids', [])
            if article_ids:
                print(f"âš ï¸ [View] Fallback: Loading {len(article_ids)} articles individually")
                for aid in article_ids:
                    article = db.get_article(aid)
                    if article:
                        full_articles.append(article)

        return jsonify({
            'success': True,
            'publication': record,
            'articles': full_articles
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@publications_bp.route('/api/publications/remove_articles', methods=['POST'])
def publications_remove_articles():
    """
    ë°œí–‰ëœ íšŒì°¨ì—ì„œ ì„ íƒëœ ê¸°ì‚¬ë¥¼ ì œê±°í•˜ì—¬ ë¯¸ë°œí–‰ ìƒíƒœë¡œ ë³€ê²½
    ìš”ì²­: { publish_id: str, article_ids: list, filenames: list (optional) }
    """
    try:
        from src.pipeline import get_db
        db = get_db()
        
        data = request.json or {}
        publish_id = data.get('publish_id')
        article_ids = data.get('article_ids', [])
        filenames = data.get('filenames', [])
        
        if not publish_id:
            return jsonify({'success': False, 'error': 'publish_id required'}), 400
        
        if not article_ids and not filenames:
            return jsonify({'success': False, 'error': 'article_ids or filenames required'}), 400
        
        # 1. ë°œí–‰ íšŒì°¨ ì¡°íšŒ
        pub_record = db.get_publication(publish_id)
        if not pub_record:
            return jsonify({'success': False, 'error': 'Publication not found'}), 404
        
        removed_count = 0
        failed_count = 0
        
        # 2. ê° ê¸°ì‚¬ì—ì„œ publish_id ì œê±° (Firestore)
        for article_id in article_ids:
            try:
                # Firestoreì—ì„œ ê¸°ì‚¬ ì¡°íšŒ
                article_doc = db.get_article(article_id)
                if article_doc and article_doc.get('publish_id') == publish_id:
                    # publish_id í•„ë“œ ì œê±° (ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •)
                    db.update_article(article_id, {
                        'publish_id': '',
                        'edition_code': '',
                        'edition_name': ''
                    })
                    removed_count += 1
                    print(f"ğŸ”™ [Remove] Article {article_id} removed from issue {publish_id}")
            except Exception as e:
                print(f"âš ï¸ [Remove] Failed to update article {article_id}: {e}")
                failed_count += 1
        
        # 3. ìºì‹œ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸ (ë¡œì»¬)
        for filename in filenames:
            try:
                cache_filepath = None
                
                # ëª¨ë“  ë‚ ì§œ í´ë”ì—ì„œ íŒŒì¼ ì°¾ê¸°
                for date_folder in os.listdir(CACHE_DIR):
                    check_path = os.path.join(CACHE_DIR, date_folder, filename)
                    if os.path.exists(check_path):
                        cache_filepath = check_path
                        break
                
                if cache_filepath:
                    with open(cache_filepath, 'r', encoding='utf-8') as f:
                        cache_data = json.load(f)
                    
                    # ë°œí–‰ ìƒíƒœ ì œê±°
                    if cache_data.get('publish_id') == publish_id:
                        cache_data.pop('published', None)
                        cache_data.pop('publish_id', None)
                        cache_data.pop('published_at', None)
                        cache_data.pop('edition_code', None)
                        cache_data.pop('edition_name', None)
                        
                        with open(cache_filepath, 'w', encoding='utf-8') as f:
                            json.dump(cache_data, f, ensure_ascii=False, indent=2)
                        
                        print(f"ğŸ”™ [Remove] Cache updated: {filename}")
                        if filename not in [a for a in article_ids]:
                            removed_count += 1
            except Exception as e:
                print(f"âš ï¸ [Remove] Cache update failed for {filename}: {e}")
                failed_count += 1
        
        # 4. ë°œí–‰ íšŒì°¨ì˜ article_ids ë°°ì—´ ì—…ë°ì´íŠ¸
        current_ids = pub_record.get('article_ids', [])
        updated_ids = [aid for aid in current_ids if aid not in article_ids]
        
        db.update_publication_record(publish_id, {
            'article_ids': updated_ids,
            'article_count': len(updated_ids),
            'updated_at': datetime.now(timezone.utc).isoformat()
        })
        
        return jsonify({
            'success': True,
            'removed': removed_count,
            'failed': failed_count,
            'remaining_count': len(updated_ids),
            'message': f'{removed_count}ê°œ ê¸°ì‚¬ê°€ íšŒì°¨ì—ì„œ ì œê±°ë˜ì–´ ë¯¸ë°œí–‰ ìƒíƒœë¡œ ë³€ê²½ë¨'
        })
        
    except Exception as e:
        print(f"âŒ [Remove Articles] Error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500



@publications_bp.route('/api/desk/delete_from_db', methods=['POST'])
def publications_delete_from_db():
    """ğŸ”¥ Firestore DBì—ì„œ ì„ íƒëœ ê¸°ì‚¬ ì‚­ì œ (ë¡œì»¬ íŒŒì¼ì€ ìœ ì§€)"""
    try:
        from src.pipeline import get_db
        db = get_db()
        
        data = request.json or {}
        articles = data.get('articles', [])
        
        if not articles:
            return jsonify({'success': False, 'error': 'ì‚­ì œí•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.'}), 400
        
        deleted_count = 0
        failed_count = 0
        
        for article in articles:
            url = article.get('url', '')
            
            try:
                if url:
                    doc_id = get_article_id(url)
                    doc_ref = db.db.collection('articles').document(doc_id)
                    doc = doc_ref.get()
                    
                    if doc.exists:
                        doc_ref.delete()
                        deleted_count += 1
                    else:
                        failed_count += 1
                else:
                    failed_count += 1
                    
            except Exception as e:
                print(f"âš ï¸ [DB Delete] Error: {e}")
                failed_count += 1
        
        return jsonify({
            'success': True,
            'deleted': deleted_count,
            'failed': failed_count,
            'message': f'{deleted_count}ê°œ ê¸°ì‚¬ DBì—ì„œ ì‚­ì œ ì™„ë£Œ'
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@publications_bp.route('/api/desk/unpublish_selected', methods=['POST'])
def publications_unpublish_selected():
    """
    ğŸ”„ ë°œí–‰ ì·¨ì†Œ: ë°ì´í„° íŒŒì¼ ì‚­ì œ + ìºì‹œ ìƒíƒœ ë¦¬ì…‹
    """
    try:
        data = request.json or {}
        filenames = data.get('filenames', [])
        delete_firestore = data.get('delete_firestore', False)
        
        if not filenames:
            return jsonify({'success': False, 'error': 'ì„ íƒëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.'}), 400
        
        unpublished_count = 0
        failed_count = 0
        
        for filename in filenames:
            try:
                cache_filepath = None
                
                for date_folder in os.listdir(CACHE_DIR):
                    check_path = os.path.join(CACHE_DIR, date_folder, filename)
                    if os.path.exists(check_path):
                        cache_filepath = check_path
                        break
                
                if not cache_filepath:
                    failed_count += 1
                    continue
                
                with open(cache_filepath, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                
                if not cache_data.get('published'):
                    continue
                
                # 1. ë°ì´í„° íŒŒì¼ ì‚­ì œ
                data_file = cache_data.get('data_file')
                if data_file:
                    for date_folder in os.listdir(DATA_DIR):
                        data_path = os.path.join(DATA_DIR, date_folder, data_file)
                        if os.path.exists(data_path):
                            os.remove(data_path)
                            update_manifest(date_folder)
                            break
                
                # 2. Firestore ì‚­ì œ (ì„ íƒì )
                if delete_firestore and cache_data.get('url'):
                    try:
                        doc = db.get_article_by_url(cache_data['url'])
                        if doc and doc.get('id'):
                            db.delete_article(doc['id'])
                    except Exception as fs_err:
                        print(f"âš ï¸ [Unpublish] Firestore delete failed: {fs_err}")
                
                # 3. ìºì‹œ íŒŒì¼ ìƒíƒœ ë¦¬ì…‹
                cache_data.pop('published', None)
                cache_data.pop('data_file', None)
                cache_data.pop('published_at', None)
                
                with open(cache_filepath, 'w', encoding='utf-8') as f:
                    json.dump(cache_data, f, ensure_ascii=False, indent=2)
                
                # 4. History ë¦¬ì…‹
                if cache_data.get('url'):
                    db.remove_from_history(cache_data['url'])
                
                unpublished_count += 1
                
            except Exception as e:
                print(f"âš ï¸ [Unpublish] Error on {filename}: {e}")
                failed_count += 1
        
        return jsonify({
            'success': True,
            'unpublished': unpublished_count,
            'failed': failed_count,
            'message': f'{unpublished_count}ê°œ ê¸°ì‚¬ ë°œí–‰ ì·¨ì†Œ ì™„ë£Œ'
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@publications_bp.route('/api/publications/migrate_edition_names', methods=['POST'])
def publications_migrate_edition_names():
    """
    ë°œí–‰ ì‹œê°„ ìˆœì„œëŒ€ë¡œ í˜¸ìˆ˜ ì¬ì •ë ¬
    """
    try:
        # ëª¨ë“  publications ê°€ì ¸ì˜¤ê¸°
        docs = list(db.db.collection('publications').stream())
        
        # published_at ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        issues = []
        for doc in docs:
            data = doc.to_dict()
            data['id'] = doc.id
            issues.append(data)
        
        issues.sort(key=lambda x: x.get('published_at', ''))
        
        # ìˆœì„œëŒ€ë¡œ í˜¸ìˆ˜ í• ë‹¹
        updated = 0
        for idx, issue in enumerate(issues, 1):
            new_name = f"{idx}í˜¸"
            if issue.get('edition_name') != new_name:
                db.db.collection('publications').document(issue['id']).update({'edition_name': new_name})
                print(f"âœ… Updated: {issue.get('edition_name')} -> {new_name}")
                updated += 1
        
        return jsonify({
            'success': True,
            'updated': updated,
            'total': len(issues),
            'message': f'í˜¸ìˆ˜ ì¬ì •ë ¬ ì™„ë£Œ: {updated}ê°œ ì—…ë°ì´íŠ¸ (ì´ {len(issues)}ê°œ)'
        })
    except Exception as e:
        print(f"âŒ [Migration] Error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@publications_bp.route('/api/publications/delete', methods=['POST'])
def publications_delete():
    """
    ğŸ—‘ï¸ íšŒì°¨ ì „ì²´ ì‚­ì œ
    - íšŒì°¨ ë¬¸ì„œ ì‚­ì œ
    - í•´ë‹¹ íšŒì°¨ì˜ ê¸°ì‚¬ë“¤ ë°œí–‰ ì •ë³´ ì´ˆê¸°í™”
    """
    try:
        data = request.json or {}
        publish_id = data.get('publish_id')
        
        if not publish_id:
            return jsonify({'success': False, 'error': 'publish_id í•„ìˆ˜'}), 400
        
        # 1. íšŒì°¨ ì •ë³´ ì¡°íšŒ
        pub_record = db.get_publication(publish_id)
        if not pub_record:
            return jsonify({'success': False, 'error': 'íšŒì°¨ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}), 404
        
        edition_name = pub_record.get('edition_name', publish_id)
        article_ids = pub_record.get('article_ids', [])
        
        # 2. í•´ë‹¹ íšŒì°¨ì˜ ê¸°ì‚¬ë“¤ ë°œí–‰ ì •ë³´ ì´ˆê¸°í™”
        reset_count = 0
        for article_id in article_ids:
            try:
                # [FIX] Update DB + Local Cache
                article_doc = db.get_article(article_id)
                if article_doc:
                    # 1. DB Update
                    db.update_article(article_id, {
                        'publish_id': '',
                        'edition_code': '',
                        'edition_name': '',
                        'published': False
                    })
                    
                    
                    # 2. Local Cache Update (In-Place) - article_idì™€ url_hash ë‘ ê°€ì§€ë¡œ ê²€ìƒ‰
                    import glob
                    
                    found = False
                    cache_paths_to_check = []
                    
                    # ë°©ë²• 1: article_idë¡œ íŒŒì¼ëª… ê²€ìƒ‰ (ë” í™•ì‹¤í•¨)
                    article_id_pattern = os.path.join(CACHE_DIR, '*', f'*{article_id}*.json')
                    cache_paths_to_check.extend(glob.glob(article_id_pattern))
                    
                    # ë°©ë²• 2: URL í•´ì‹œë¡œ ê²€ìƒ‰ (í´ë°±)
                    url = article_doc.get('url')
                    if url:
                        from src.core_logic import get_url_hash
                        url_hash = get_url_hash(url)
                        url_hash_pattern = os.path.join(CACHE_DIR, '*', f'*{url_hash}*.json')
                        cache_paths_to_check.extend(glob.glob(url_hash_pattern))
                    
                    # ì¤‘ë³µ ì œê±°
                    cache_paths_to_check = list(set(cache_paths_to_check))
                    
                    for cache_path in cache_paths_to_check:
                        try:
                            with open(cache_path, 'r', encoding='utf-8') as f:
                                cached_data = json.load(f)
                            
                            # í•´ë‹¹ íšŒì°¨ì— ì†í•œ ê¸°ì‚¬ì¸ì§€ í™•ì¸
                            if cached_data.get('publish_id') != publish_id:
                                continue
                            
                            # Remove published flags
                            keys_to_reset = ['published', 'publish_id', 'edition_code', 'edition_name', 'published_at', 'data_file', 'status']
                            changed = False
                            for k in keys_to_reset:
                                if k in cached_data:
                                    cached_data.pop(k)
                                    changed = True
                            
                            # Ensure saved is True so it stays in Staged
                            if not cached_data.get('saved'):
                                cached_data['saved'] = True
                                changed = True
                            
                            if changed:
                                with open(cache_path, 'w', encoding='utf-8') as f:
                                    json.dump(cached_data, f, ensure_ascii=False, indent=2)
                                print(f"ğŸ”„ [Cache] Reset published status: {os.path.basename(cache_path)}")
                            found = True
                        except Exception as e:
                            print(f"âš ï¸ Failed to update cache file {cache_path}: {e}")
                    
                    if not found:
                        print(f"âš ï¸ Cache file not found for article_id: {article_id}")

                reset_count += 1
            except Exception as e:
                print(f"âš ï¸ Article reset failed: {article_id} - {e}")
        
        # 3. íšŒì°¨ ë¬¸ì„œ ì‚­ì œ
        db.db.collection('publications').document(publish_id).delete()
        print(f"ğŸ—‘ï¸ [Delete] Deleted publication: {publish_id} ({edition_name})")
        
        # 3-1. [NEW] _meta ë¬¸ì„œì—ì„œë„ íšŒì°¨ ì œê±°
        db.remove_issue_from_meta(edition_code)
        
        # 4. ì—°ì‡„ ì¬ì •ë ¬ (Cascade Renumbering)
        renumbered_count = 0
        renumber_msg = ""
        
        try:
            deleted_num_match = re.search(r'(\d+)', edition_name)
            if deleted_num_match:
                deleted_num = int(deleted_num_match.group(1))
                print(f"ğŸ”„ Starting renumbering check from {deleted_num}...")
                
                # ëª¨ë“  íšŒì°¨ ì¡°íšŒ (Firestore ì¿¼ë¦¬ ì‚¬ìš© ê°€ëŠ¥í•˜ë‚˜, ë©”ëª¨ë¦¬ì—ì„œ ì •ë ¬ì´ ì•ˆì „)
                all_pubs = db.db.collection('publications').stream()
                targets = []
                
                for pub in all_pubs:
                    pub_data = pub.to_dict()
                    pub_name = pub_data.get('edition_name', '')
                    match = re.search(r'(\d+)', pub_name)
                    if match:
                        num = int(match.group(1))
                        if num > deleted_num:
                            targets.append({'id': pub.id, 'num': num, 'data': pub_data})
                
                # ë²ˆí˜¸ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ (ì‘ì€ ë²ˆí˜¸ë¶€í„° ì¦‰, deleted_num+1 ë¶€í„° ì²˜ë¦¬)
                targets.sort(key=lambda x: x['num'])
                
                for t in targets:
                    old_num = t['num']
                    new_num = old_num - 1
                    new_name = f"{new_num}í˜¸" # í‘œì¤€ í¬ë§· ì ìš©
                    
                    # íšŒì°¨ ë¬¸ì„œ ì—…ë°ì´íŠ¸
                    db.db.collection('publications').document(t['id']).update({
                        'edition_name': new_name,
                        'updated_at': datetime.now(timezone.utc).isoformat()
                    })
                    
                    # í•´ë‹¹ íšŒì°¨ ê¸°ì‚¬ë“¤ ì—…ë°ì´íŠ¸
                    t_article_ids = t['data'].get('article_ids', [])
                    for aid in t_article_ids:
                        try:
                            db.update_article(aid, {'edition_name': new_name})
                        except Exception as ae:
                            print(f"âš ï¸ Failed to update article {aid} during renumber: {ae}")
                    
                    print(f"ğŸ”„ [Renumber] {old_num}í˜¸ -> {new_num}í˜¸ (ID: {t['id']})")
                    renumbered_count += 1
                
                # Config ì—…ë°ì´íŠ¸ (next_issue_number ê°ì†Œ)
                config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'publication_config.json')
                if os.path.exists(config_path):
                    with open(config_path, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                    current_next = config.get('next_issue_number', 1)
                    
                    # ë§Œì•½ ì‚­ì œëœ ë²ˆí˜¸ê°€ nextë³´ë‹¤ ì‘ìœ¼ë©´, í•˜ë‚˜ ì¤„ì—¬ì•¼ í•¨
                    if current_next > deleted_num:
                        config['next_issue_number'] = max(1, current_next - 1)
                        config['last_updated'] = datetime.now(timezone.utc).isoformat()
                        with open(config_path, 'w', encoding='utf-8') as f:
                            json.dump(config, f, ensure_ascii=False, indent=2)
                        print(f"âš™ï¸ Config updated: next_issue_number -> {config['next_issue_number']}")
                        renumber_msg = f"\nì´í›„ íšŒì°¨ {renumbered_count}ê°œê°€ ìˆœì„œëŒ€ë¡œ ì¬ì •ë ¬ë˜ì—ˆìŠµë‹ˆë‹¤."
        except Exception as e:
            print(f"âš ï¸ Renumbering failed: {e}")
            renumber_msg = f"\n(ì¬ì •ë ¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e})"
        
        return jsonify({
            'success': True,
            'deleted_issue': edition_name,
            'reset_articles': reset_count,
            'renumbered_issues': renumbered_count,
            'message': f'"{edition_name}" íšŒì°¨ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤. ({reset_count}ê°œ ê¸°ì‚¬ ì´ˆê¸°í™”){renumber_msg}'
        })
    except Exception as e:
        print(f"âŒ [Delete] Error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@publications_bp.route('/api/publications/update_edition', methods=['POST'])
def publications_update_edition():
    """
    âœï¸ íšŒì°¨ ì´ë¦„(ë²ˆí˜¸) ìˆ˜ì •
    - íšŒì°¨ ë¬¸ì„œì˜ edition_name ìˆ˜ì •
    - í•´ë‹¹ íšŒì°¨ì— ì†í•œ ëª¨ë“  ê¸°ì‚¬ì˜ edition_name ìˆ˜ì •
    """
    try:
        data = request.json or {}
        publish_id = data.get('publish_id')
        new_edition_name = data.get('new_edition_name')
        
        if not publish_id or not new_edition_name:
            return jsonify({'success': False, 'error': 'publish_idì™€ new_edition_name í•„ìˆ˜'}), 400
        
        # 1. íšŒì°¨ ì •ë³´ ì¡°íšŒ
        pub_record = db.get_publication(publish_id)
        if not pub_record:
            return jsonify({'success': False, 'error': 'íšŒì°¨ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}), 404
        
        old_edition_name = pub_record.get('edition_name', '')
        article_ids = pub_record.get('article_ids', [])
        
        # 1-1. ìƒˆ ì´ë¦„ ë¶„ì„ ë° ì¶©ëŒ í™•ì¸ (Cascade Shift)
        shifted_count = 0
        shift_msg = ""
        
        try:
            match = re.search(r'(\d+)', new_edition_name)
            if match:
                new_issue_num = int(match.group(1))
                
                # ëª¨ë“  íšŒì°¨ ì¡°íšŒ
                all_pubs = db.db.collection('publications').stream()
                conflicting_pubs = []
                
                for pub in all_pubs:
                    if pub.id == publish_id: continue # ë‚˜ëŠ” ì œì™¸
                    
                    pub_data = pub.to_dict()
                    pub_name = pub_data.get('edition_name', '')
                    p_match = re.search(r'(\d+)', pub_name)
                    if p_match:
                        p_num = int(p_match.group(1))
                        # ìƒˆ ë²ˆí˜¸ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ì€ íšŒì°¨ê°€ ì´ë¯¸ ìˆë‹¤ë©´ ë°€ì–´ì•¼ í•¨
                        if p_num >= new_issue_num:
                            conflicting_pubs.append({'id': pub.id, 'num': p_num, 'data': pub_data})
                
                # ë§Œì•½ ì¶©ëŒì´ í•˜ë‚˜ë¼ë„ ìˆë‹¤ë©´ Shift ì‹œì‘
                if conflicting_pubs:
                    print(f"ğŸ”„ Detected conflict for issue {new_issue_num}. Shifting {len(conflicting_pubs)} issues...")
                    
                    # í° ë²ˆí˜¸ë¶€í„° ì—­ìˆœìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ ì•ˆì „í•¨ (7->8, 6->7, 5->6)
                    conflicting_pubs.sort(key=lambda x: x['num'], reverse=True)
                    
                    for t in conflicting_pubs:
                        old_num = t['num']
                        next_num = old_num + 1
                        next_name = f"{next_num}í˜¸"
                        
                        # íšŒì°¨ ë¬¸ì„œ ì—…ë°ì´íŠ¸
                        db.db.collection('publications').document(t['id']).update({
                            'edition_name': next_name,
                            'updated_at': datetime.now(timezone.utc).isoformat()
                        })
                        
                        # ê¸°ì‚¬ ì—…ë°ì´íŠ¸
                        t_article_ids = t['data'].get('article_ids', [])
                        for aid in t_article_ids:
                            try:
                                db.update_article(aid, {'edition_name': next_name})
                            except: pass
                            
                        print(f"ğŸ”„ [Shift] {old_num}í˜¸ -> {next_num}í˜¸ (ID: {t['id']})")
                        shifted_count += 1
                        
                    shift_msg = f"\nê¸°ì¡´ íšŒì°¨ë“¤ê³¼ ì¶©ëŒí•˜ì—¬ {shifted_count}ê°œë¥¼ ë’¤ë¡œ ë°€ì—ˆìŠµë‹ˆë‹¤."

        except Exception as e:
            print(f"âš ï¸ Shift logic failed: {e}")

        # 2. íšŒì°¨ ë¬¸ì„œ ì—…ë°ì´íŠ¸
        db.db.collection('publications').document(publish_id).update({
            'edition_name': new_edition_name,
            'updated_at': datetime.now(timezone.utc).isoformat()
        })
        
        # 3. í•´ë‹¹ íšŒì°¨ì˜ ê¸°ì‚¬ë“¤ ì—…ë°ì´íŠ¸
        updated_count = 0
        for article_id in article_ids:
            try:
                db.update_article(article_id, {
                    'edition_name': new_edition_name
                })
                updated_count += 1
            except Exception as e:
                print(f"âš ï¸ Article update failed: {article_id} - {e}")
        
        # 4. Config íŒŒì¼ ì—…ë°ì´íŠ¸ (ë§Œì•½ ìˆ«ìê°€ ì»¤ì¡Œìœ¼ë©´ next_issue_number ì¡°ì •)
        config_updated = False
        try:
            match = re.search(r'(\d+)', new_edition_name)
            if match:
                new_issue_num = int(match.group(1))
                config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'publication_config.json')
                
                # Config ì½ê¸°
                current_config = {}
                if os.path.exists(config_path):
                    with open(config_path, 'r', encoding='utf-8') as f:
                        current_config = json.load(f)
                
                next_issue = current_config.get('next_issue_number', 1)
                
                # ìƒˆ ë²ˆí˜¸ê°€ í˜„ì¬ nextë³´ë‹¤ í¬ê±°ë‚˜ ê°™ìœ¼ë©´, nextë¥¼ ìƒˆ ë²ˆí˜¸ + 1ë¡œ ì„¤ì •
                if new_issue_num >= next_issue:
                    current_config['next_issue_number'] = new_issue_num + 1
                    current_config['last_updated'] = datetime.now(timezone.utc).isoformat()
                    
                    with open(config_path, 'w', encoding='utf-8') as f:
                        json.dump(current_config, f, ensure_ascii=False, indent=2)
                    config_updated = True
                    print(f"âš™ï¸ Config updated: next_issue_number -> {new_issue_num + 1}")
        except Exception as e:
            print(f"âš ï¸ Config update failed during rename: {e}")
        
        print(f"âœï¸ [Update] Edition renamed: {publish_id} ({old_edition_name} -> {new_edition_name})")
        
        message = f'íšŒì°¨ëª…ì´ "{new_edition_name}"(ìœ¼)ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. ({updated_count}ê°œ ê¸°ì‚¬ ì—…ë°ì´íŠ¸)'
        if shift_msg:
            message += shift_msg
        if config_updated:
            message += f'\në‹¤ìŒ ë°œí–‰ í˜¸ìˆ˜ë„ {new_issue_num + 1}í˜¸ë¡œ ìë™ ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤.'
        
        return jsonify({
            'success': True,
            'updated_articles': updated_count,
            'message': message
        })
    except Exception as e:
        print(f"âŒ [Update Edition] Error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500
