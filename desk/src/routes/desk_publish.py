# -*- coding: utf-8 -*-
"""
ë°œí–‰(Publish) API - ê¸°ì‚¬ ë°œí–‰, ìºì‹œ ë™ê¸°í™”, ë°œí–‰ ì„¤ì • ê´€ë¦¬
"""
import os
import json
from datetime import datetime, timezone
from flask import Blueprint, request, jsonify

publish_bp = Blueprint('publish', __name__)

CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'cache')


@publish_bp.route('/api/desk/publish_selected', methods=['POST'])
def desk_publish_selected():
    """ì„ íƒëœ Staging íŒŒì¼ë§Œ ë°œí–‰ (New or Append to Issue)"""
    try:
        from src.pipeline import save_article, get_db
        db = get_db()
        
        data = request.json or {}
        filenames = data.get('filenames', [])
        mode = data.get('mode', 'new')
        target_publish_id = data.get('target_publish_id')
        
        if not filenames:
            return jsonify({'success': False, 'error': 'ì„ íƒëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.'}), 400
        
        today_str = datetime.now().strftime('%Y-%m-%d')
        cache_date_dir = os.path.join(CACHE_DIR, today_str)
        
        # 1. Edition Info
        edition_code = ""
        edition_name = ""
        publish_id = ""
        
        if mode == 'new':
            # publication_config.jsonì—ì„œ ë‹¤ìŒ í˜¸ìˆ˜ ì½ê¸°
            config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'publication_config.json')
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                next_idx = config.get('next_issue_number', 1)
            except:
                next_idx = 1
                
            yy = today_str[2:4]
            mm = today_str[5:7]
            dd = today_str[8:10]
            edition_code = f"{yy}{mm}{dd}_{next_idx}"
            edition_name = f"{next_idx}í˜¸"
            
            # ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸ (ë‹¤ìŒ í˜¸ìˆ˜ ì¦ê°€)
            try:
                config['next_issue_number'] = next_idx + 1
                config['last_updated'] = datetime.now(timezone.utc).isoformat()
                with open(config_path, 'w', encoding='utf-8') as f:
                    json.dump(config, f, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"âš ï¸ Config update failed: {e}")
            
            pub_data = {
                'edition_code': edition_code,
                'edition_name': edition_name,
                'article_count': 0,
                'article_ids': [],  # IDë§Œ ì €ì¥ (ì¤‘ë³µ ì œê±°)
                'articles': [],     # [NEW] ê¸°ì‚¬ ìƒì„¸ ë‚´ì¥
                'published_at': datetime.now(timezone.utc).isoformat(),
                'date': today_str,
                'status': 'preview'
            }
            publish_id = db.create_publication_record(pub_data)
            if not publish_id:
                return jsonify({'success': False, 'error': 'Failed to create publication record'}), 500
        
        elif mode == 'append':
            if not target_publish_id:
                return jsonify({'success': False, 'error': 'Target publish ID required for append mode'}), 400
            
            publish_id = target_publish_id
            pub_record = db.get_publication(publish_id)
            if not pub_record:
                return jsonify({'success': False, 'error': 'Target publication not found'}), 404
            
            edition_code = pub_record.get('edition_code')
            edition_name = pub_record.get('edition_name')
        
        # 2. Process Articles
        published_count = 0
        failed_count = 0
        skipped_count = 0  # [NEW] ì´ë¯¸ ë°œí–‰ëœ ê¸°ì‚¬ ìŠ¤í‚µ ì¹´ìš´í„°
        skipped_articles = []  # [NEW] ìŠ¤í‚µëœ ê¸°ì‚¬ ID ëª©ë¡
        published_article_ids = []       # DBìš©: IDë§Œ ì €ì¥
        published_articles_detail = []   # ë¡œì»¬ ì¸ë±ìŠ¤ìš©: ìƒì„¸ ì •ë³´
        
        # [NEW] Firebaseì—ì„œ ì´ë¯¸ ë°œí–‰ëœ article_ids ì¡°íšŒ
        already_published_ids = set()
        try:
            from src.published_articles import get_published_article_ids, invalidate_cache
            already_published_ids = get_published_article_ids(force_refresh=True)
        except Exception as e:
            print(f"âš ï¸ [Publish] Failed to load published IDs: {e}")
        
        for filename in filenames:
            filepath = os.path.join(cache_date_dir, filename)
            if not os.path.exists(filepath):
                for d in os.listdir(CACHE_DIR):
                    check_path = os.path.join(CACHE_DIR, d, filename)
                    if os.path.exists(check_path):
                        filepath = check_path
                        break
                else:
                    failed_count += 1
                    continue
            
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    staging_data = json.load(f)
                
                # [NEW] ì´ë¯¸ ë°œí–‰ëœ ê¸°ì‚¬ì¸ì§€ í™•ì¸
                article_id = staging_data.get('article_id', '')
                if article_id and article_id in already_published_ids:
                    skipped_count += 1
                    skipped_articles.append(article_id)
                    print(f"â­ï¸ [Publish] Skipped (already published): {article_id}")
                    continue
                
                staging_data['publish_id'] = publish_id
                staging_data['edition_code'] = edition_code
                staging_data['edition_name'] = edition_name
                
                result = save_article(staging_data, source_id=staging_data.get('source_id'), skip_evaluation=True)
                
                if result.get('status') == 'saved':
                    staging_data['published'] = True
                    staging_data['status'] = 'PUBLISHED'
                    staging_data['published_at'] = datetime.now(timezone.utc).isoformat()
                    staging_data['data_file'] = result.get('filename')
                    
                    with open(filepath, 'w', encoding='utf-8') as f:
                        json.dump(staging_data, f, ensure_ascii=False, indent=2)
                    
                    published_count += 1
                    
                    article_id = result.get('article_id', staging_data.get('article_id'))
                    published_article_ids.append(article_id)
                    
                    # ë¡œì»¬ ì¸ë±ìŠ¤ íŒŒì¼ìš© ìƒì„¸ ì •ë³´ (Firebase ë‚´ì¥ êµ¬ì¡°)
                    published_articles_detail.append({
                        'id': article_id,
                        'title': staging_data.get('title_ko') or staging_data.get('title'),
                        'title_ko': staging_data.get('title_ko', ''),
                        'title_en': staging_data.get('title', ''),
                        'summary': staging_data.get('summary', ''),
                        'url': staging_data.get('url'),
                        'source_id': staging_data.get('source_id', ''),
                        'zero_echo_score': staging_data.get('zero_echo_score'),
                        'impact_score': staging_data.get('impact_score'),
                        'layout_type': staging_data.get('layout_type', 'Standard'),
                        'tags': staging_data.get('tags', []),
                        'category': staging_data.get('category', 'ë¯¸ë¶„ë¥˜'),
                        'filename': result.get('filename'),
                        'date': result.get('date'),
                        'published_at': staging_data.get('published_at', datetime.now(timezone.utc).isoformat())
                    })
                else:
                    failed_count += 1
                    
            except Exception as e:
                print(f"âš ï¸ [Publish] Error on {filename}: {e}")
                failed_count += 1
        
        # 3. Update Index
        # DBìš©: ID ë¦¬ìŠ¤íŠ¸
        final_article_ids = published_article_ids
        # ë¡œì»¬ìš©: ìƒì„¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        final_article_detail = published_articles_detail
        
        if mode == 'append':
            current_record = db.get_publication(publish_id)
            existing_ids = current_record.get('article_ids', [])
            # ê¸°ì¡´ articles ë°°ì—´ì—ì„œ ID ì¶”ì¶œ (í•˜ìœ„ í˜¸í™˜)
            if not existing_ids:
                existing_ids = [a.get('id') for a in current_record.get('articles', []) if a.get('id')]
            final_article_ids = existing_ids + published_article_ids
            
            # ë¡œì»¬ ì¸ë±ìŠ¤ìš© ìƒì„¸ ì •ë³´ë„ í•©ì¹˜ê¸°
            existing_detail = current_record.get('articles', [])
            final_article_detail = existing_detail + published_articles_detail
        
        # ë¡œì»¬ issue ì¸ë±ìŠ¤ íŒŒì¼ (ìƒì„¸ ì •ë³´ í¬í•¨ - WEB í˜¸í™˜)
        index_data = {
            'id': publish_id,
            'edition_code': edition_code,
            'edition_name': edition_name,
            'published_at': datetime.now(timezone.utc).isoformat(),
            'date': today_str,
            'article_count': len(final_article_ids),
            'articles': final_article_detail,  # ë¡œì»¬ì—ëŠ” ìƒì„¸ ì •ë³´ ìœ ì§€
            'schema_version': '2.0.0' # [NEW] ìŠ¤í‚¤ë§ˆ ë²„ì „
        }
        db.save_issue_index_file(index_data)
        
        # Firestore DB (ë‚´ì¥í˜• êµ¬ì¡°: articles ë°°ì—´ í¬í•¨)
        db.update_publication_record(publish_id, {
            'article_count': len(final_article_ids),
            'article_ids': final_article_ids,
            'articles': final_article_detail,  # [NEW] ê¸°ì‚¬ ìƒì„¸ ë‚´ì¥
            'updated_at': datetime.now(timezone.utc).isoformat(),
            'schema_version': '2.0.0' # [NEW] ìŠ¤í‚¤ë§ˆ ë²„ì „
        })
        
        # [NEW] ë°œí–‰ í›„ ìºì‹œ ë¬´íš¨í™”
        try:
            invalidate_cache()
        except Exception as e:
            print(f"âš ï¸ [Publish] Cache invalidation failed: {e}")
        
        # ì‘ë‹µ ë©”ì‹œì§€ êµ¬ì„±
        message = f'{published_count}ê°œ ê¸°ì‚¬ ë°œí–‰ ì™„ë£Œ ({edition_name})'
        if skipped_count > 0:
            message += f' / {skipped_count}ê°œ ì¤‘ë³µ ìŠ¤í‚µ'
        
        return jsonify({
            'success': True,
            'published': published_count,
            'failed': failed_count,
            'skipped': skipped_count,
            'skipped_articles': skipped_articles,
            'publish_id': publish_id,
            'edition_name': edition_name,
            'message': message
        })
    except Exception as e:
        print(f"âŒ [Publish] Error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@publish_bp.route('/api/cache/sync', methods=['POST'])
def cache_sync():
    """
    â˜ï¸ ë¡œì»¬ ìºì‹œ + í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ë¥¼ Firebaseì— ë™ê¸°í™”
    
    - synced_at í•„ë“œë¡œ ë™ê¸°í™” ì—¬ë¶€ íŒë‹¨ (Firestore ì¡°íšŒ ë¶ˆí•„ìš” = ë¹„ìš© 0)
    - ë™ê¸°í™” í›„ ë¡œì»¬ íŒŒì¼ì— synced_at ë§ˆí‚¹
    - í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ë„ í•¨ê»˜ ë™ê¸°í™”
    """
    try:
        from src.db_client import DBClient
        
        db = DBClient()
        if not db.db:
            return jsonify({'success': False, 'error': 'Firestore ì—°ê²° ì‹¤íŒ¨. serviceAccountKey.jsonì„ í™•ì¸í•˜ì„¸ìš”.'}), 500
        
        data = request.json or {}
        date_str = data.get('date')  # Noneì´ë©´ ì „ì²´ ë‚ ì§œ
        sync_all = date_str is None
        
        synced_count = 0
        skipped_count = 0
        failed_count = 0
        
        # ë™ê¸°í™” ëŒ€ìƒ í´ë” ê²°ì •
        if date_str:
            date_folders = [date_str] if os.path.exists(os.path.join(CACHE_DIR, date_str)) else []
        else:
            date_folders = [d for d in os.listdir(CACHE_DIR) 
                          if os.path.isdir(os.path.join(CACHE_DIR, d)) and len(d) == 10]
        
        # ë‚ ì§œë³„ ìºì‹œ ë™ê¸°í™”
        for date_folder in date_folders:
            cache_date_dir = os.path.join(CACHE_DIR, date_folder)
            cache_list = []
            
            for filename in os.listdir(cache_date_dir):
                if not filename.endswith('.json'):
                    continue
                
                filepath = os.path.join(cache_date_dir, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        cache_data = json.load(f)
                    
                    # synced_at í•„ë“œê°€ ìˆìœ¼ë©´ ì´ë¯¸ ë™ê¸°í™”ë¨ â†’ ìŠ¤í‚µ (Firestore ì¡°íšŒ ì—†ìŒ!)
                    if cache_data.get('synced_at'):
                        skipped_count += 1
                        continue
                    
                    # article_id í™•ì¸
                    article_id = cache_data.get('article_id')
                    if not article_id:
                        article_id = filename.replace('.json', '')
                        cache_data['article_id'] = article_id
                    
                    cache_list.append(cache_data)
                    
                except Exception as e:
                    print(f"âš ï¸ [Sync] Read error {filename}: {e}")
                    failed_count += 1
            
            # ë°°ì¹˜ ì—…ë¡œë“œ
            if cache_list:
                result = db.upload_cache_batch(date_folder, cache_list)
                synced_count += result.get('success', 0)
                failed_count += result.get('failed', 0)
                
                # ì—…ë¡œë“œ ì„±ê³µí•œ íŒŒì¼ì— synced_at ë§ˆí‚¹
                synced_at = datetime.now(timezone.utc).isoformat()
                for cache_data in cache_list:
                    article_id = cache_data.get('article_id')
                    if not article_id:
                        continue
                    
                    filepath = os.path.join(cache_date_dir, f"{article_id}.json")
                    if not os.path.exists(filepath):
                        # í•´ì‹œ ê¸°ë°˜ íŒŒì¼ëª… ì°¾ê¸°
                        for fn in os.listdir(cache_date_dir):
                            if fn.endswith('.json'):
                                try:
                                    with open(os.path.join(cache_date_dir, fn), 'r', encoding='utf-8') as f:
                                        d = json.load(f)
                                        if d.get('article_id') == article_id:
                                            filepath = os.path.join(cache_date_dir, fn)
                                            break
                                except:
                                    pass
                    
                    if os.path.exists(filepath):
                        try:
                            with open(filepath, 'r', encoding='utf-8') as f:
                                file_data = json.load(f)
                            file_data['synced_at'] = synced_at
                            with open(filepath, 'w', encoding='utf-8') as f:
                                json.dump(file_data, f, ensure_ascii=False, indent=2)
                        except:
                            pass
        
        # í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ ë™ê¸°í™”
        history_count = 0
        try:
            data_dir = os.path.join(os.path.dirname(CACHE_DIR), 'data')
            history_file = os.path.join(data_dir, 'crawling_history.json')
            
            if os.path.exists(history_file):
                with open(history_file, 'r', encoding='utf-8') as f:
                    local_history = json.load(f)
                
                if local_history:
                    result = db.upload_crawling_history(local_history)
                    history_count = result.get('count', 0)
        except Exception as e:
            print(f"âš ï¸ [Sync] History sync error: {e}")
        
        return jsonify({
            'success': True,
            'synced': synced_count,
            'skipped': skipped_count,
            'failed': failed_count,
            'history_count': history_count,
            'message': f'â˜ï¸ ë™ê¸°í™” ì™„ë£Œ: ìºì‹œ {synced_count}ê°œ, íˆìŠ¤í† ë¦¬ {history_count}ê°œ URL'
        })
        
    except Exception as e:
        print(f"âŒ [Cache Sync] Error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@publish_bp.route('/api/publication/config', methods=['GET', 'POST'])
def publication_config():
    """
    ğŸ“‹ ë°œí–‰ ì„¤ì • ì¡°íšŒ ë° ìˆ˜ì •
    GET: í˜„ì¬ ì„¤ì • ì¡°íšŒ
    POST: ë‹¤ìŒ í˜¸ìˆ˜ ìˆ˜ë™ ì„¤ì • { "next_issue_number": N }
    """
    config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'publication_config.json')
    
    if request.method == 'GET':
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            return jsonify({'success': True, 'config': config})
        except Exception as e:
            return jsonify({'success': False, 'error': str(e)}), 500
    
    elif request.method == 'POST':
        try:
            data = request.json or {}
            
            # ê¸°ì¡´ ì„¤ì • ì½ê¸°
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
            except:
                config = {}
            
            # ì—…ë°ì´íŠ¸
            if 'next_issue_number' in data:
                config['next_issue_number'] = int(data['next_issue_number'])
            
            config['last_updated'] = datetime.now(timezone.utc).isoformat()
            
            # ì €ì¥
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            
            return jsonify({
                'success': True,
                'config': config,
                'message': f"ë‹¤ìŒ ë°œí–‰ í˜¸ìˆ˜ê°€ {config.get('next_issue_number')}í˜¸ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤."
            })
        except Exception as e:
            return jsonify({'success': False, 'error': str(e)}), 500


@publish_bp.route('/api/firebase/stats')
def firebase_stats():
    """
    ğŸ”¥ Firebase ì‚¬ìš©ëŸ‰ í†µê³„ ì¡°íšŒ
    - ì´ë²ˆ ì„¸ì…˜ì˜ ì½ê¸°/ì“°ê¸°/ì‚­ì œ íšŸìˆ˜
    """
    try:
        from src.db_client import DBClient
        stats = DBClient.get_usage_stats()
        return jsonify({
            'success': True,
            'stats': stats
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@publish_bp.route('/api/firebase/stats/reset', methods=['POST'])
def firebase_stats_reset():
    """
    ğŸ”„ Firebase ì‚¬ìš©ëŸ‰ í†µê³„ ë¦¬ì…‹
    """
    try:
        from src.db_client import DBClient
        DBClient.reset_usage_stats()
        stats = DBClient.get_usage_stats()
        return jsonify({
            'success': True,
            'stats': stats,
            'message': 'í†µê³„ê°€ ë¦¬ì…‹ë˜ì—ˆìŠµë‹ˆë‹¤.'
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@publish_bp.route('/api/publication/<publish_id>/update', methods=['POST'])
def update_publication_format(publish_id):
    """
    ğŸ”„ íšŒì°¨ ë°ì´í„° ìµœì‹  í¬ë§·ìœ¼ë¡œ ì—…ë°ì´íŠ¸
    - ë¡œì»¬ ìºì‹œì—ì„œ ê¸°ì‚¬ ìƒì„¸ ì •ë³´ë¥¼ ì½ì–´ì™€ ë³´ê°•
    - ë¡œì»¬ ì¸ë±ìŠ¤ + Firebase ë™ì‹œ ì—…ë°ì´íŠ¸
    """
    try:
        from src.db_client import DBClient
        
        db = DBClient()
        
        # 1. í˜„ì¬ íšŒì°¨ ì •ë³´ ì¡°íšŒ
        pub_data = db.get_publication(publish_id)
        if not pub_data:
            return jsonify({'success': False, 'error': f'íšŒì°¨ {publish_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}), 404
        
        articles = pub_data.get('articles', [])
        article_ids = pub_data.get('article_ids', [])
        
        # articles ë°°ì—´ì´ ì—†ìœ¼ë©´ article_idsì—ì„œ ë³µì›
        if not articles and article_ids:
            articles = [{'id': aid} for aid in article_ids]
        
        if not articles:
            return jsonify({'success': False, 'error': 'ì—…ë°ì´íŠ¸í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.'}), 400
        
        # 2. ë¡œì»¬ ìºì‹œì—ì„œ ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ë³´ê°•
        enriched_articles = []
        enriched_count = 0
        not_found_count = 0
        
        for article in articles:
            article_id = article.get('id', '')
            
            # ì´ë¯¸ summaryê°€ ìˆìœ¼ë©´ ë³´ê°•ë¨
            if article.get('summary') and article.get('zero_echo_score') is not None:
                enriched_articles.append(article)
                continue
            
            # ìºì‹œì—ì„œ ì°¾ê¸°
            cache_data = find_article_in_cache(article_id)
            
            if cache_data:
                enriched = build_enriched_article(article, cache_data)
                enriched_articles.append(enriched)
                enriched_count += 1
            else:
                # ìºì‹œì— ì—†ìœ¼ë©´ ê¸°ì¡´ ë°ì´í„° ìœ ì§€
                enriched_articles.append(article)
                not_found_count += 1
        
        # 3. ì—…ë°ì´íŠ¸
        update_data = {
            'articles': enriched_articles,
            'article_count': len(enriched_articles),
            'updated_at': datetime.now(timezone.utc).isoformat(),
            'schema_version': '2.0.0'  # ìŠ¤í‚¤ë§ˆ ë²„ì „ ê¸°ë¡
        }
        
        # Firebase ì—…ë°ì´íŠ¸
        db.update_publication_record(publish_id, update_data)
        
        # ë¡œì»¬ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        index_data = {
            'id': publish_id,
            'edition_code': pub_data.get('edition_code', publish_id),
            'edition_name': pub_data.get('edition_name', ''),
            'published_at': pub_data.get('published_at', ''),
            'date': pub_data.get('date', ''),
            'article_count': len(enriched_articles),
            'articles': enriched_articles
        }
        db.save_issue_index_file(index_data)
        
        return jsonify({
            'success': True,
            'enriched': enriched_count,
            'not_found': not_found_count,
            'total': len(enriched_articles),
            'message': f'{enriched_count}ê°œ ê¸°ì‚¬ ë³´ê°• ì™„ë£Œ ({not_found_count}ê°œ ìºì‹œ ì—†ìŒ)'
        })
        
    except Exception as e:
        print(f"âŒ [Update Format] Error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


def find_article_in_cache(article_id: str) -> dict | None:
    """ìºì‹œì—ì„œ article_idë¡œ ê¸°ì‚¬ ì°¾ê¸°"""
    import glob
    
    if not os.path.exists(CACHE_DIR):
        return None
    
    # íŒŒì¼ëª… íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰: *_article_id.json ë˜ëŠ” article_id.json
    patterns = [
        os.path.join(CACHE_DIR, '*', f'*{article_id}.json'),
        os.path.join(CACHE_DIR, '*', f'{article_id}.json')
    ]
    
    for pattern in patterns:
        found = glob.glob(pattern)
        if found:
            try:
                with open(found[0], 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # article_id ì¼ì¹˜ í™•ì¸
                    if data.get('article_id') == article_id or article_id in found[0]:
                        return data
            except Exception:
                pass
    
    return None


def build_enriched_article(article: dict, cache_data: dict) -> dict:
    """ìºì‹œ ë°ì´í„°ë¡œ articles ë°°ì—´ í•­ëª© ë³´ê°•"""
    return {
        'id': article.get('id', ''),
        'title': cache_data.get('title_ko') or cache_data.get('title') or article.get('title', ''),
        'title_ko': cache_data.get('title_ko', ''),
        'title_en': cache_data.get('title', ''),
        'summary': cache_data.get('summary', ''),
        'url': cache_data.get('url') or article.get('url', ''),
        'image_url': cache_data.get('image_url', ''),  # [NEW] ëŒ€í‘œ ì´ë¯¸ì§€
        'author': cache_data.get('author', ''),        # [NEW] ì‘ì„±ì
        'source_id': cache_data.get('source_id', ''),
        'zero_echo_score': cache_data.get('zero_echo_score'),
        'impact_score': cache_data.get('impact_score'),
        'layout_type': cache_data.get('layout_type', 'Standard'), # ê¸°ë³¸ê°’ Standard
        'tags': cache_data.get('tags', []),
        'category': cache_data.get('category', 'ë¯¸ë¶„ë¥˜'),
        'reading_time': cache_data.get('reading_time', 0), # [NEW] ì˜ˆìƒ ì½ê¸° ì‹œê°„
        'filename': article.get('filename', ''),
        'date': article.get('date', cache_data.get('crawled_at', '')[:10] if cache_data.get('crawled_at') else ''),
        'published_at': cache_data.get('published_at', article.get('published_at', '')),
        # [NEW] ê¸°ì‚¬ ì›ë³¸ ì…ë ¥ ì‹œê°„ (Real Input Time)
        'origin_published_at': cache_data.get('published_at', ''), 
        # [NEW] ì›ë³¸ ë°ì´í„° ì¼ë¶€ ë³´ì¡´ (í•„ìš” ì‹œ)
        'meta_description': cache_data.get('description', '')
    }


@publish_bp.route('/api/debug/latest_issue')
def debug_latest_issue():
    """ğŸ› ë””ë²„ê·¸: Firestoreì˜ ìµœì‹  íšŒì°¨ ë°ì´í„° ì›ë³¸ ì¡°íšŒ"""
    try:
        from src.db_client import DBClient
        db = DBClient()
        issues = db.get_issues_from_meta()
        if not issues:
            return jsonify({'error': 'No issues found'})
            
        latest_id = issues[0].get('id') or issues[0].get('edition_code')
        data = db.get_publication(latest_id)
        
        return jsonify({
            'issue_id': latest_id,
            'data': data
        })
    except Exception as e:
        return jsonify({'error': str(e)})


@publish_bp.route('/api/debug/meta')
def debug_meta_doc():
    """ğŸ› ë””ë²„ê·¸: Firestore _meta ë¬¸ì„œ ì›ë³¸ ì¡°íšŒ"""
    try:
        from src.db_client import DBClient
        db = DBClient()
        meta_ref = db.db.collection('publications').document('_meta')
        meta_doc = meta_ref.get()
        if meta_doc.exists:
            return jsonify(meta_doc.to_dict())
        return jsonify({'error': '_meta not found'})
    except Exception as e:
        return jsonify({'error': str(e)})

