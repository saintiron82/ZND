"""
ë°œí–‰ëœ ê¸°ì‚¬ ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸
- ZES >= 4.0 (Noise) ê¸°ì‚¬ ì‚­ì œ
- ë°œí–‰ ê¸°ì¤€: ZES < 4.0ë§Œ ìœ íš¨

ì‚¬ìš©ë²•:
    python cleanup_published.py           # ì‹œë®¬ë ˆì´ì…˜ (ì‚­ì œ ì•ˆí•¨)
    python cleanup_published.py --force   # ì‹¤ì œ ì‚­ì œ
    python cleanup_published.py --date 2025-12-10  # íŠ¹ì • ë‚ ì§œë§Œ
"""
import os
import json
import glob
import argparse
import sys

# Windows console encoding fix
sys.stdout.reconfigure(encoding='utf-8')

# ë°œí–‰ ê¸°ì¤€: ZES 4.0 ë¯¸ë§Œë§Œ ìœ íš¨ (4.0 ì´ìƒì€ Noise)
NOISE_THRESHOLD = 4.0


def cleanup_published(data_dir, dry_run=True, target_date=None):
    """
    ë°œí–‰ëœ ê¸°ì‚¬ ì¤‘ Noise(ZES >= 4.0) ì‚­ì œ
    """
    print("=" * 60)
    print("  ğŸ“° ZND ë°œí–‰ ê¸°ì‚¬ ì •ë¦¬ ë„êµ¬")
    print("=" * 60)
    print(f"ğŸ“ ëŒ€ìƒ í´ë”: {data_dir}")
    print(f"ğŸ“ Noise ê¸°ì¤€: ZES >= {NOISE_THRESHOLD}")
    if target_date:
        print(f"ğŸ“… ëŒ€ìƒ ë‚ ì§œ: {target_date}")
    print(f"ğŸ”§ ëª¨ë“œ: {'ì‹œë®¬ë ˆì´ì…˜ (ì‚­ì œ ì•ˆí•¨)' if dry_run else 'âš ï¸ ì‹¤ì œ ì‚­ì œ'}")
    print("=" * 60)
    print()
    
    # ìŠ¤ìº” ëŒ€ìƒ ê²°ì •
    if target_date:
        search_path = os.path.join(data_dir, target_date, "*.json")
    else:
        search_path = os.path.join(data_dir, "**", "*.json")
    
    files = glob.glob(search_path, recursive=True)
    
    targets = []
    kept = []
    skipped = []
    
    for file_path in files:
        if not os.path.isfile(file_path):
            continue
        
        filename = os.path.basename(file_path)
        
        # ì‹œìŠ¤í…œ íŒŒì¼ ì œì™¸
        if filename in ['index.json', 'daily_summary.json', 'crawling_history.json']:
            skipped.append((file_path, 'system_file'))
            continue
            
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            zs = float(data.get('zero_echo_score', 10))  # ê¸°ë³¸ê°’ 10 (ì‚­ì œ ëŒ€ìƒ)
            title = data.get('title_ko', '(ì œëª© ì—†ìŒ)')[:40]
            
            if zs >= NOISE_THRESHOLD:
                targets.append({
                    'path': file_path,
                    'zs': zs,
                    'title': title,
                    'date': os.path.basename(os.path.dirname(file_path))
                })
            else:
                kept.append({
                    'path': file_path,
                    'zs': zs,
                    'title': title
                })
                
        except Exception as e:
            skipped.append((file_path, str(e)))
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"ğŸ“Š ìŠ¤ìº” ê²°ê³¼:")
    print(f"   âœ… ìœ ì§€: {len(kept)}ê°œ (ZES < {NOISE_THRESHOLD})")
    print(f"   ğŸ—‘ï¸ ì‚­ì œ ëŒ€ìƒ: {len(targets)}ê°œ (ZES >= {NOISE_THRESHOLD})")
    print(f"   â­ï¸ ìŠ¤í‚µ: {len(skipped)}ê°œ")
    print()
    
    if not targets:
        print("âœ… ì‚­ì œí•  Noise ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # ì‚­ì œ ëŒ€ìƒ ëª©ë¡ ì¶œë ¥
    print("ğŸ—‘ï¸ ì‚­ì œ ëŒ€ìƒ ëª©ë¡:")
    print("-" * 60)
    
    # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í•‘
    by_date = {}
    for t in targets:
        date = t['date']
        if date not in by_date:
            by_date[date] = []
        by_date[date].append(t)
    
    deleted_count = 0
    
    for date in sorted(by_date.keys(), reverse=True):
        print(f"\nğŸ“… {date} ({len(by_date[date])}ê°œ):")
        for item in by_date[date]:
            action = "ğŸ—‘ï¸ DELETE" if not dry_run else "âš ï¸ FOUND"
            print(f"   [{action}] ZES:{item['zs']:.1f} | {item['title']}")
            
            if not dry_run:
                try:
                    os.remove(item['path'])
                    print(f"            â†’ âœ… ì‚­ì œ ì™„ë£Œ")
                    deleted_count += 1
                except Exception as e:
                    print(f"            â†’ âŒ ì˜¤ë¥˜: {e}")
    
    print()
    print("=" * 60)
    
    if dry_run:
        print("âš ï¸ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œì…ë‹ˆë‹¤. ì‹¤ì œ ì‚­ì œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print()
        print("ì‹¤ì œ ì‚­ì œí•˜ë ¤ë©´:")
        print(f"   python {os.path.basename(__file__)} --force")
        if target_date:
            print(f"   python {os.path.basename(__file__)} --force --date {target_date}")
    else:
        print(f"âœ… ì‚­ì œ ì™„ë£Œ: {deleted_count}ê°œ íŒŒì¼")
        print()
        print("ğŸ’¡ index.json ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    print("=" * 60)


def list_dates(data_dir):
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ëª©ë¡ ì¶œë ¥"""
    print("ğŸ“… ë°œí–‰ëœ ë‚ ì§œ ëª©ë¡:")
    print("-" * 40)
    
    dates = []
    for item in os.listdir(data_dir):
        item_path = os.path.join(data_dir, item)
        if os.path.isdir(item_path) and len(item) == 10 and item[4] == '-':
            # Count articles
            json_files = glob.glob(os.path.join(item_path, "*.json"))
            count = len([f for f in json_files if os.path.basename(f) not in ['index.json', 'daily_summary.json']])
            dates.append((item, count))
    
    dates.sort(reverse=True)
    
    for date, count in dates:
        print(f"   {date}: {count}ê°œ ê¸°ì‚¬")
    
    print("-" * 40)
    print(f"ì´ {len(dates)}ê°œ ë‚ ì§œ")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="ë°œí–‰ëœ ê¸°ì‚¬ ì •ë¦¬ (Noise ì‚­ì œ)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
    python cleanup_published.py              # ì‹œë®¬ë ˆì´ì…˜
    python cleanup_published.py --force      # ì‹¤ì œ ì‚­ì œ
    python cleanup_published.py --date 2025-12-10  # íŠ¹ì • ë‚ ì§œ
    python cleanup_published.py --list       # ë‚ ì§œ ëª©ë¡
        """
    )
    parser.add_argument('--force', action='store_true', help='ì‹¤ì œ ì‚­ì œ ì‹¤í–‰')
    parser.add_argument('--date', type=str, help='íŠ¹ì • ë‚ ì§œë§Œ ì²˜ë¦¬ (YYYY-MM-DD)')
    parser.add_argument('--list', action='store_true', help='ë°œí–‰ ë‚ ì§œ ëª©ë¡ í‘œì‹œ')
    args = parser.parse_args()
    
    # ê²½ë¡œ ì„¤ì •
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    data_dir = os.path.join(base_dir, 'data')
    
    if not os.path.exists(data_dir):
        print(f"âŒ ë°ì´í„° í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        sys.exit(1)
    
    if args.list:
        list_dates(data_dir)
    else:
        cleanup_published(data_dir, dry_run=not args.force, target_date=args.date)
