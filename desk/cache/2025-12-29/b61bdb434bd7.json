{
  "_classification": {
    "classified_at": "2025-12-29T01:56:15.669863+00:00",
    "is_selected": true,
    "classified_by": "desk_user",
    "category": "AI/ML"
  },
  "_original": {
    "source_id": "aitimes",
    "description": "SK텔레콤(대표 정재헌)은 국내 최초로 매개변수 500B(5000억개) 규모의 초거대 AI 모델 ‘A.X K1(에이닷엑스 케이원)’을 선보인다고 28일 밝혔다.S",
    "published_at": "2025-12-28T17:48:49+09:00",
    "crawled_at": "2025-12-29T01:41:27.770287+00:00",
    "text": "기사를 읽어드립니다. (사진=SKT) SK텔레콤(대표 정재헌)은 국내 최초로 매개변수 500B(5000억개) 규모의 초거대 AI 모델 ‘A.X K1(에이닷엑스 케이원)’을 선보인다고 28일 밝혔다. SKT는 오는 30일 과학기술정보통신부 주최 '독자 AI 파운데이션 모델 프로젝트 1차 발표회'에서 A.X K1을 공개한다고 전했다. A.X K1은 5190억개 매개변수로 구성된 대형언어모델(LLM)이다. 해외의 빅테크 모델에 비하면 상대적으로 규모는 작지만, 국내에서는 최대 규모다. 참고로, 오픈 소스 모델 중 최대 매개변수는 중국의 문샷 AI가 지난 7월 출시한 '키미 K2'의 1조개(1T)다. 또, 이전까지는 딥시크의 모델 군이 6850억개에 달했다. 앞서 유영상 전 SKT 대표는 지난 8월 링크드인을 통해 5000B 모델 출시를 예고한 바 있다. 독자 AI 프로젝트에서 '규모'로 승부하겠다는 의도다. 특히 A.X K1은 최근 추세대로 '전문가 혼합(MoE)' 아키텍처를 채택했다. 모델 학습에는 6850억개의 매개변수를 통해 뛰어난 성능을 갖추지만, 사용자의 쿼리에는 330억개의 매개변수만 활성화되는 구조다. 이를 통해 효율성까지 확보했다는 설명이다. 이를 통해 복잡한 문맥 이해와 정교한 추론이 가능하다는 설명이다. 수학 추론과 고난도 코딩, AI 에이전트 작업까지 수행할 수 있다고 강조했다. 또 처음부터 한국어로 학습, 국내 문화와 경제, 역사를 잘 이해하는 국민 맞춤형 서비스 구현에 최적화됐다고 전했다. 여기에 '증류(Distillation)'를 활용하면 A.X K1이 70B 이하 소형 모델에 지식을 공급하는 '교사(Teacher) 모델'로서 역할을 수행할 것으로 기대된다고 덧붙였다. A.X K1은 SKT의 AI 서비스 '에이닷'에 탑재돼 통화 서비스와 문자, 웹, 앱 등 다양한 채널로 일반 사용자에게 제공된다. 오픈 소스와 API도 공개해 '모두의 AI'를 실현한다는 계획이다. 산업 현장 적용도 추진된다. SK하이닉스와 SK이노베이션, SK AX, SK브로드밴드 등 주요 관계사를 포함한 20여개 기관이 현장 적용에 참여할 예정이다. 한편, 정부의 독자 AI 파운데이션 모델 프로젝트에서 SKT 컨소시엄은 ▲SKT ▲크래프톤 ▲포티투닷(42dot) ▲리벨리온 ▲라이너 ▲셀렉트스타 ▲서울대학교 ▲KAIST 등 8개 기관으로 구성됐다. 여기에서 라이너는 1100만명 사용자 기반 전문지식 검색 기술로 정확성을, 셀렉트스타는 대규모 데이터 구축·검증 기술로 신뢰성을 확보했다. 크래프톤은 글로벌 멀티모달 R&D 경험으로 확장성을, 포티투닷은 온디바이스 AI 기술로 범용성을, 리벨리온은 국산 신경망처리장치(NPU) 기술로 효율성을 담당했다. 이처럼 새로운 모델 구축을 위해 국내 독자 기술 기반 '풀 스택 소버린 AI'를 구현했다고 강조했다. 김태윤 SKT 파운데이션 모델 담당은 “국내 최초 매개변수 500B급 모델 개발로 치열한 글로벌 경쟁 속에서 대한민국의 글로벌 AI 3강 도약을 위한 새로운 전환점을 마련했다”라며 ”앞으로도 국가대표 AI 기업으로서 모두의 AI를 달성하기 위한 노력을 지속할 것”이라고 말했다. 김해원 기자 hwkim@aitimes.com",
    "title": "﻿SKT, 국내 최대 5000억 매개변수 모델 'A.X K1' 공개 - AI타임스",
    "image": "https://cdn.aitimes.com/news/photo/202512/205137_206658_735.png",
    "url": "https://www.aitimes.com/news/articleView.html?idxno=205137"
  },
  "_publication": {
    "edition_name": "제9호",
    "edition_code": "251229_9",
    "released_at": null,
    "published_at": "2025-12-29T06:33:54.903863+00:00",
    "status": "preview",
    "firestore_synced": true
  },
  "_analysis": {
    "summary": "SK텔레콤이 국내 최대 규모인 5000억(500B) 매개변수를 가진 LLM 'A.X K1'을 공개했다. 전문가 혼합(MoE) 구조를 채택하여 6850억 개의 매개변수 중 330억 개만 활성화하는 방식으로 효율성을 높였다. 이는 정부의 독자 AI 프로젝트 일환으로 개발되었으며, 한국어와 국내 문화에 최적화된 '소버린 AI'를 지향한다.",
    "title_ko": "SKT, 국내 최대 5000억 매개변수 모델 'A.X K1' 공개",
    "mll_raw": {
      "ZES_Raw_Metrics": {
        "Noise": {
          "P3": 1,
          "P2": 3,
          "Rationale": "'국내 최대', '도약' 등의 표현이 있으나 팩트에 기반한 수식어임. 비교 대상(중국 모델 등)을 명시하고 글로벌 모델 대비 규모가 작음을 인정하여 왜곡이 적음.",
          "P1": 2
        },
        "Utility": {
          "V2": 8,
          "V1": 6,
          "Rationale": "국내 AI 시장에서의 상징성과 에이닷 서비스 즉시 적용 가능성(V2)이 높음. 국내 기업의 고용량 MoE 모델 상세 스펙 공개는 희소성이 있음(V3).",
          "V3": 8
        },
        "Signal": {
          "T2": 8,
          "T3": 8,
          "Rationale": "매개변수(500B, 5190억), MoE 활성화 파라미터(33B) 등 기술적 제원이 매우 구체적이며(T1), 작동 원리(Distillation, MoE)에 대한 설명이 충실함(T2).",
          "T1": 9
        },
        "Fine_Adjustment": {
          "Score": 0.5,
          "Reason": "보도자료 기반 기사임에도 불구하고, 단순 홍보를 넘어 기술적 아키텍처(MoE)와 구체적 수치를 명확히 전달하여 정보 가치가 높음."
        }
      },
      "Article_ID": "b61bdb434bd7",
      "IS_Analysis": {
        "Calculations": {
          "IW_Analysis": {
            "Inputs": {
              "Se_Tier": 0,
              "Pe_Entity_Name": "SK Telecom",
              "PE/SE 선정이유": "PE: 모델 개발 및 출시를 주도하고 자금을 투입한 주체(P4). SE: 기사 내 파트너사(크래프톤 등)가 나열되어 있으나 공동 협력 관계이며 단일한 대립/수혜 대상이 부재하여 None 처리.",
              "Se_Entity_Name": "None",
              "Pe_Selection_Rule": "P4",
              "Pe_Tier": 3
            },
            "IW_Score": 1,
            "Tier_Score": 1,
            "Gap_Score": 0
          },
          "IE_Analysis": {
            "Inputs": {
              "SOTA_Check_Result": "False",
              "Criticality_C1_Provenness": 1,
              "Criticality_Total": 1.5,
              "Y_Evidence_Code": 3,
              "Criticality_C2_Societal_Weight": 0.5,
              "Scope_Matrix_Score": 1.5,
              "X_Magnitude_Code": 2
            },
            "IE_Score": 3
          }
        },
        "Score_Commentary": "국내 통신사(Tier 3)가 정부 국책 과제(Sovereign AI)의 일환으로 국내 최대 규모 모델을 출시한 사례. 글로벌 Big Tech 대비 규모는 작으나, 국내 인프라(National)에 미치는 영향과 MoE 기술 적용의 구체성이 입증됨."
      },
      "Meta": {
        "Tags": [
          "Launch",
          "Model",
          "SK"
        ],
        "Summary": "SK텔레콤이 국내 최대 규모인 5000억(500B) 매개변수를 가진 LLM 'A.X K1'을 공개했다. 전문가 혼합(MoE) 구조를 채택하여 6850억 개의 매개변수 중 330억 개만 활성화하는 방식으로 효율성을 높였다. 이는 정부의 독자 AI 프로젝트 일환으로 개발되었으며, 한국어와 국내 문화에 최적화된 '소버린 AI'를 지향한다.",
        "Headline": "SKT, 국내 최대 5000억 매개변수 모델 'A.X K1' 공개",
        "Specification_Version": "v 1.0.0"
      }
    },
    "tags": [
      "Launch",
      "Model",
      "SK"
    ],
    "evidence": {
      "raw_metrics": {
        "Noise": {
          "P3": 1,
          "P2": 3,
          "Rationale": "'국내 최대', '도약' 등의 표현이 있으나 팩트에 기반한 수식어임. 비교 대상(중국 모델 등)을 명시하고 글로벌 모델 대비 규모가 작음을 인정하여 왜곡이 적음.",
          "P1": 2
        },
        "Utility": {
          "V2": 8,
          "V1": 6,
          "Rationale": "국내 AI 시장에서의 상징성과 에이닷 서비스 즉시 적용 가능성(V2)이 높음. 국내 기업의 고용량 MoE 모델 상세 스펙 공개는 희소성이 있음(V3).",
          "V3": 8
        },
        "Signal": {
          "T2": 8,
          "T3": 8,
          "Rationale": "매개변수(500B, 5190억), MoE 활성화 파라미터(33B) 등 기술적 제원이 매우 구체적이며(T1), 작동 원리(Distillation, MoE)에 대한 설명이 충실함(T2).",
          "T1": 9
        },
        "Fine_Adjustment": {
          "Score": 0.5,
          "Reason": "보도자료 기반 기사임에도 불구하고, 단순 홍보를 넘어 기술적 아키텍처(MoE)와 구체적 수치를 명확히 전달하여 정보 가치가 높음."
        }
      },
      "breakdown": {
        "Noise": {
          "P3": 1.0,
          "N_Avg": 2.0,
          "P2": 3.0,
          "P1": 2.0
        },
        "ZS_Final": 3.5,
        "Signal": {
          "T2": 8.0,
          "S_Avg": 8.33,
          "T3": 8.0,
          "T1": 9.0
        },
        "Utility": {
          "V2": 8.0,
          "V1": 6.0,
          "U_Avg": 7.33,
          "V3": 8.0
        },
        "Fine_Adjustment": 0.5,
        "ZS_Raw": 3.51
      }
    },
    "zero_echo_score": 3.5,
    "impact_score": 4.0,
    "analyzed_at": "2025-12-29T01:47:18.863696+00:00"
  },
  "_header": {
    "state": "PUBLISHED",
    "updated_at": "2025-12-29T06:33:55.299527+00:00",
    "article_id": "b61bdb434bd7",
    "state_history": [
      {
        "by": "crawler",
        "at": "2025-12-29T01:41:27.770287+00:00",
        "state": "COLLECTED"
      },
      {
        "by": "analyzer",
        "at": "2025-12-29T01:47:19.506552+00:00",
        "state": "ANALYZED"
      },
      {
        "state": "CLASSIFIED",
        "at": "2025-12-29T01:56:16.049642+00:00",
        "by": "desk"
      },
      {
        "by": "publisher",
        "at": "2025-12-29T06:33:55.299527+00:00",
        "state": "PUBLISHED"
      }
    ],
    "version": "2.0"
  },
  "id": "b61bdb434bd7"
}