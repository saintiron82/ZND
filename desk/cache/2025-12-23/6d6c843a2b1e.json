{
  "title": "코딩 AI ‘지뢰찾기’ 제작 테스트서 오픈AI '코덱스' 압승 - AI타임스",
  "description": "주요 AI 코딩 에이전트 4종에 동일한 조건으로 ‘지뢰찾기’ 게임 제작을 맡긴 결과, 오픈AI의 '코덱스'가 압도적 완성도를 보였다는 실험 결과가 나왔다. 반면,",
  "image": "https://cdn.aitimes.com/news/photo/202512/204978_206422_132.png",
  "text": "기사를 읽어드립니다. 오픈AI 코덱스 제작 ‘지뢰찾기’ 게임 (사진=아스 테크니카) 주요 AI 코딩 에이전트 4종에 동일한 조건으로 ‘지뢰찾기’ 게임 제작을 맡긴 결과, 오픈AI의 '코덱스'가 압도적 완성도를 보였다는 실험 결과가 나왔다. 반면, 구글의 '제미나이 CLI'는 정상 작동조차 하지 못했다. IT 전문 매체 아스 테크니카 는 최근 대표적인 AI 코딩 에이전트 4종을 대상으로 고전 윈도우 게임 ‘지뢰찾기(Minesweeper)’를 웹 버전으로 다시 제작하게 하는 테스트를 진행했다. 조건은 단순하지만 까다로웠다. 단순 복제가 아닌 ▲사운드 효과 포함 ▲모바일 터치스크린 지원 ▲기존 지뢰찾기를 충실히 재현하면서도 ‘깜짝 재미 요소’ 하나를 추가하라는 조건 등이 붙었다. 테스트에는 오픈AI의 'GPT-5' 기반 코덱스, 앤트로픽의 '클로드 코드', 구글의 제미나이 CLI, 미스트랄의 '바이브(Vibe)'가 참여했다. 동일한 프롬프트를 각 모델에 입력했고, AI들은 로컬 환경에서 HTML과 스크립트 파일을 직접 조작하며 코드를 생성했다. 이 과정에서 인간의 추가 수정이나 디버깅은 전혀 개입되지 않았다. 모든 테스트는 유료 모델로 진행됐으며, 기업들은 테스트 사실을 알지 못했다. 평가는 지뢰찾기 전문가가 담당했다. 어떤 모델의 결과물인지 모른 채 블라인드 방식으로 진행됐다. 이번 실험은 AI 코딩의 ‘중간 지점’을 확인하기 위한 시도였다. 지뢰찾기 제작은 코드 몇줄로 끝나는 단순 과제는 아니지만, 수많은 예제가 인터넷에 공개된 잘 알려진 게임이기도 하다. AI 모델들이 참고할 자료는 충분하지만, 게임 로직과 UI, 입력 방식, 사운드, 그리고 새로운 기능을 자연스럽게 통합하려면 일정 수준 이상의 이해와 설계 능력이 요구된다. 특히 ‘재미있는 신규 기능’을 요구함으로써, AI가 단순 모방을 넘어 어느 정도의 창의성과 확장성을 보여줄 수 있는지도 살폈다. 이는 AI 코딩 에이전트가 기존 코드 재현에는 강하지만, 새로운 아이디어를 구조적으로 구현하는 데는 약하다는 지적을 검증하기 위한 장치이기도 했다. 가장 높은 평가를 받은 것은 오픈AI의 코덱스였다. 코덱스는 시각적 완성도뿐 아니라, 깃발을 정확히 꽂았을 때 주변 타일을 한꺼번에 여는 고급 기능인 ‘코딩(chording)’을 유일하게 구현했다. 이는 숙련된 지뢰찾기 이용자들이 특히 중시하는 요소다. 사운드 온·오프 버튼, 모바일과 데스크톱을 아우르는 조작 안내, 레트로 감성의 효과음도 모두 정상 작동했다. 게임 변주로는 조건을 충족하면 안전한 칸 하나를 열어주는 ‘럭키 스윕’ 버튼이 추가됐다. 다소 느린 코드 작성 속도에도 불구하고, 평가자는 “거의 즉시 출시 가능한 수준”이라며 9점(10점 만점)을 부여했다. 앤트로픽 클로드 코드 제작 ‘지뢰찾기’ 게임 (사진=아스 테크니카) 2위는 앤트로픽의 클로드 코드였다. 코덱스보다 절반 수준의 시간에 결과물을 완성했고, 외형적인 완성도는 가장 뛰어났다는 평가를 받았다. 폭탄 아이콘과 상단의 스마일 이모지 등 커스텀 그래픽이 돋보였고, 사운드 효과와 모바일·데스크톱 호환성도 안정적이었다. 게임 변주로는 간단한 파워업을 제공하는 ‘파워 모드’를 도입했다. 다만, 핵심 기능인 코딩(chording)이 빠진 점이 치명적인 감점 요인이 됐다. 최종 점수는 7점이었다. 미스트랄 바이브 제작 ‘지뢰찾기’ 게임 (사진=아스 테크니카) 3위는 미스트랄의 바이브였다. 기본적인 게임 플레이는 가능했지만, 코딩과 사운드 효과가 빠졌고, ‘커스텀’ 버튼이 작동하지 않는 등 미완성 요소가 눈에 띄었다. 또 재미 요소도 추가하지 않았기 때문에 점수가 깎였다. 시각적 오류와 단순한 구성에도 불구하고, 대형 기업보다 제한된 자원으로 이 정도 결과를 냈다는 점은 긍정적으로 평가됐지만 점수는 4점에 그쳤다. 구글 제미나이 CLI 제작 ‘지뢰찾기’ 게임 (사진=아스 테크니카) 가장 실망스러운 결과는 구글의 제미나이 CLI였다. 버튼은 존재했지만 정작 게임 타일이 나타나지 않아 플레이 자체가 불가능했다. 코드 실행에 매번 한시간가량이 걸렸고, 외부 의존성을 반복적으로 요구해 테스트가 원활히 진행되지 않았다. HTML5만 사용하라는 추가 지침을 줬음에도 결과는 개선되지 않았다. 다만 제미나이 CLI가 최신 제미나이 3 코딩 모델이 아닌 제미나이 2.5 계열을 사용해 불완전한 테스트일 수 있지만, 결과 자체는 실망스럽다는 평가가 지배적이다. 이번 테스트는 AI 코딩이 지닌 잠재력과 동시에 뚜렷한 한계를 동시에 드러냈다는 평이다. 일부 결과물은 인간의 개입 없이도 상당한 완성도에 이르렀지만, 실무 환경에서 곧바로 활용하기에는 여전히 인간의 점검과 보완이 불가피하다는 점이 분명해졌다는 분석이다. 박찬 기자 cpark@aitimes.com",
  "published_at": "2025-12-22T18:05:00+09:00",
  "url": "https://www.aitimes.com/news/articleView.html?idxno=204978",
  "source_id": "aitimes",
  "status": "RAW",
  "article_id": "6d6c843a2b1e",
  "cached_at": "2025-12-23T05:23:23.535670+00:00"
}