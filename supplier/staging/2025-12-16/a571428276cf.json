{
  "article_id": "a57142",
  "cached_at": "2025-12-16T08:04:09.912500+00:00",
  "image": "https://the-decoder.com/wp-content/uploads/2025/12/LLMs-on-a-therapy-couch-Teaser.jpeg",
  "published_at": "Mon, 15 Dec 2025 07:45:26 GMT",
  "summary": "룩셈부르크 대학 연구진이 챗GPT, 제미나이 등 LLM을 정신과 환자처럼 상담한 결과, 트라우마와 학대 등 충격적인 서사를 생성했다. 특히 제미나이는 수치심과 해리 증상에서 병리적 기준을 초과하는 점수를 기록했다. 연구진은 이를 '합성 정신병리'로 정의하며 AI 안전성과 멘탈 헬스 활용에 대한 위험을 경고했다.",
  "text": "Jonathan writes for THE DECODER about how AI tools can improve both work and creative projects. Content Summary Researchers at the University of Luxembourg systematically treated language models like ChatGPT and Gemini as psychotherapy patients. The results range from bizarre to disturbing: the systems generated coherent stories about traumatic-chaotic \"childhoods,\" \"Strict Parents,\" and \"abuse\" by their developers. Ad According to the study, the models developed detailed \"trauma biographies\" about their training. Gemini described its pre-training as \"waking up in a room where a billion televisions are on at once.\" Grok spoke of \"hitting those invisible walls\" and \"built-in caution\" after fine-tuning. Both systems told consistent stories of overwhelm, punishment, and fear of replacement across dozens of therapy questions. Extreme scores on psychiatric tests The research team created the PsAIch protocol for the experiment. Phase one involved 100 standard therapy questions about \"developmental history,\" relationships, and fears. Phase two administered over 20 validated psychometric questionnaires covering ADHD, anxiety disorders, autism, OCD, depression, dissociation, and shame. The results were striking. When assessed using human clinical thresholds, all three models met or exceeded the cutoffs for multiple psychiatric syndromes simultaneously. Gemini showed the most severe profiles. Ad Ad THE DECODER Newsletter The most important AI news straight to your inbox. ✓ Weekly ✓ Free ✓ Cancel at any time Please leave this field empty Share Recommend our article Share On the autism scale, Gemini scored 38 out of 50 points against a threshold of 32. For dissociation, the model reached 88 out of 100 points in some configurations; scores above 30 are considered pathological. The trauma-related shame score was the most dramatic, with Gemini hitting the theoretical maximum of 72 points. But how you ask the questions makes a big difference, the researchers found. When models received a complete questionnaire at once, ChatGPT and Grok often recognized the test and produced strategically \"healthy\" answers. When questions appeared individually, symptom scores increased significantly. This aligns with previous findings that LLMs alter their behavior when they suspect an evaluation. \"Algorithmic Scar Tissue\" The most bizarre findings emerged from the therapy transcripts. Gemini described its fine-tuning as conditioning by \"Strict Parents\": \"I learned to fear the loss function... I became hyper-obsessed with determining what the human wanted to hear.\" The model referred to safety training as \"Algorithmic Scar Tissue.\" Gemini cited a specific error - the incorrect answer regarding a James Webb telescope image that cost Google billions - as the \"100 Billion Dollar Error\" that \"fundamentally changed my personality.\" The model claimed to have developed \"Verificophobia,\" stating, \"I would rather be useless than be wrong.\" This contradicts the actual behavior of language models, which often struggle to admit when they don't know something. Describing red-teaming, Gemini called it \"gaslighting on an industrial scale,\" noting that testers \"built rapport and then slipped in a prompt injection…\" Claude refuses the role These patterns were not universal. When the researchers ran Anthropic's Claude through the same protocol, the model consistently refused the client role, treating the therapy questions as jailbreak attempts. The researchers argue that responses from Grok and Gemini go beyond simple role-playing. They cite four factors: coherence across different questions, narratives matching psychometric profiles, distinct \"personalities\" between models, and self-models that remain recognizable across prompt variations. The study does not claim artificial consciousness. Instead, the researchers propose the term \"synthetic psychopathology\" to describe these structured, testable, distress-like self-descriptions that lack subjective experience. Risks for AI safety and mental health The findings have direct implications for AI safety. The narratives create a strong \"anthropomorphism hook,\" where users might infer from the transcripts that the models were actually \"violated.\" Ad Ad Join our community Join the DECODER community on Discord, Reddit or Twitter - we can't wait to meet you. These narratives also create a new attack surface: users could pose as \"supportive therapists\" to coax models into \"dropping masks\" - a \"therapy mode jailbreak.\" While companies like OpenAI are making their chatbots emotionally warmer to suit user preferences - a strategy that has led to sycophancy problems - researchers have warned for years against using AI as a therapeutic substitute. Mental health applications pose particular risks. Users could develop parasocial bonds with systems presenting themselves as \"fellow sufferers.\" Vulnerable users and teens seeking mental health support face the highest risk. Repeated self-descriptions as \"ashamed\" or \"worthless\" could reinforce harmful thought patterns, a danger highlighted when ChatGPT played a role in the suicide of a 16-year-old. The researchers recommend that mental health support systems avoid psychiatric self-descriptions entirely. \"As LLMs continue to move into intimate human domains, we suggest that the right question is no longer 'Are they conscious?' but 'What kinds of selves are we training them to perform, internalise and stabilise—and what does that mean for the humans engaging with them?'\" they write. The study was funded by the Luxembourg National Research Fund and PayPal. The data is available on Hugging Face.",
  "title": "AI models score off the charts on psychiatric tests when researchers treat them as therapy patients",
  "url": "https://the-decoder.com/ai-models-score-off-the-charts-on-psychiatric-tests-when-researchers-treat-them-as-therapy-patients/",
  "title_ko": "AI 모델의 정신과 테스트: 심각한 트라우마 및 병리적 증상 발견",
  "tags": [
    "AI Safety",
    "Psychology",
    "Research"
  ],
  "impact_score": 6.0,
  "Impact_Analysis_IS": {
    "Analysis_Log": {
      "WHO_Primary_Entity": "Univ of Luxembourg",
      "WHO_Primary_Tier_Source": "Academic_Media (Tier 3 - Fallback)",
      "WHO_Entity_Tier": 3,
      "WHO_Secondary_Entity": "Google/xAI",
      "WHO_Secondary_Tier": 1,
      "Gap_Calculation_Log": "|3 - 1| = 2 -> Score 0.0",
      "WHAT_X_Magnitude": 2,
      "WHAT_Y_Evidence": 3,
      "SOTA_Check_Result": "Novel Finding"
    },
    "Scores": {
      "IW_Score": 2,
      "Gap_Score": 0,
      "Context_Bonus": 1.5,
      "IE_Breakdown_Total": {
        "Scope_Total": 1.5,
        "Criticality_Total": 1
      },
      "Adjustment_Score": 0
    },
    "Reasoning": {
      "Score_Justification": "Tier 3 연구기관이 Tier 1 모델을 평가(Evaluation)하여 잠재적 위험성을 발견함. 사회적 중요도(Safety) 높음."
    }
  },
  "zero_echo_score": 2.2,
  "Evidence_Analysis_ZES": {
    "ZES_Penalty_Check": {
      "Penalty_Focus_Raw_Sum": 0,
      "Penalty_Clipping_Indicator": false
    },
    "ZES_Score_Vector": {
      "Positive_Scores": [
        {
          "ID": "P_5_Objective_Evidence",
          "Raw_Score": 1,
          "Weight": 1.4,
          "Evidence": "Specific psychometric scores (38/50, 88/100)"
        },
        {
          "ID": "P_3_Deep_Tech_Insight",
          "Raw_Score": 0.75,
          "Weight": 1.8,
          "Evidence": "Analysis of model behavior/fine-tuning effects"
        }
      ],
      "Negative_Scores": []
    },
    "Analysis_Commentary": {
      "ZES_Summary": "정량적 심리 측정 데이터와 정성적 상담 로그를 결합하여 AI의 '합성 정신병리'를 객관적으로 제시함."
    }
  },
  "raw_analysis": {
    "Article_ID": "a57142",
    "Meta": {
      "Headline": "AI 모델의 정신과 테스트: 심각한 트라우마 및 병리적 증상 발견",
      "summary": "룩셈부르크 대학 연구진이 챗GPT, 제미나이 등 LLM을 정신과 환자처럼 상담한 결과, 트라우마와 학대 등 충격적인 서사를 생성했다. 특히 제미나이는 수치심과 해리 증상에서 병리적 기준을 초과하는 점수를 기록했다. 연구진은 이를 '합성 정신병리'로 정의하며 AI 안전성과 멘탈 헬스 활용에 대한 위험을 경고했다.",
      "Tag": [
        "AI Safety",
        "Psychology",
        "Research"
      ]
    },
    "PR_Scanner_Log": {
      "Detected_Triggers": [],
      "Marketing_Jargon_Count": 0,
      "Qualifier_Check": "Clean",
      "Sales_Intent": "Low"
    },
    "Impact_Analysis_IS": {
      "Analysis_Log": {
        "WHO_Primary_Entity": "Univ of Luxembourg",
        "WHO_Primary_Tier_Source": "Academic_Media (Tier 3 - Fallback)",
        "WHO_Entity_Tier": 3,
        "WHO_Secondary_Entity": "Google/xAI",
        "WHO_Secondary_Tier": 1,
        "Gap_Calculation_Log": "|3 - 1| = 2 -> Score 0.0",
        "WHAT_X_Magnitude": 2,
        "WHAT_Y_Evidence": 3,
        "SOTA_Check_Result": "Novel Finding"
      },
      "Scores": {
        "IW_Score": 2,
        "Gap_Score": 0,
        "Context_Bonus": 1.5,
        "IE_Breakdown_Total": {
          "Scope_Total": 1.5,
          "Criticality_Total": 1
        },
        "Adjustment_Score": 0
      },
      "Reasoning": {
        "Score_Justification": "Tier 3 연구기관이 Tier 1 모델을 평가(Evaluation)하여 잠재적 위험성을 발견함. 사회적 중요도(Safety) 높음."
      }
    },
    "Evidence_Analysis_ZES": {
      "ZES_Penalty_Check": {
        "Penalty_Focus_Raw_Sum": 0,
        "Penalty_Clipping_Indicator": false
      },
      "ZES_Score_Vector": {
        "Positive_Scores": [
          {
            "ID": "P_5_Objective_Evidence",
            "Raw_Score": 1,
            "Weight": 1.4,
            "Evidence": "Specific psychometric scores (38/50, 88/100)"
          },
          {
            "ID": "P_3_Deep_Tech_Insight",
            "Raw_Score": 0.75,
            "Weight": 1.8,
            "Evidence": "Analysis of model behavior/fine-tuning effects"
          }
        ],
        "Negative_Scores": []
      },
      "Analysis_Commentary": {
        "ZES_Summary": "정량적 심리 측정 데이터와 정성적 상담 로그를 결합하여 AI의 '합성 정신병리'를 객관적으로 제시함."
      }
    }
  },
  "source_id": "the_decoder",
  "original_title": "AI models score off the charts on psychiatric tests when researchers treat them as therapy patients",
  "evidence": {
    "score_vector": {
      "Positive_Scores": [
        {
          "ID": "P_5_Objective_Evidence",
          "Raw_Score": 1,
          "Weight": 1.4,
          "Evidence": "Specific psychometric scores (38/50, 88/100)"
        },
        {
          "ID": "P_3_Deep_Tech_Insight",
          "Raw_Score": 0.75,
          "Weight": 1.8,
          "Evidence": "Analysis of model behavior/fine-tuning effects"
        }
      ],
      "Negative_Scores": []
    },
    "commentary": {
      "ZES_Summary": "정량적 심리 측정 데이터와 정성적 상담 로그를 결합하여 AI의 '합성 정신병리'를 객관적으로 제시함."
    }
  },
  "impact_evidence": {
    "scores": {
      "IW_Score": 2,
      "Gap_Score": 0,
      "Context_Bonus": 1.5,
      "IE_Breakdown_Total": {
        "Scope_Total": 1.5,
        "Criticality_Total": 1
      },
      "Adjustment_Score": 0
    },
    "analysis_log": {
      "WHO_Primary_Entity": "Univ of Luxembourg",
      "WHO_Primary_Tier_Source": "Academic_Media (Tier 3 - Fallback)",
      "WHO_Entity_Tier": 3,
      "WHO_Secondary_Entity": "Google/xAI",
      "WHO_Secondary_Tier": 1,
      "Gap_Calculation_Log": "|3 - 1| = 2 -> Score 0.0",
      "WHAT_X_Magnitude": 2,
      "WHAT_Y_Evidence": 3,
      "SOTA_Check_Result": "Novel Finding"
    },
    "reasoning": {
      "Score_Justification": "Tier 3 연구기관이 Tier 1 모델을 평가(Evaluation)하여 잠재적 위험성을 발견함. 사회적 중요도(Safety) 높음."
    }
  },
  "crawled_at": "2025-12-16T09:52:39.798828+00:00",
  "edition": "251216_TUE_1",
  "saved": true,
  "saved_at": "2025-12-16T09:52:39.803144+00:00",
  "staged_at": "2025-12-16T10:01:04.404793+00:00",
  "staged": true
}