{
  "_header": {
    "version": "2.0",
    "article_id": "7afeab6ca22a",
    "state": "PUBLISHED",
    "state_history": [
      {
        "state": "COLLECTED",
        "at": "2025-12-28T06:51:36.865136+00:00",
        "by": "crawler"
      },
      {
        "state": "PUBLISHED",
        "at": "2025-12-28T17:10:56.153913+00:00",
        "by": "publisher"
      }
    ],
    "updated_at": "2025-12-28T17:10:56.153913+00:00"
  },
  "_original": {
    "title": "Meta brings Segment Anything to audio, letting editors pull sounds from video with a click or text prompt",
    "description": "Filtering a dog bark from street noise or isolating a sound source with a single click on a video: Meta's SAM Audio brings the company's visual segmentation approach to the audio world. The model lets users edit audio using text commands, clicks, or time markers. Code and weights are open source.",
    "image": "https://the-decoder.com/wp-content/uploads/2025/12/meta-segment-anything-audio-train.jpg",
    "text": "Ask about this article… Search After images and 3D models, Meta is bringing its \"Segment Anything\" approach to audio. The new AI model SAM Audio isolates individual sound sources from complex mixtures using text commands, time markers, or visual clicks. Meta says the system is the first unified model that handles this task across different input methods. Instead of requiring separate tools for each use case, it responds flexibly to whatever type of command users throw at it. The system offers three control methods that can be combined. Users can type text commands like \"dog barking\" or \"singing voice\" to isolate specific sounds. They can click directly on an object or person in a video to pull out the matching audio. Or they can use time markers—called span prompts—to pinpoint segments where a target sound occurs. Ad Meta sees potential applications in music production, podcasting, and film editing; filtering traffic noise from an exterior shot, for example, or separating instruments in a recording. Ad DEC_D_Incontent-1 Perception Encoder bridges image and sound SAM Audio runs on a generative modeling framework using a flow-matching diffusion transformer. This processes the audio mix alongside input commands to generate both the desired audio track and the residual sounds. A key component is the Perception Encoder Audiovisual (PE-AV). This model builds on the Perception Encoder released in April and extends its computer vision capabilities to audio. Meta describes PE-AV as the \"ears\" that work with SAM Audio's \"brain\" to tackle complex segmentation tasks. Ad The system extracts features at the individual frame level and aligns them precisely with audio signals. This tight temporal sync lets SAM Audio separate sound sources that are visually anchored, like a speaker visible on screen. Without this synchronization, the model would lack the fine visual understanding needed to isolate sounds flexibly and accurately. Meta says PE-AV was trained on more than 100 million videos. The model scales efficiently and comes in variants ranging from 500 million to 3 billion parameters. The developers say processing speed exceeds real-time. Ad DEC_D_Incontent-2 New benchmarks for audio separation Meta is rolling out two new tools to measure model performance: SAM Audio-Bench and SAM Audio Judge. Traditional audio separation metrics often require clean reference tracks for comparison, something that's rarely available in real-world scenarios. Ad SAM Audio Judge is an automatic evaluation model that assesses segmentation quality without a reference track. It's designed to mimic human perception, scoring criteria like fidelity and precision. This makes it particularly useful for benchmarks that aim to reflect actual listening experiences. The SAM Audio Benchmark covers different audio domains including speech, music, and sound effects. Unlike previous datasets, it uses real audio and video sources instead of purely synthetic mixes, providing a more realistic foundation for evaluation. Limitations and availability SAM Audio doesn't accept audio files as prompts yet. Separating very similar sounds - like isolating a single singer from a choir or picking out one instrument from an orchestra - also remains a challenge, Meta says. The model is available to try in the Segment Anything Playground, and Meta has published code and weights. The company is also partnering with Starkey, a US hearing aid manufacturer, and the startup incubator 2gether-International to explore accessibility applications. Meta recently introduced SAM 3, the third generation of its segmentation model, which analyzes images and videos using open text prompts instead of rigid categories. The system features Promptable Concept Segmentation to flexibly isolate visual concepts. Alongside it, SAM 3D was released, reconstructing spatial objects and human poses from simple 2D images and expanding the AI's understanding of the physical world.",
    "published_at": "2025-12-26T18:48:35+00:00",
    "url": "https://the-decoder.com/meta-brings-segment-anything-to-audio-letting-editors-pull-sounds-from-video-with-a-click-or-text-prompt/",
    "source_id": "the_decoder",
    "crawled_at": "2025-12-28T06:51:36.865136+00:00"
  },
  "_analysis": {
    "title_ko": "Meta, 오디오 분할 모델 'SAM Audio' 공개... 클릭으로 소리 편집",
    "summary": "Meta가 이미지 분할 모델(SAM)을 오디오로 확장한 'SAM Audio'를 공개. 텍스트, 클릭 등으로 비디오 내 특정 소리를 분리/제거 가능. 코드와 가중치를 오픈소스로 공개하여 접근성을 높임.",
    "tags": [
      "Audio",
      "Meta",
      "Open Source"
    ],
    "impact_score": 6.0,
    "zero_echo_score": 3.2,
    "analyzed_at": "2025-12-28T06:58:04.892132+00:00",
    "mll_raw": {
      "Article_ID": "7afeab6ca22a",
      "Meta": {
        "Specification_Version": "v 1.0.0",
        "Headline": "Meta, 오디오 분할 모델 'SAM Audio' 공개... 클릭으로 소리 편집",
        "Summary": "Meta가 이미지 분할 모델(SAM)을 오디오로 확장한 'SAM Audio'를 공개. 텍스트, 클릭 등으로 비디오 내 특정 소리를 분리/제거 가능. 코드와 가중치를 오픈소스로 공개하여 접근성을 높임.",
        "Tags": [
          "Audio",
          "Meta",
          "Open Source"
        ]
      },
      "IS_Analysis": {
        "Score_Commentary": "Meta(T2)가 멀티모달 AI의 기술적 패러다임을 오디오로 확장(X3 Paradigm Shift/Industry). 즉시 사용 가능한 오픈소스(Weights Available)로 공개되어 실질적 파급력(Y4)이 매우 높음. ",
        "Calculations": {
          "IW_Analysis": {
            "Inputs": {
              "Pe_Selection_Rule": "P4",
              "Pe_Entity_Name": "Meta",
              "Pe_Tier": 2,
              "Se_Entity_Name": "None",
              "Se_Tier": 0,
              "PE/SE_Rationale": "모델 개발 및 공개 주체 Meta(P4). 독자 공개이므로 SE 없음."
            },
            "Tier_Score": 2,
            "Gap_Score": 0,
            "IW_Score": 2
          },
          "IE_Analysis": {
            "Inputs": {
              "X_Magnitude_Code": 3,
              "Y_Evidence_Code": 4,
              "Scope_Matrix_Score": 2.5,
              "Criticality_C1_Provenness": 1,
              "Criticality_C2_Societal_Weight": 0.5,
              "Criticality_Total": 1.5,
              "SOTA_Check_Result": "True"
            },
            "IE_Score": 4
          }
        }
      },
      "ZES_Raw_Metrics": {
        "Signal": {
          "T1": 7,
          "T2": 8,
          "T3": 8,
          "Rationale": "모델 아키텍처(PE-AV), 학습 데이터 규모, 벤치마크 도구 등 상세 정보 포함."
        },
        "Noise": {
          "P1": 2,
          "P2": 1,
          "P3": 1,
          "Rationale": "기술 사양 중심의 명확한 서술."
        },
        "Utility": {
          "V1": 7,
          "V2": 9,
          "V3": 7,
          "Rationale": "오픈소스 공개로 즉시 활용 가능(V2 만점 가까움)."
        },
        "Fine_Adjustment": {
          "Score": 0.5,
          "Reason": "실제 개발자에게 높은 효용을 주는 정보."
        }
      }
    }
  },
  "_classification": {
    "category": "AI/ML",
    "is_selected": true,
    "classified_at": "2025-12-28T07:59:56.918304+00:00",
    "classified_by": "desk_user",
    "edition_code": null,
    "edition_name": null,
    "published_at": null,
    "released_at": null
  },
  "_publication": {
    "edition_code": "251228_6",
    "edition_name": "제6호",
    "published_at": "2025-12-28T12:06:49.944357+00:00",
    "released_at": null,
    "firestore_synced": true
  }
}