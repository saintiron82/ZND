# -*- coding: utf-8 -*-
"""
ZND Crawler Scheduler - PM2 Entry Point
APScheduler ê¸°ë°˜ ë…ë¦½ ìŠ¤ì¼€ì¤„ëŸ¬

ì‹¤í–‰: python scheduler.py
PM2: pm2 start ecosystem.config.js
"""
import os
import sys
import json
import signal
import time
from datetime import datetime

# ê²½ë¡œ ì„¤ì •
CRAWLER_DIR = os.path.dirname(os.path.abspath(__file__))
ZND_ROOT = os.path.dirname(CRAWLER_DIR)

# Add paths for imports
sys.path.insert(0, CRAWLER_DIR)  # for core.xxx
sys.path.insert(0, ZND_ROOT)     # for crawler.xxx (when called from outside)
sys.path.insert(0, os.path.join(ZND_ROOT, 'desk'))  # for desk modules

from apscheduler.schedulers.blocking import BlockingScheduler
from apscheduler.triggers.cron import CronTrigger

# Use relative imports (core.xxx) since we're inside crawler directory
from core.extractor import run_full_pipeline
from core.logger import log_crawl_event

# ì„¤ì • íŒŒì¼ ê²½ë¡œ
CONFIG_DIR = os.path.join(CRAWLER_DIR, 'config')
SCHEDULES_FILE = os.path.join(CONFIG_DIR, 'schedules.json')


def load_schedules() -> list:
    """ìŠ¤ì¼€ì¤„ ì„¤ì • ë¡œë“œ"""
    if not os.path.exists(SCHEDULES_FILE):
        return []
    try:
        with open(SCHEDULES_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('schedules', [])
    except Exception as e:
        print(f"âŒ Failed to load schedules: {e}")
        return []


def save_schedules(schedules: list):
    """ìŠ¤ì¼€ì¤„ ì„¤ì • ì €ì¥"""
    os.makedirs(CONFIG_DIR, exist_ok=True)
    with open(SCHEDULES_FILE, 'w', encoding='utf-8') as f:
        json.dump({'schedules': schedules}, f, indent=2, ensure_ascii=False)


def run_scheduled_crawl():
    """ìŠ¤ì¼€ì¤„ì— ì˜í•´ í˜¸ì¶œë˜ëŠ” í¬ë¡¤ë§ ì‘ì—…"""
    print(f"\n{'='*50}")
    print(f"â° [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Scheduled crawl triggered")
    print(f"{'='*50}\n")
    
    try:
        result = run_full_pipeline()
        log_crawl_event("Scheduled", f"Pipeline completed: {result.get('message', 'OK')}", 0, success=result.get('success', True))
    except Exception as e:
        log_crawl_event("Scheduled", f"Pipeline failed: {str(e)}", 0, success=False)
        print(f"âŒ Scheduled crawl error: {e}")


def create_scheduler() -> BlockingScheduler:
    """ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„± ë° ì‘ì—… ë“±ë¡"""
    scheduler = BlockingScheduler(timezone='Asia/Seoul')
    
    schedules = load_schedules()
    
    if not schedules:
        # ê¸°ë³¸ ìŠ¤ì¼€ì¤„: ë§¤ 6ì‹œê°„ë§ˆë‹¤
        print("ğŸ“‹ No schedules found. Using default: every 6 hours")
        scheduler.add_job(
            run_scheduled_crawl,
            CronTrigger(hour='*/6', minute=0),
            id='default_crawl',
            name='Default 6-hour Crawl'
        )
    else:
        for sched in schedules:
            if not sched.get('enabled', True):
                continue
            
            cron = sched.get('cron', '0 8 * * *')  # ê¸°ë³¸: ë§¤ì¼ 8ì‹œ
            parts = cron.split()
            
            try:
                trigger = CronTrigger(
                    minute=parts[0] if len(parts) > 0 else '0',
                    hour=parts[1] if len(parts) > 1 else '*',
                    day=parts[2] if len(parts) > 2 else '*',
                    month=parts[3] if len(parts) > 3 else '*',
                    day_of_week=parts[4] if len(parts) > 4 else '*'
                )
                
                scheduler.add_job(
                    run_scheduled_crawl,
                    trigger,
                    id=sched.get('id', f"job_{sched.get('name', 'unknown')}"),
                    name=sched.get('name', 'Unnamed')
                )
                print(f"âœ… Registered: {sched.get('name')} ({cron})")
            except Exception as e:
                print(f"âš ï¸ Failed to register schedule '{sched.get('name')}': {e}")
    
    return scheduler


def signal_handler(signum, frame):
    """ì¢…ë£Œ ì‹œê·¸ë„ ì²˜ë¦¬"""
    print("\nğŸ›‘ Shutdown signal received. Stopping scheduler...")
    sys.exit(0)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘       ğŸ• ZND Crawler Scheduler                   â•‘
â•‘       Independent Background Service             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Config Dir: {CONFIG_DIR}
    """)
    
    # ì¢…ë£Œ ì‹œê·¸ë„ ë“±ë¡
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„± ë° ì‹œì‘
    scheduler = create_scheduler()
    
    print("\nğŸ“… Registered Jobs:")
    for job in scheduler.get_jobs():
        print(f"   - {job.name}: {job.trigger}")
    
    print("\nğŸš€ Scheduler is running. Press Ctrl+C to stop.\n")
    
    try:
        scheduler.start()
    except (KeyboardInterrupt, SystemExit):
        print("\nğŸ‘‹ Scheduler stopped.")


if __name__ == '__main__':
    main()
