# -*- coding: utf-8 -*-
"""
Crawler Collector - ë§í¬ ìˆ˜ì§‘ ëª¨ë“ˆ
targets.json ê¸°ë°˜ ìƒˆ ë§í¬ ìˆ˜ì§‘
"""
import os
import sys
import json
import time

# Path setup - must be done before imports
CORE_DIR = os.path.dirname(os.path.abspath(__file__))
CRAWLER_DIR = os.path.dirname(CORE_DIR)
ZND_ROOT = os.path.dirname(CRAWLER_DIR)
DESK_DIR = os.path.join(ZND_ROOT, 'desk')
DESK_SRC_CORE_DIR = os.path.join(DESK_DIR, 'src', 'core')  # firestore_client.py ìœ„ì¹˜

# Add to sys.path
if CRAWLER_DIR not in sys.path:
    sys.path.insert(0, CRAWLER_DIR)
if DESK_DIR not in sys.path:
    sys.path.insert(0, DESK_DIR)
if DESK_SRC_CORE_DIR not in sys.path:
    sys.path.insert(0, DESK_SRC_CORE_DIR)  # firestore_client ì„í¬íŠ¸ìš©

from core.logger import log_crawl_event
from firestore_client import FirestoreClient

# Import from desk/desk_crawler.py
import desk_crawler as desk_crawler


def collect_links() -> dict:
    """
    ëª¨ë“  í™œì„± íƒ€ê²Ÿì—ì„œ ìƒˆ ë§í¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    Returns:
        dict: {success: bool, links: list, message: str}
    """
    start_time = time.time()
    db = FirestoreClient()
    
    try:
        # load_targets returns (settings, targets_list) tuple
        settings, targets = desk_crawler.load_targets()
        all_links = []
        
        # ìºì‹œ ì²´í¬ìš© í•¨ìˆ˜
        from src.core_logic import load_from_cache
        
        for target in targets:
            print(f"ğŸ“¡ [Collect] Fetching from target: {target.get('id')} ({target.get('url')})")
            links = desk_crawler.fetch_links(target)
            print(f"   found {len(links)} raw links")
            
            limit = target.get('limit', 5)
            links = links[:limit]
            
            skipped_history = 0
            skipped_cache = 0
            
            for link in links:
                # 1. íˆìŠ¤í† ë¦¬ ì²´í¬ (ì´ë¯¸ ì²˜ë¦¬ëœ ê²ƒ ì œì™¸: ACCEPTED, REJECTED ë“±)
                is_in_history = db.check_history(link)
                if is_in_history:
                    # print(f"   [Skip] History: {link}")
                    skipped_history += 1
                    continue
                
                # 2. ìºì‹œ ì²´í¬ (ì´ë¯¸ ì¶”ì¶œëœ ê²ƒ ì œì™¸)
                cached = load_from_cache(link)
                if cached and cached.get('text'):
                    # print(f"   [Skip] Cache: {link}")
                    skipped_cache += 1
                    continue
                
                print(f"   âœ… [New] Adding link: {link}")
                all_links.append({
                    'url': link,
                    'source_id': target['id'],
                    'target_name': target.get('name', target['id'])
                })
            
            print(f"   â­ï¸ [{target['id']}] Result: Added={len(links)-skipped_history-skipped_cache}, SkipHistory={skipped_history}, SkipCache={skipped_cache}")
        
        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique_links = []
        for item in all_links:
            if item['url'] not in seen:
                seen.add(item['url'])
                unique_links.append(item)
        
        duration = time.time() - start_time
        msg = f"Collected {len(unique_links)} new links"
        log_crawl_event("Collect", msg, duration, success=True)
        
        print(f"ğŸ“¡ [Collect] ìˆ˜ì§‘ ì™„ë£Œ: {len(unique_links)} ìƒˆ ë§í¬")
        return {
            'success': True,
            'links': unique_links,
            'total': len(unique_links),
            'message': msg
        }
        
    except Exception as e:
        duration = time.time() - start_time
        log_crawl_event("Collect", f"Error: {str(e)}", duration, success=False)
        print(f"âŒ [Collect] Error: {e}")
        return {'success': False, 'error': str(e), 'links': []}
