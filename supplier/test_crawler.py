"""
í…ŒìŠ¤íŠ¸ í¬ë¡¤ëŸ¬ - íŠ¹ì • ì†ŒìŠ¤ IDì—ì„œ ë”± 1ê°œ ê¸°ì‚¬ë§Œ í¬ë¡¤ë§í•˜ëŠ” í…ŒìŠ¤íŠ¸ìš© ìŠ¤í¬ë¦½íŠ¸
ì‚¬ìš©ë²•: python test_crawler.py [source_id]
ì˜ˆì‹œ: python test_crawler.py aitimes
"""
import os
import sys
import json
import asyncio
from dotenv import load_dotenv

# í™˜ê²½ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ENV_PATH = os.path.join(BASE_DIR, '.env')
TARGETS_FILE = os.path.join(BASE_DIR, 'config/targets.json')

load_dotenv(dotenv_path=ENV_PATH)

# Import ê¸°ì¡´ í¬ë¡¤ëŸ¬ í•¨ìˆ˜ë“¤
from crawler import fetch_links, is_recent
from src.mll_client import MLLClient
from src.pipeline import process_article, get_db


def load_targets():
    """íƒ€ê²Ÿ ì„¤ì • ë¡œë“œ"""
    with open(TARGETS_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)


def get_target_by_id(source_id: str):
    """ì†ŒìŠ¤ IDë¡œ íƒ€ê²Ÿ ì°¾ê¸°"""
    targets = load_targets()
    for target in targets:
        if target['id'] == source_id:
            return target
    return None


def list_available_sources():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ì†ŒìŠ¤ ëª©ë¡ ì¶œë ¥"""
    targets = load_targets()
    print("\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì†ŒìŠ¤ ID ëª©ë¡:")
    print("-" * 40)
    for t in targets:
        print(f"  â€¢ {t['id']:20} ({t['type']}) - {t['url'][:40]}...")
    print("-" * 40)


async def test_single_article(source_id: str, skip_mll: bool = False):
    """
    íŠ¹ì • ì†ŒìŠ¤ì—ì„œ ë”± 1ê°œ ê¸°ì‚¬ë§Œ í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§
    
    Args:
        source_id: targets.jsonì— ì •ì˜ëœ ì†ŒìŠ¤ ID
        skip_mll: Trueë©´ MLL í‰ê°€ ê±´ë„ˆë›°ê¸° (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
    """
    print(f"\nğŸ§ª [í…ŒìŠ¤íŠ¸ ëª¨ë“œ] ì†ŒìŠ¤: {source_id}")
    print("=" * 50)
    
    # 1. íƒ€ê²Ÿ ì°¾ê¸°
    target = get_target_by_id(source_id)
    if not target:
        print(f"âŒ ì†ŒìŠ¤ ID '{source_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        list_available_sources()
        return
    
    print(f"âœ… íƒ€ê²Ÿ ë°œê²¬: {target}")
    
    # 2. ë§í¬ ê°€ì ¸ì˜¤ê¸°
    print(f"\nğŸ”— ë§í¬ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    links = fetch_links(target)
    
    if not links:
        print("âŒ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“‹ ë°œê²¬ëœ ë§í¬ ìˆ˜: {len(links)}")
    print(f"ğŸ¯ ì²« ë²ˆì§¸ ë§í¬: {links[0]}")
    
    # 3. ì¤‘ë³µ ì²´í¬ (ì„ íƒì )
    db = get_db()
    test_url = links[0]
    
    is_duplicate = db.check_history(test_url)
    if is_duplicate:
        print(f"âš ï¸ ì´ URLì€ ì´ë¯¸ ì²˜ë¦¬ë¨: {test_url}")
        print("   ìƒˆ URLë¡œ ì‹œë„í•˜ë ¤ë©´ 'y'ë¥¼ ì…ë ¥í•˜ì„¸ìš”, ê·¸ëŒ€ë¡œ ì§„í–‰í•˜ë ¤ë©´ Enter:")
        
        user_input = input().strip().lower()
        if user_input == 'y':
            # ì¤‘ë³µë˜ì§€ ì•Šì€ ì²« ë²ˆì§¸ ë§í¬ ì°¾ê¸°
            for link in links[1:]:
                if not db.check_history(link):
                    test_url = link
                    print(f"ğŸ”„ ìƒˆ URLë¡œ ë³€ê²½: {test_url}")
                    break
            else:
                print("âŒ ëª¨ë“  ë§í¬ê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return
    
    # 4. ë‹¨ì¼ ê¸°ì‚¬ ì²˜ë¦¬
    print(f"\nğŸš€ ê¸°ì‚¬ ì²˜ë¦¬ ì‹œì‘...")
    print(f"   URL: {test_url}")
    print(f"   MLL ì‚¬ìš©: {'ì•„ë‹ˆì˜¤' if skip_mll else 'ì˜ˆ'}")
    
    try:
        mll = MLLClient() if not skip_mll else None
        
        result = await process_article(
            url=test_url,
            source_id=source_id,
            mll_client=mll,
            skip_mll=skip_mll
        )
        
        print(f"\nğŸ“Š ê²°ê³¼:")
        print("-" * 40)
        print(json.dumps(result, indent=2, ensure_ascii=False, default=str))
        
        status = result.get('status', 'unknown')
        if status == 'saved':
            print(f"\nâœ… ì„±ê³µ! ì €ì¥ëœ article_id: {result.get('article_id')}")
        elif status == 'worthless':
            print(f"\nğŸš« ê°€ì¹˜ì—†ìŒ: {result.get('reason')}")
        elif status == 'mll_failed':
            print(f"\nâš ï¸ MLL ì‹¤íŒ¨: {result.get('reason')}")
        elif status == 'already_processed':
            print(f"\nâ­ï¸ ì´ë¯¸ ì²˜ë¦¬ë¨: {result.get('history_status')}")
        else:
            print(f"\nâ“ ì•Œ ìˆ˜ ì—†ëŠ” ìƒíƒœ: {status}")
            
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


def main():
    """ë©”ì¸ ì§„ì…ì """
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python test_crawler.py <source_id> [--skip-mll]")
        print("ì˜ˆì‹œ:")
        print("  python test_crawler.py aitimes")
        print("  python test_crawler.py techcrunch_ai --skip-mll")
        list_available_sources()
        return
    
    source_id = sys.argv[1]
    skip_mll = '--skip-mll' in sys.argv
    
    asyncio.run(test_single_article(source_id, skip_mll))


if __name__ == "__main__":
    main()
