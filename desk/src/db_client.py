import os
import firebase_admin
from firebase_admin import credentials, firestore

class DBClient:
    # í´ë˜ìŠ¤ ë ˆë²¨ ì‚¬ìš©ëŸ‰ ì¹´ìš´í„° (ì„¸ì…˜ ìœ ì§€)
    _usage_stats = {
        'reads': 0,
        'writes': 0,
        'deletes': 0,
        'session_start': None
    }
    
    def __init__(self):
        self.db = self._initialize_firebase()
        self.history = self._load_history()
        
        # ì„¸ì…˜ ì‹œì‘ ì‹œê°„ ê¸°ë¡ (ìµœì´ˆ 1íšŒ)
        if DBClient._usage_stats['session_start'] is None:
            from datetime import datetime, timezone
            DBClient._usage_stats['session_start'] = datetime.now(timezone.utc).isoformat()

    def _initialize_firebase(self):
        # supplier/src/db_client.py -> supplier/ í´ë” ê¸°ì¤€ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        key_filename = os.getenv('FIREBASE_SERVICE_ACCOUNT_KEY', 'serviceAccountKey.json')
        service_account_key = os.path.join(base_dir, key_filename)
        
        if not os.path.exists(service_account_key):
            # [Fix] Fallback to zeroechodaily-serviceAccountKey.json
            fallback_key = os.path.join(base_dir, 'zeroechodaily-serviceAccountKey.json')
            if os.path.exists(fallback_key):
                service_account_key = fallback_key
                print(f"âš ï¸ [DB] Using fallback key: {service_account_key}")
                
                # Update credential with found key
                try:
                    cred = credentials.Certificate(service_account_key)
                    try:
                        firebase_admin.get_app()
                    except ValueError:
                        firebase_admin.initialize_app(cred)
                    return firestore.client()
                except Exception as e:
                    print(f"âŒ Firebase Init Failed with fallback: {e}")
                    return None
            
            print(f"âš ï¸ {service_account_key} not found. DB operations will be skipped.")
            return None
        
        try:
            cred = credentials.Certificate(service_account_key)
            try:
                firebase_admin.get_app()
            except ValueError:
                firebase_admin.initialize_app(cred)
            return firestore.client()
        except Exception as e:
            print(f"âŒ Firebase Init Failed: {e}")
            return None

    def _get_env(self):
        """í™˜ê²½ ì„¤ì • ë°˜í™˜ (dev ë˜ëŠ” release)"""
        return os.getenv('ZND_ENV', 'dev')
    
    def _get_collection(self, collection_name):
        """í™˜ê²½ë³„ ì»¬ë ‰ì…˜ ì°¸ì¡° ë°˜í™˜
        
        êµ¬ì¡°: {env}/data/{collection_name}
        ì˜ˆ: dev/data/publications, release/data/publications
        """
        if not self.db:
            return None
        env = self._get_env()
        # í™˜ê²½ë³„ í•˜ìœ„ ì»¬ë ‰ì…˜ ê²½ë¡œ
        return self.db.collection(env).document('data').collection(collection_name)

    # ============================================
    # Firebase ì‚¬ìš©ëŸ‰ ì¶”ì  ë©”ì„œë“œ
    # ============================================
    
    @classmethod
    def _track_read(cls, count=1):
        """ì½ê¸° ì‘ì—… ì¹´ìš´íŠ¸"""
        cls._usage_stats['reads'] += count
    
    @classmethod
    def _track_write(cls, count=1):
        """ì“°ê¸° ì‘ì—… ì¹´ìš´íŠ¸"""
        cls._usage_stats['writes'] += count
    
    @classmethod
    def _track_delete(cls, count=1):
        """ì‚­ì œ ì‘ì—… ì¹´ìš´íŠ¸"""
        cls._usage_stats['deletes'] += count
    
    @classmethod
    def get_usage_stats(cls):
        """í˜„ì¬ ì„¸ì…˜ ì‚¬ìš©ëŸ‰ í†µê³„ ë°˜í™˜"""
        return {
            'reads': cls._usage_stats['reads'],
            'writes': cls._usage_stats['writes'],
            'deletes': cls._usage_stats['deletes'],
            'total': cls._usage_stats['reads'] + cls._usage_stats['writes'] + cls._usage_stats['deletes'],
            'session_start': cls._usage_stats['session_start']
        }
    
    @classmethod
    def reset_usage_stats(cls):
        """ì‚¬ìš©ëŸ‰ í†µê³„ ë¦¬ì…‹"""
        from datetime import datetime, timezone
        cls._usage_stats = {
            'reads': 0,
            'writes': 0,
            'deletes': 0,
            'session_start': datetime.now(timezone.utc).isoformat()
        }
        print("ğŸ”„ [Stats] Firebase usage stats reset")

    def _get_data_dir(self):
        # supplier/src/db_client.py -> supplier/data
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        return os.path.join(base_dir, 'data')

    def _load_history(self):
        import json
        file_path = os.path.join(self._get_data_dir(), 'crawling_history.json')
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return {}
        return {}

    def reload_history(self):
        """Force reload history from disk"""
        self.history = self._load_history()
        print(f"ğŸ”„ History reloaded. {len(self.history)} items.")


    def _save_history_file(self):
        import json
        file_path = os.path.join(self._get_data_dir(), 'crawling_history.json')
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Limit to last 5000 entries (FIFO)
        if len(self.history) > 5000:
            # Sort by timestamp if available, otherwise just slice
            # Since dicts are insertion-ordered in modern Python, slicing might work if we re-create it,
            # but safer to just keep it simple or sort.
            # For simplicity and performance, we'll just keep the last 5000 keys added if we assume append-only.
            # But here we might be updating existing keys.
            # Let's just keep the size in check roughly.
            keys_to_keep = list(self.history.keys())[-5000:]
            self.history = {k: self.history[k] for k in keys_to_keep}

        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(self.history, f, ensure_ascii=False, indent=2)

    def check_history(self, url):
        """
        Checks if the URL has been processed with a final state.
        Returns True if status is ACCEPTED, REJECTED, SKIPPED, WORTHLESS, or MLL_FAILED.
        For EXTRACT_FAILED: allows retry after 24 hours.
        """
        from datetime import datetime, timezone, timedelta
        
        if url in self.history:
            entry = self.history[url]
            status = entry.get('status')
            
            # ì˜êµ¬ ì°¨ë‹¨ ìƒíƒœ (ANALYZED í¬í•¨)
            if status in ['ACCEPTED', 'REJECTED', 'SKIPPED', 'WORTHLESS', 'MLL_FAILED', 'ANALYZED']:
                return True
            
            # EXTRACT_FAILED: 24ì‹œê°„ í›„ ì¬ì‹œë„ í—ˆìš©
            if status == 'EXTRACT_FAILED':
                timestamp = entry.get('timestamp')
                if timestamp:
                    try:
                        failed_at = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                        if datetime.now(timezone.utc) - failed_at < timedelta(hours=24):
                            return True  # 24ì‹œê°„ ì•ˆë¨, ìŠ¤í‚µ
                        # 24ì‹œê°„ ì§€ë‚¨, ì¬ì‹œë„ í—ˆìš©
                        return False
                    except:
                        pass
                return True  # íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                
        return False

    def get_history_status(self, url):
        """
        Returns the status of the URL if it exists in history, else None.
        """
        if url in self.history:
            return self.history[url].get('status')
        return None

    def save_history(self, url, status, reason=None):
        """
        Records the processing state of a URL.
        status: 'ACCEPTED', 'REJECTED', 'SKIPPED'
        """
        from datetime import datetime, timezone
        
        self.history[url] = {
            'status': status,
            'reason': reason,
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
        self._save_history_file()

    def remove_from_history(self, url):
        """
        Removes a URL from history, effectively resetting it to NEW state.
        """
        if url in self.history:
            del self.history[url]
            self._save_history_file()
            print(f"ğŸ—‘ï¸ [History] Removed from history: {url[:50]}...")
        else:
            print(f"âš ï¸ [History] URL not found in history: {url[:50]}...")

    def save_article(self, article_data):
        """ê¸°ì‚¬ ì €ì¥ - Firestore ìš°ì„ , ë¡œì»¬ ì €ì¥ ì œê±°"""
        from datetime import datetime, timezone
        import hashlib
        
        # Ensure crawled_at is set
        crawled_at = article_data.get('crawled_at')
        if isinstance(crawled_at, str):
            try:
                crawled_at_dt = datetime.fromisoformat(crawled_at)
            except ValueError:
                crawled_at_dt = datetime.now(timezone.utc)
        elif isinstance(crawled_at, datetime):
            crawled_at_dt = crawled_at
        else:
            crawled_at_dt = datetime.now(timezone.utc)
            article_data['crawled_at'] = crawled_at_dt.isoformat()

        # Generate article_id from URL
        url = article_data.get('url', '')
        article_id = article_data.get('article_id') or hashlib.md5(url.encode()).hexdigest()[:12]
        now = datetime.now(timezone.utc).isoformat()
        
        # V2 Schema ë³€í™˜
        v2_article = {
            '_header': {
                'version': '2.0',
                'article_id': article_id,
                'state': 'ANALYZED',  # ì €ì¥ ì‹œ ANALYZED ìƒíƒœ
                'created_at': article_data.get('crawled_at', now),
                'updated_at': now,
                'state_history': [
                    {'state': 'COLLECTED', 'at': article_data.get('crawled_at', now), 'by': 'crawler'},
                    {'state': 'ANALYZED', 'at': now, 'by': 'pipeline'}
                ]
            },
            '_original': {
                'url': url,
                'title': article_data.get('original_title') or article_data.get('title', ''),
                'text': article_data.get('text', '')[:5000],  # í…ìŠ¤íŠ¸ ì œí•œ
                'image': article_data.get('image'),
                'source_id': article_data.get('source_id', 'unknown'),
                'crawled_at': article_data.get('crawled_at', now),
                'published_at': article_data.get('published_at')
            },
            '_analysis': {
                'title_ko': article_data.get('title_ko') or article_data.get('title', ''),
                'summary': article_data.get('summary', ''),
                'tags': article_data.get('tags', []),
                'impact_score': float(article_data.get('impact_score', 0) or 0),
                'zero_echo_score': float(article_data.get('zero_echo_score', 0) or 0),
                'analyzed_at': now,
                'mll_raw': article_data.get('raw_analysis')
            },
            '_classification': None,
            '_publication': None
        }
        
        # Firestore ì €ì¥
        try:
            self._get_collection('articles').document(article_id).set(v2_article, merge=True)
            self._track_write()
            print(f"âœ… [Firestore] Saved article: {article_id}")
        except Exception as e:
            print(f"âŒ [Firestore] Save failed: {e}")

        # Update history as ANALYZED
        if url:
            self.save_history(url, 'ANALYZED', reason='mll_complete')








    # ============================================
    # Firestore CRUD Operations
    # ============================================
    

    


    
    # ============================================
    # Publication History Operations (New)
    # ============================================


    def create_publication_record(self, summary_data):
        """
        Create a new publication record in Firestore (ë©”íƒ€+ë‚´ì¥í˜• êµ¬ì¡°).
        
        summary_data: {
            'published_at': isoformat string,
            'edition_code': '251220_1',
            'edition_name': '12/20 1í˜¸',
            'article_count': int,
            'articles': [list of article dicts with full data],
            'status': 'preview' or 'released'
        }
        Returns: edition_code (= publish_id)
        """
        if not self.db:
            print("âš ï¸ [Firestore] DB not connected")
            return None

        try:
            from datetime import datetime, timezone
            
            edition_code = summary_data.get('edition_code')
            if not edition_code:
                print("âŒ [Firestore] edition_code is required")
                return None
            
            # Ensure timestamps
            if 'published_at' not in summary_data:
                summary_data['published_at'] = datetime.now(timezone.utc).isoformat()
            if 'updated_at' not in summary_data:
                summary_data['updated_at'] = summary_data['published_at']
            
            # 1. íšŒì°¨ ë¬¸ì„œ ì €ì¥ (edition_code = ë¬¸ì„œ ID)
            self._get_collection('publications').document(edition_code).set(summary_data)
            self._track_write()  # í†µê³„ ì¶”ì 
            print(f"ğŸ‰ [Firestore] Created Publication: {edition_code} ({summary_data.get('edition_name')})")
            
            # 2. _meta ë¬¸ì„œ ì—…ë°ì´íŠ¸
            self._update_meta(summary_data)
            
            # 3. _article_ids ë¬¸ì„œ ì—…ë°ì´íŠ¸
            article_ids = [a.get('id') or a.get('article_id') for a in summary_data.get('articles', []) if a.get('id') or a.get('article_id')]
            if article_ids:
                self._add_to_article_ids(article_ids)
            
            return edition_code
            
        except Exception as e:
            print(f"âŒ [Firestore] Publication Record Creation Failed: {e}")
            return None

    def update_publication_record(self, publish_id, update_data):
        """
        Update an existing publication record.
        """
        # [DEBUG] Entry Logging
        try:
            with open('debug_update.log', 'a', encoding='utf-8') as f:
                from datetime import datetime
                f.write(f"[{datetime.now()}] update_publication_record called for {publish_id}\n")
                f.write(f"Keys: {list(update_data.keys())}\n")
        except Exception as err:
            print(f"Log Error: {err}")

        if not self.db:
            return False
            
        try:
            from datetime import datetime, timezone
            update_data['updated_at'] = datetime.now(timezone.utc).isoformat()
            
            self._get_collection('publications').document(publish_id).update(update_data)
            self._track_write()  # í†µê³„ ì¶”ì 
            print(f"âœï¸ [Firestore] Updated Publication: {publish_id}")
            
            # _meta ì—…ë°ì´íŠ¸ (article_count, articles, status, schema_version ë³€ê²½ ì‹œ)
            if 'article_count' in update_data or 'articles' in update_data or 'status' in update_data or 'schema_version' in update_data:
                self._update_meta_for_issue(publish_id, update_data)
            
            return True
        except Exception as e:
            print(f"âŒ [Firestore] Publication Update Failed: {e}")
            return False

    # ============================================
    # Meta + Article IDs Management (ìƒˆ êµ¬ì¡°)
    # ============================================
    
    def _update_meta(self, issue_data):
        """_meta ë¬¸ì„œì— íšŒì°¨ ì •ë³´ ì¶”ê°€/ì—…ë°ì´íŠ¸"""
        if not self.db:
            return
            
        try:
            from google.cloud.firestore_v1 import ArrayUnion
            
            meta_ref = self._get_collection('publications').document('_meta')
            
            issue_entry = {
                'code': issue_data.get('edition_code'),
                'name': issue_data.get('edition_name'),
                'count': issue_data.get('article_count', 0),
                'updated_at': issue_data.get('updated_at'),
                'status': issue_data.get('status', 'preview'),
                'schema_version': issue_data.get('schema_version') # [NEW]
            }
            
            # ê¸°ì¡´ _meta ê°€ì ¸ì˜¤ê¸°
            meta_doc = meta_ref.get()
            if meta_doc.exists:
                existing_data = meta_doc.to_dict()
                issues = existing_data.get('issues', [])
                
                # ê°™ì€ codeê°€ ìˆìœ¼ë©´ êµì²´, ì—†ìœ¼ë©´ ì¶”ê°€
                updated = False
                for i, iss in enumerate(issues):
                    if iss.get('code') == issue_entry['code']:
                        issues[i] = issue_entry
                        updated = True
                        break
                if not updated:
                    issues.append(issue_entry)
                
                meta_ref.update({
                    'issues': issues,
                    'latest_updated_at': issue_data.get('updated_at')
                })
            else:
                # ìƒˆë¡œ ìƒì„±
                meta_ref.set({
                    'issues': [issue_entry],
                    'latest_updated_at': issue_data.get('updated_at')
                })
            
            print(f"ğŸ“‹ [Firestore] Updated _meta for {issue_entry['code']}")
        except Exception as e:
            print(f"âš ï¸ [Firestore] _meta update failed: {e}")
    
    def _update_meta_for_issue(self, edition_code, update_data):
        """ê¸°ì¡´ íšŒì°¨ì˜ _meta í•­ëª© ì—…ë°ì´íŠ¸"""
        # [DEBUG] File Logging
        try:
            with open('debug_meta.log', 'a', encoding='utf-8') as f:
                f.write(f"[{datetime.now()}] _update_meta_for_issue called for {edition_code}\n")
                f.write(f"Update Data Keys: {list(update_data.keys())}\n")
                if 'schema_version' in update_data:
                    f.write(f"Schema Version: {update_data['schema_version']}\n")
                else:
                    f.write("Schema Version NOT FOUND in update_data\n")
        except Exception as log_err:
            print(f"Log Error: {log_err}")

        if not self.db:
            return
            
        try:
            meta_ref = self._get_collection('publications').document('_meta')
            meta_doc = meta_ref.get()
            
            if meta_doc.exists:
                existing_data = meta_doc.to_dict()
                issues = existing_data.get('issues', [])
                
                for i, iss in enumerate(issues):
                    if iss.get('code') == edition_code:
                        if 'article_count' in update_data:
                            issues[i]['count'] = update_data['article_count']
                        if 'updated_at' in update_data:
                            issues[i]['updated_at'] = update_data['updated_at']
                        if 'schema_version' in update_data: # [NEW]
                            issues[i]['schema_version'] = update_data['schema_version']
                        if 'status' in update_data: # [NEW] Release ì‹œ status ë™ê¸°í™”
                            issues[i]['status'] = update_data['status']
                        break
                
                meta_ref.update({
                    'issues': issues,
                    'latest_updated_at': update_data.get('updated_at')
                })
        except Exception as e:
            print(f"âš ï¸ [Firestore] _meta update for issue failed: {e}")
    
    def _add_to_article_ids(self, article_ids: list):
        """_article_ids ë¬¸ì„œì— ì‹ ê·œ ID ì¶”ê°€"""
        if not self.db or not article_ids:
            return
            
        try:
            from google.cloud.firestore_v1 import ArrayUnion
            
            ids_ref = self._get_collection('publications').document('_article_ids')
            ids_ref.set({
                'ids': ArrayUnion(article_ids)
            }, merge=True)
            
            print(f"ğŸ”‘ [Firestore] Added {len(article_ids)} article IDs to _article_ids")
        except Exception as e:
            print(f"âš ï¸ [Firestore] _article_ids update failed: {e}")
    
    def _remove_from_article_ids(self, article_ids: list):
        """_article_ids ë¬¸ì„œì—ì„œ ID ì œê±°"""
        if not self.db or not article_ids:
            return
            
        try:
            from google.cloud.firestore_v1 import ArrayRemove
            
            ids_ref = self._get_collection('publications').document('_article_ids')
            ids_ref.update({
                'ids': ArrayRemove(article_ids)
            })
            
            print(f"ğŸ—‘ï¸ [Firestore] Removed {len(article_ids)} article IDs from _article_ids")
        except Exception as e:
            print(f"âš ï¸ [Firestore] _article_ids removal failed: {e}")
    
    def get_published_article_ids_from_firestore(self) -> set:
        """Firestore _article_ids ë¬¸ì„œì—ì„œ ë°œí–‰ëœ article_id ëª©ë¡ ì¡°íšŒ (1 READ)"""
        if not self.db:
            print("âš ï¸ [Firestore] DB not connected")
            return set()
            
        try:
            ids_doc = self._get_collection('publications').document('_article_ids').get()
            if ids_doc.exists:
                ids = ids_doc.to_dict().get('ids', [])
                print(f"ğŸ”‘ [Firestore] Loaded {len(ids)} published article IDs")
                return set(ids)
            return set()
        except Exception as e:
            print(f"âŒ [Firestore] Failed to get _article_ids: {e}")
            return set()

    def get_issues_from_meta(self, status_filter=None):
        """
        _meta ë¬¸ì„œì—ì„œ íšŒì°¨ ëª©ë¡ ì¡°íšŒ (1 READë¡œ ìµœì í™”)
        Args:
            status_filter: 'preview' ë˜ëŠ” 'released' (Noneì´ë©´ ì „ì²´)
        Returns: list of issue dicts [{code, name, count, updated_at, status}, ...]
        """
        if not self.db:
            print("âš ï¸ [Firestore] DB not connected")
            return []
            
        try:
            meta_doc = self._get_collection('publications').document('_meta').get()
            self._track_read()  # í†µê³„ ì¶”ì 
            
            if not meta_doc.exists:
                print("ğŸ“‹ [Firestore] No _meta document found")
                return []
            
            meta_data = meta_doc.to_dict()
            issues = meta_data.get('issues', [])
            
            # status í•„í„° ì ìš©
            if status_filter:
                issues = [i for i in issues if i.get('status') == status_filter]
            
            # _meta ë¬¸ì„œì˜ issues ë°°ì—´ì—ëŠ” _meta, _article_ids ê°™ì€ ì‹œìŠ¤í…œ ë¬¸ì„œê°€ ì—†ìœ¼ë¯€ë¡œ
            # codeê°€ '_'ë¡œ ì‹œì‘í•˜ëŠ” í•­ëª© í•„í„°ë§ (ì•ˆì „ì¥ì¹˜)
            issues = [i for i in issues if not i.get('code', '').startswith('_')]
            
            # code (edition_code) ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ë°œí–‰ìˆœ ìœ ì§€)
            issues.sort(key=lambda x: x.get('code', ''), reverse=True)
            
            # API ì‘ë‹µ í˜•ì‹ì— ë§ê²Œ ë³€í™˜ (id í•„ë“œ ì¶”ê°€)
            result = []
            for iss in issues:
                result.append({
                    'id': iss.get('code'),  # edition_code = publish_id
                    'edition_code': iss.get('code'),
                    'edition_name': iss.get('name'),
                    'article_count': iss.get('count', 0),
                    'updated_at': iss.get('updated_at'),
                    'status': iss.get('status', 'preview'),
                    'schema_version': iss.get('schema_version')  # [NEW]
                })
            
            print(f"ğŸ“‹ [Firestore] Loaded {len(result)} issues from _meta (1 READ)")
            return result
            
        except Exception as e:
            print(f"âŒ [Firestore] Failed to get _meta: {e}")
            return []

    def remove_issue_from_meta(self, edition_code):
        """_meta ë¬¸ì„œì—ì„œ íšŒì°¨ ì œê±°"""
        if not self.db:
            return
            
        try:
            meta_ref = self._get_collection('publications').document('_meta')
            meta_doc = meta_ref.get()
            
            if meta_doc.exists:
                from datetime import datetime, timezone
                
                existing_data = meta_doc.to_dict()
                issues = existing_data.get('issues', [])
                
                # í•´ë‹¹ code ì œì™¸
                issues = [i for i in issues if i.get('code') != edition_code]
                
                meta_ref.update({
                    'issues': issues,
                    'latest_updated_at': datetime.now(timezone.utc).isoformat()
                })
                
                print(f"ğŸ—‘ï¸ [Firestore] Removed {edition_code} from _meta")
        except Exception as e:
            print(f"âš ï¸ [Firestore] Failed to remove from _meta: {e}")

    def get_issues_by_date(self, date_str=None):
        """
        Get publication records, optionally filtered by date.
        date_str: 'YYYY-MM-DD' (assumes we store a 'date_str' field or filter by range)
        For simplicity, we'll fetch recent ones if date_str is None.
        If date_str provided, we filter by 'edition_code' prefix or a specific date field.
        """
        if not self.db:
            print("âš ï¸ [Firestore] DB not connected in get_issues_by_date")
            return []
            
        try:
            # date_str í•„í„°ë§ì€ ì¼ë‹¨ ë¹„í™œì„±í™”í•˜ê³  ì „ì²´ ì¡°íšŒ
            # (ë³µí•© ì¸ë±ìŠ¤ ë¬¸ì œ ë°©ì§€)
            docs = self._get_collection('publications').stream()
            
            issues = []
            for doc in docs:
                data = doc.to_dict()
                data['id'] = doc.id
                
                # date_str í•„í„°ê°€ ìˆìœ¼ë©´ Pythonì—ì„œ í•„í„°ë§
                if date_str:
                    edition_code = data.get('edition_code', '')
                    # 2025-12-20 -> 251220
                    yy = date_str[2:4]
                    mm = date_str[5:7]
                    dd = date_str[8:10]
                    prefix = f"{yy}{mm}{dd}"
                    if not edition_code.startswith(prefix):
                        continue
                        
                issues.append(data)
            
            # Pythonì—ì„œ published_at ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            issues.sort(key=lambda x: x.get('published_at', ''), reverse=True)
            
            print(f"ğŸ“° [Firestore] Found {len(issues)} publications")
            return issues
        except Exception as e:
            import traceback
            print(f"âŒ [Firestore] Get Issues Failed: {e}")
            traceback.print_exc()
            return []

    def get_publication(self, publish_id):
        """Get single publication record"""
        if not self.db:
            return None
        try:
            doc = self._get_collection('publications').document(publish_id).get()
            self._track_read()  # í†µê³„ ì¶”ì 
            if doc.exists:
                data = doc.to_dict()
                data['id'] = doc.id
                return data
            return None
        except Exception as e:
            return None

    def get_articles_by_publish_id(self, publish_id):
        """
        Firestoreì—ì„œ publish_idë¡œ ê¸°ì‚¬ ëª©ë¡ ì¡°íšŒ
        Args:
            publish_id: ë°œí–‰ íšŒì°¨ ID
        Returns: list of article dicts
        """
        if not self.db:
            print("âš ï¸ [Firestore] DB not connected")
            return []
            
        try:
            docs = self.db.collection('articles').where('publish_id', '==', publish_id).stream()
            
            articles = []
            for doc in docs:
                data = doc.to_dict()
                data['id'] = doc.id
                articles.append(data)
                
            print(f"ğŸ“° [Firestore] Found {len(articles)} articles for publish_id: {publish_id}")
            return articles
        except Exception as e:
            print(f"âŒ [Firestore] Get Articles by Publish ID Failed: {e}")
            return []

    def save_issue_index_file(self, issue_data):
        """
        Save a standalone JSON index file for the publication.
        issue_data: dict containing full publication info
        Path: data/YYYY-MM-DD/issue_{edition_code}.json
        """
        import json
        from datetime import datetime
        
        try:
            published_at = issue_data.get('published_at')
            if not published_at:
                published_at = datetime.now().isoformat()
                
            # Extract date for folder
            # If issue_data has 'date', use it. Else parse published_at
            if 'date' in issue_data:
                date_str = issue_data['date']
            else:
                date_str = published_at.split('T')[0]
                
            data_dir = self._get_data_dir()
            dir_path = os.path.join(data_dir, date_str)
            os.makedirs(dir_path, exist_ok=True)
            
            edition_code = issue_data.get('edition_code', 'unknown')
            filename = f"issue_{edition_code}.json"
            file_path = os.path.join(dir_path, filename)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(issue_data, f, ensure_ascii=False, indent=2)
                
            print(f"ğŸ’¾ [Issue Index] Saved to {file_path}")
            return file_path
        except Exception as e:
            print(f"âŒ [Issue Index] Save Failed: {e}")
            return None

    # ============================================
    # Cache Sync Operations (ìºì‹œ ë™ê¸°í™”)
    # ============================================
    
    def upload_cache_batch(self, date_str: str, cache_list: list) -> dict:
        """
        ìºì‹œ ë°ì´í„°ë¥¼ Firestore cache_sync/{date} ì»¬ë ‰ì…˜ì— ì¼ê´„ ì €ì¥
        
        Args:
            date_str: 'YYYY-MM-DD' í˜•ì‹
            cache_list: ìºì‹œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ [{'article_id': ..., ...}, ...]
        
        Returns:
            {'success': int, 'failed': int, 'errors': [...]}
        """
        if not self.db:
            print("âš ï¸ [Firestore] DB not connected")
            return {'success': 0, 'failed': len(cache_list), 'errors': ['DB not connected']}
        
        result = {'success': 0, 'failed': 0, 'errors': []}
        
        try:
            from datetime import datetime, timezone
            
            batch = self.db.batch()
            batch_count = 0
            MAX_BATCH = 500  # Firestore ë°°ì¹˜ í•œë„
            
            for cache_data in cache_list:
                try:
                    article_id = cache_data.get('article_id')
                    if not article_id:
                        result['failed'] += 1
                        result['errors'].append(f"Missing article_id")
                        continue
                    
                    # cache_sync/{date}/{article_id} ê²½ë¡œì— ì €ì¥
                    doc_ref = self.db.collection('cache_sync').document(date_str).collection('articles').document(article_id)
                    batch.set(doc_ref, cache_data)
                    batch_count += 1
                    result['success'] += 1
                    
                    # ë°°ì¹˜ í•œë„ ë„ë‹¬ ì‹œ ì»¤ë°‹
                    if batch_count >= MAX_BATCH:
                        batch.commit()
                        self._track_write(batch_count)
                        batch = self.db.batch()
                        batch_count = 0
                        
                except Exception as e:
                    result['failed'] += 1
                    result['errors'].append(str(e))
            
            # ë‚¨ì€ ë°°ì¹˜ ì»¤ë°‹
            if batch_count > 0:
                batch.commit()
                self._track_write(batch_count)
            
            print(f"â˜ï¸ [Sync] Uploaded {result['success']} cache items for {date_str}")
            
            # ë™ê¸°í™” ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            self.update_sync_metadata({
                'last_push': datetime.now(timezone.utc).isoformat(),
                'last_push_date': date_str,
                'last_push_count': result['success']
            })
            
            return result
            
        except Exception as e:
            print(f"âŒ [Sync] Upload Failed: {e}")
            result['errors'].append(str(e))
            return result
    
    def download_cache_batch(self, date_str: str) -> list:
        """
        Firestore cache_sync/{date} ì»¬ë ‰ì…˜ì—ì„œ ìºì‹œ ë°ì´í„° ì¼ê´„ ì¡°íšŒ
        
        Args:
            date_str: 'YYYY-MM-DD' í˜•ì‹
        
        Returns:
            ìºì‹œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ [{'article_id': ..., ...}, ...]
        """
        if not self.db:
            print("âš ï¸ [Firestore] DB not connected")
            return []
        
        try:
            cache_list = []
            
            # cache_sync/{date}/articles ì„œë¸Œì»¬ë ‰ì…˜ ì¡°íšŒ
            docs = self.db.collection('cache_sync').document(date_str).collection('articles').stream()
            
            read_count = 0
            for doc in docs:
                data = doc.to_dict()
                data['article_id'] = doc.id  # ë¬¸ì„œ ID = article_id
                cache_list.append(data)
                read_count += 1
            
            self._track_read(read_count)
            print(f"â˜ï¸ [Sync] Downloaded {len(cache_list)} cache items for {date_str}")
            
            # ë™ê¸°í™” ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            from datetime import datetime, timezone
            self.update_sync_metadata({
                'last_pull': datetime.now(timezone.utc).isoformat(),
                'last_pull_date': date_str,
                'last_pull_count': len(cache_list)
            })
            
            return cache_list
            
        except Exception as e:
            print(f"âŒ [Sync] Download Failed: {e}")
            return []
    
    def get_sync_metadata(self) -> dict:
        """
        ë™ê¸°í™” ë©”íƒ€ë°ì´í„° ì¡°íšŒ
        
        Returns:
            {
                'last_push': ISO ì‹œê°„,
                'last_pull': ISO ì‹œê°„,
                'last_push_date': 'YYYY-MM-DD',
                'last_pull_date': 'YYYY-MM-DD',
                ...
            }
        """
        if not self.db:
            return {}
        
        try:
            doc = self.db.collection('cache_sync').document('_meta').get()
            self._track_read()
            
            if doc.exists:
                return doc.to_dict()
            return {}
            
        except Exception as e:
            print(f"âŒ [Sync] Get Metadata Failed: {e}")
            return {}
    
    def update_sync_metadata(self, info: dict) -> bool:
        """
        ë™ê¸°í™” ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        
        Args:
            info: ì—…ë°ì´íŠ¸í•  ì •ë³´ ë”•ì…”ë„ˆë¦¬
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if not self.db:
            return False
        
        try:
            self.db.collection('cache_sync').document('_meta').set(info, merge=True)
            self._track_write()
            return True
            
        except Exception as e:
            print(f"âŒ [Sync] Update Metadata Failed: {e}")
            return False
    
    def get_cache_sync_dates(self) -> list:
        """
        Firestoreì— ë™ê¸°í™”ëœ ë‚ ì§œ ëª©ë¡ ì¡°íšŒ
        
        Returns:
            ë‚ ì§œ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ ['2025-12-24', '2025-12-23', ...]
        """
        if not self.db:
            return []
        
        try:
            # cache_sync ì»¬ë ‰ì…˜ì˜ ë¬¸ì„œ ID = ë‚ ì§œ
            docs = self._get_collection('cache_sync').stream()
            
            dates = []
            for doc in docs:
                if doc.id != '_meta':  # ë©”íƒ€ ë¬¸ì„œ ì œì™¸
                    dates.append(doc.id)
            
            self._track_read(len(dates))
            dates.sort(reverse=True)  # ìµœì‹  ë‚ ì§œ ìš°ì„ 
            return dates
            
        except Exception as e:
            print(f"âŒ [Sync] Get Dates Failed: {e}")
            return []

    def upload_crawling_history(self, history: dict) -> dict:
        """
        í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ë¥¼ Firestoreì— ì—…ë¡œë“œ (ë³‘í•© ë°©ì‹)
        
        Args:
            history: {url: {status, reason, timestamp}, ...}
        
        Returns:
            {'success': bool, 'count': int}
        """
        if not self.db:
            print("âš ï¸ [Firestore] DB not connected")
            return {'success': False, 'count': 0}
        
        try:
            from datetime import datetime, timezone
            
            # crawling_history ë¬¸ì„œì— ì €ì¥ (ë³‘í•©)
            # Firestore ë¬¸ì„œ í¬ê¸° ì œí•œ(1MB) ë•Œë¬¸ì— ì²­í¬ë¡œ ë¶„í• 
            CHUNK_SIZE = 500
            urls = list(history.keys())
            total_count = 0
            
            for i in range(0, len(urls), CHUNK_SIZE):
                chunk_urls = urls[i:i+CHUNK_SIZE]
                chunk_data = {url: history[url] for url in chunk_urls}
                
                # ì²­í¬ë³„ ë¬¸ì„œì— ì €ì¥
                chunk_id = f"chunk_{i // CHUNK_SIZE}"
                self._get_collection('crawling_history').document(chunk_id).set(chunk_data, merge=True)
                self._track_write()
                total_count += len(chunk_data)
            
            # ë©”íƒ€ ì •ë³´ ì—…ë°ì´íŠ¸
            self._get_collection('crawling_history').document('_meta').set({
                'total_count': len(history),
                'last_sync': datetime.now(timezone.utc).isoformat(),
                'chunk_count': (len(urls) + CHUNK_SIZE - 1) // CHUNK_SIZE
            }, merge=True)
            self._track_write()
            
            print(f"â˜ï¸ [Sync] Uploaded crawling history: {total_count} URLs")
            return {'success': True, 'count': total_count}
            
        except Exception as e:
            print(f"âŒ [Sync] History Upload Failed: {e}")
            return {'success': False, 'count': 0}
    
    def download_crawling_history(self) -> dict:
        """
        Firestoreì—ì„œ í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ ë‹¤ìš´ë¡œë“œ
        
        Returns:
            {url: {status, reason, timestamp}, ...}
        """
        if not self.db:
            print("âš ï¸ [Firestore] DB not connected")
            return {}
        
        try:
            history = {}
            
            # ëª¨ë“  ì²­í¬ ë¬¸ì„œ ì¡°íšŒ
            docs = self._get_collection('crawling_history').stream()
            
            read_count = 0
            for doc in docs:
                if doc.id.startswith('_'):  # ë©”íƒ€ ë¬¸ì„œ ìŠ¤í‚µ
                    continue
                    
                chunk_data = doc.to_dict()
                history.update(chunk_data)
                read_count += 1
            
            self._track_read(read_count)
            print(f"â˜ï¸ [Sync] Downloaded crawling history: {len(history)} URLs")
            return history
            
        except Exception as e:
            print(f"âŒ [Sync] History Download Failed: {e}")
            return {}
