{
  "_header": {
    "version": "2.0",
    "article_id": "356899285f3a",
    "state": "PUBLISHED",
    "state_history": [
      {
        "state": "COLLECTED",
        "at": "2025-12-26T16:45:08.256913+00:00",
        "by": "crawler"
      },
      {
        "state": "PUBLISHED",
        "at": "2025-12-28T17:11:28.063422+00:00",
        "by": "publisher"
      }
    ],
    "updated_at": "2025-12-28T17:11:28.063422+00:00"
  },
  "_original": {
    "title": "구글, '젬마 3' 내부 동작 파악할 수 있는 도구 오픈 소스 출시 - AI타임스",
    "description": "구글이 대표 오픈 소스 모델 '젬마 3'의 작동 방식을 추적할 수 있는 도구를 선보였다. 이는 모델의 블랙 박스 현상을 규명하기 위한 이전 연구를 실제 모델에 적",
    "image": "https://cdn.aitimes.com/news/photo/202512/205118_206609_5527.png",
    "text": "(사진=구글) 구글이 대표 오픈 소스 모델 '젬마 3'의 작동 방식을 추적할 수 있는 도구를 선보였다. 이는 모델의 블랙 박스 현상을 규명하기 위한 이전 연구를 실제 모델에 적용하고 그 결과를 공개한 사례다. 구글 딥마인드는 최근 젬마 3 해석 가능성 도구 모음인 ' 젬마 스코프 2(Gemma Scope 2) '를 출시했다. 이는 AI 안전이나 정렬 팀이 입력-출력 분석에만 의존하는 대신, 모델 동작을 내부 기능으로 추거적할 수 있는 방법을 제공한다. 모델이 탈옥하거나 환각을 보이거나 아첨할 때 어떤 내부 기능이 활황성화됐는지 그리고 어떻게 네트워크를 통해 전달됐는지를 검사할 수 있다. 이는 지난 7월 발표한 ‘점프렐루 희소 오토인코더(JumpReLU SAE)’를 구체적으로 젬마 3에 적용하고 확장한 결과다. 여기에서 '오토인코더(autoencoder)'는 입력을 중간 표현으로 인코딩한 다음, 이를 원래 형태로 디코딩하도록 학습하는 신경망이다. SAE(Sparse autoencorder)는 중간 표현으로 인코딩하는 단계에서 소수의 뉴런만 활성화되도록 오토인코더를 약간 수정한 것이다. 많은 수의 활성화된 뉴런을 소수의 중간 뉴런으로 압축, 희소한 중간 특징만으로 원래의 활성화 상태로 재구성을 시도한다. 이를 통해 인간이 쉽게 분석할 수 있는 개념이나 행동에 해당하는 희소한 특징(Feature) 집합으로 분해하는 것이다. 즉, SAE는 모델에서 해석 가능한 특징을 추출해 내부 작동 방식을 탐색하는 '현미경' 역할을 맡는다. 이는 앤트로픽이나 오픈AI의 모델 내부 규명을 위한 연구에도 활용된 핵심 개념이다. 젬마 스코프 2는 270M와 1B, 4B, 12B, 27B 매개변수 등 모든 젬마 3 모델군을 대상으로, 정보를 처리하고 표현하는 방식을 보여준다. 이에 따라 더 큰 모델에서만 나타는 복잡한 행동의 연구도 가능해졌다는 설명이다. '젬마 2'에 포커스를 맞췄던 이전 버전의 젬마 스코프를 업그레이드한 것으로, 이번에는 탈옥이나 환각 등 AI 안전과 관련된 행동을 추적하는 데 특화된 것이다. 또 구글은 모든 레이어에 '트랜스코더(Transcoder)'가 포함돼 있다. 이는 SAE가 분해한 특징들이 모델의 여러 계층(레이어)을 거치면서 어떻게 전파되고 계산되는지 그 경로를 추적할 수 있게 해준다. 젬마 스코프 2가 희소 오토인코더와 트랜스코더를 사용해 모델이 사기성 이메일을 어떻게 판별하는지 보여주는 내용 (사진=구글) 여기에 다양한 크기의 특징 벡터를 동시에 학습하는 '마트료시카 기법'을 SAE 학습에 적용, 해석 가능성 도구로서의 신뢰도를 향상했다고 덧붙였다. 실제로 젬마 스코프 2를 학습하기 위해 사용한 모델의 활성화 데이터는 무려 110페타바이트(PB)에 이른다. 이처럼 270M부터 27B까지의 모든 모델의 모든 레이어에서 발생하는 내부 활동을 포괄적으로 수집하고 저장해야 했다는 것을 의미한다. 결국 구글은 젬마 3 모델군의 방대한 내부 활동 데이터로 1조개 이상의 매개변수를 가진 해석 모델을 학습, 복잡한 내부 동작을 파악할 수 있는 거대한 현미경을 완성한 것이다. 이는 AI 안전과 해석 가능성 연구가 얼마나 데이터와 계산 집약적인 분야인지를 단적으로 보여준다. 구글은 \"최첨단 해석 도구를 출시, AI 안전 연구 커뮤니티의 발전을 지원하는 것이 목표\"라고 밝혔다. 또 \"이런 규모의 해석 가능성 도구 모음을 오픈 소스로 공개한 것은 최초로, 이는 첨단 LLM에서 발생하는 실제 안전 문제를 해결하는 데 매우 중요한 역할을 할 것\"이라고 강조했다. 젬마 스코프 2는 허깅페이스 를 통해 이용할 수 있다. 임대준 기자 ydj@aitimes.com",
    "published_at": "2025-12-26T18:00:00+09:00",
    "url": "https://www.aitimes.com/news/articleView.html?idxno=205118",
    "source_id": "aitimes",
    "crawled_at": "2025-12-26T16:45:08.256913+00:00"
  },
  "_analysis": {
    "title_ko": "구글, '젬마 3' 내부 동작 분석 도구 '젬마 스코프 2' 오픈소스 공개",
    "summary": "구글 딥마인드가 오픈형 모델 젬마 3의 내부 작동 원리를 추적할 수 있는 '젬마 스코프 2'를 공개했다. 희소 오토인코더(SAE) 기술을 적용해 AI의 환각, 탈옥 등 안전성 문제를 투명하게 분석할 수 있는 도구다.",
    "tags": [
      "Open Source",
      "Safety",
      "Google"
    ],
    "impact_score": 7.5,
    "zero_echo_score": 3.9,
    "analyzed_at": "2025-12-28T03:51:11.981483+00:00",
    "mll_raw": {
      "Article_ID": "356899285f3a",
      "Meta": {
        "Specification_Version": "v 1.0.0",
        "Headline": "구글, '젬마 3' 내부 동작 분석 도구 '젬마 스코프 2' 오픈소스 공개",
        "Summary": "구글 딥마인드가 오픈형 모델 젬마 3의 내부 작동 원리를 추적할 수 있는 '젬마 스코프 2'를 공개했다. 희소 오토인코더(SAE) 기술을 적용해 AI의 환각, 탈옥 등 안전성 문제를 투명하게 분석할 수 있는 도구다.",
        "Tags": [
          "Open Source",
          "Safety",
          "Google"
        ]
      },
      "IS_Analysis": {
        "Score_Commentary": "구글(T1)이 AI 해석 가능성(Interpretability) 분야의 핵심 도구를 공개함. 이는 AI 안전 연구 전반(X3)에 기여하며 즉시 사용 가능한 형태(Y3)로 제공됨.",
        "Calculations": {
          "IW_Analysis": {
            "Inputs": {
              "Pe_Selection_Rule": "P4",
              "Pe_Entity_Name": "Google",
              "Pe_Tier": 1,
              "Se_Entity_Name": "None",
              "Se_Tier": 0,
              "PE/SE 선정이유": "도구 개발 및 배포의 주체인 구글(Soft T1)을 PE로 선정. 내부 도구 공개로 특정 대립/협력 대상이 없어 SE는 None."
            },
            "Tier_Score": 3,
            "Gap_Score": 0,
            "IW_Score": 3
          },
          "IE_Analysis": {
            "Inputs": {
              "X_Magnitude_Code": 3,
              "Y_Evidence_Code": 3,
              "Scope_Matrix_Score": 2.5,
              "Criticality_C1_Provenness": 1,
              "Criticality_C2_Societal_Weight": 1,
              "Criticality_Total": 2,
              "SOTA_Check_Result": "True"
            },
            "IE_Score": 4.5
          }
        }
      },
      "ZES_Raw_Metrics": {
        "Signal": {
          "T1": 7,
          "T2": 9,
          "T3": 6,
          "Rationale": "SAE, 트랜스코더 등 기술적 메커니즘 설명이 매우 충실함."
        },
        "Noise": {
          "P1": 2,
          "P2": 2,
          "P3": 0,
          "Rationale": "기술 설명 위주의 건조하고 명확한 문체."
        },
        "Utility": {
          "V1": 6,
          "V2": 8,
          "V3": 7,
          "Rationale": "연구자들에게 즉시 유용한 도구 제공."
        },
        "Fine_Adjustment": {
          "Score": 0.5,
          "Reason": "전문적 기술 내용을 상세히 다룬 Deep Dive 성격."
        }
      }
    }
  },
  "_classification": {
    "category": "AI/ML",
    "is_selected": true,
    "classified_at": "2025-12-28T03:53:15.594961+00:00",
    "classified_by": "desk_user",
    "edition_code": null,
    "edition_name": null,
    "published_at": null,
    "released_at": null
  },
  "_publication": {
    "edition_code": "251228_6",
    "edition_name": "제6호",
    "published_at": "2025-12-28T12:07:19.908729+00:00",
    "released_at": null,
    "firestore_synced": true
  }
}