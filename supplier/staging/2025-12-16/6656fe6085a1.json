{
  "article_id": "6656fe",
  "cached_at": "2025-12-16T08:04:09.913538+00:00",
  "image": "https://the-decoder.com/wp-content/uploads/2025/12/financial_ai_impact.jpeg",
  "published_at": "Sun, 14 Dec 2025 12:20:53 GMT",
  "summary": "최신 추론형 AI 모델들이 금융 분석가 자격 시험인 CFA의 3개 레벨을 모두 통과했다는 연구 결과가 나왔다. 제미나이 3.0 프로는 레벨 1에서 97.6%라는 기록적인 점수를 달성했으며, GPT-5 등도 합격점을 넘겼다. 다만 윤리 문제에서는 여전히 약점을 보였다.",
  "text": "Matthias is the co-founder and publisher of THE DECODER, exploring how AI is fundamentally changing the relationship between humans and computers. Content Summary A new study shows that today's reasoning models can pass the grueling financial analyst test. Gemini 3.0 Pro set a record with a score of 97.6 percent at Level I. Ad The Chartered Financial Analyst (CFA) certification is widely considered one of finance's toughest qualifications. The three-stage exam tests progressively complex skills, ranging from fundamental knowledge to application, analysis, and complex portfolio construction. In 2023, the leading language models of the time could already answer some questions on the CFA exam. However, performance was mixed. ChatGPT (3.5) failed Levels I and II. GPT-4 managed to pass Level I but failed Level II. Eventually, GPT-4o—operating as a pure language model—succeeded in passing all three levels. A new study from researchers at Columbia University, Rensselaer Polytechnic Institute, and the University of North Carolina shows that the current generation of reasoning models passes all three levels, sometimes with near-perfect scores. Ad Ad THE DECODER Newsletter The most important AI news straight to your inbox. ✓ Weekly ✓ Free ✓ Cancel at any time Please leave this field empty Researchers put six reasoning models through 980 exam questions: three Level I exams with 540 multiple-choice questions, two Level II exams with 176 case-based questions, and three Level III exams with 264 questions, including open-answer formats. The result: Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1 passed every level based on established criteria. Gemini and GPT-5 lead the pack Gemini 3.0 Pro hit a record 97.6 percent on Level I, the foundational test consisting of independent multiple-choice questions. GPT-5 followed at 96.1 percent, with Gemini 2.5 Pro at 95.7 percent. Even the weakest model tested, DeepSeek-V3.1, scored 90.9 percent. GPT-5 took the lead on Level II, which tests application and analysis through case studies, scoring 94.3 percent. Gemini 3.0 Pro reached 93.2 percent and Gemini 2.5 Pro 92.6 percent. The researchers noted that models achieved \"nearly perfect results\" here. Ethics proved to be a stumbling block. Researchers reported relative error rates of 17 to 21 percent at Level II, even for the top-performing models. On Level III—the most complex stage combining multiple-choice with open responses—Gemini 2.5 Pro performed best on multiple-choice questions at 86.4 percent. However, Gemini 3.0 Pro dominated the constructed responses with 92.0 percent, a significant jump from its predecessor's 82.8 percent. Level Best model Result Level I (multiple choice) Gemini 3.0 Pro 97.6% Level II (multiple choice) GPT-5 94.3% Level III (multiple choice) Gemini 2.5 Pro 86.4% Level III (constructed responses) Gemini 3.0 Pro 92.0% Overall ranking Gemini 3.0 Pro 1st place The study uses mock CFA exams compiled from the official CFA Institute Practice Pack (Levels I and II) and AnalystPrep mock exams (Level III). Levels I and II used official material, while Level III used third-party mock exams to maintain comparability with previous research. An o4-mini model automated the grading of open answers. The study notes this introduces measurement errors and a possible \"verbosity bias\" where detailed answers get higher scores. Consequently, the results serve as model-based approximations. Pass thresholds were drawn from previous work: Level I requires at least 60 percent per topic and 70 percent overall. Level II needs at least 50 percent per topic and 60 percent overall. Level III requires an average of at least 63 percent across multiple-choice and constructed-response sections. Passing a test doesn't mean doing the job The researchers say the results suggest \"reasoning models surpass the expertise required of entry-level to mid-level financial analysts and may achieve senior-level financial analyst proficiency in the future.\" While LLMs had already mastered the \"codified knowledge\" of Levels I and II, the latest generation is now developing the complex synthesis skills required for Level III. The usual caveats apply. Benchmarks—especially multiple-choice formats—only hint at performance and potential economic impact. Passing a test doesn't mean a model can handle the daily grind of a financial analyst, which involves client meetings, assessing market sentiment, and making decisions with incomplete information. Ad Ad Join our community Join the DECODER community on Discord, Reddit or Twitter - we can't wait to meet you. The study also notes that models still struggle most with ethical questions, which often require contextual understanding and judgment. Exams test isolated knowledge, not the ability to apply it in complex, changing real-world situations. The researchers also can't rule out data contamination. Although they used current, paid materials, questions might have leaked into training data through paraphrased content in public datasets. This means there is a chance the models simply knew the answers rather than reasoning through them. Still, the leap from \"failed\" to \"almost perfect\" in just two years highlights the rapid advance of AI in specialized domains. For the financial sector, the question, it seems, is no longer whether AI can master the material, but how to integrate that knowledge into actual workflows.",
  "title": "Reasoning models now ace all three CFA exam levels",
  "url": "https://the-decoder.com/reasoning-models-now-ace-all-three-cfa-exam-levels/",
  "title_ko": "추론형 AI 모델, CFA 시험 3단계 모두 통과 및 최고 기록 경신",
  "tags": [
    "Benchmark",
    "Finance",
    "CFA"
  ],
  "impact_score": 5.5,
  "Impact_Analysis_IS": {
    "Analysis_Log": {
      "WHO_Primary_Entity": "Researchers (Columbia et al.)",
      "WHO_Primary_Tier_Source": "Academic_Media (Tier 3 - Univ)",
      "WHO_Entity_Tier": 3,
      "WHO_Secondary_Entity": "Google/OpenAI",
      "WHO_Secondary_Tier": 1,
      "Gap_Calculation_Log": "|3 - 1| = 2 -> Score 0.0",
      "WHAT_X_Magnitude": 2,
      "WHAT_Y_Evidence": 3,
      "SOTA_Check_Result": "Benchmark Success"
    },
    "Scores": {
      "IW_Score": 2,
      "Gap_Score": 0,
      "Context_Bonus": 1.5,
      "IE_Breakdown_Total": {
        "Scope_Total": 1.5,
        "Criticality_Total": 0.5
      },
      "Adjustment_Score": 0
    },
    "Reasoning": {
      "Score_Justification": "학계의 Tier 1 모델 성능 평가(Evaluation). 전문직무 대체 가능성을 시사하는 중간(Meso) 규모의 임팩트."
    }
  },
  "zero_echo_score": 2.1,
  "Evidence_Analysis_ZES": {
    "ZES_Penalty_Check": {
      "Penalty_Focus_Raw_Sum": 0,
      "Penalty_Clipping_Indicator": false
    },
    "ZES_Score_Vector": {
      "Positive_Scores": [
        {
          "ID": "P_5_Objective_Evidence",
          "Raw_Score": 1,
          "Weight": 1.4,
          "Evidence": "Detailed pass rates/scores per level"
        },
        {
          "ID": "P_1_Verifiable_Source",
          "Raw_Score": 0.75,
          "Weight": 2,
          "Evidence": "Multi-university study"
        }
      ],
      "Negative_Scores": []
    },
    "Analysis_Commentary": {
      "ZES_Summary": "구체적인 시험 점수와 방법론을 제시하여 모델 성능을 입증함. 일부 표현이 강하나 연구 결과에 기반함."
    }
  },
  "raw_analysis": {
    "Article_ID": "6656fe",
    "Meta": {
      "Headline": "추론형 AI 모델, CFA 시험 3단계 모두 통과 및 최고 기록 경신",
      "summary": "최신 추론형 AI 모델들이 금융 분석가 자격 시험인 CFA의 3개 레벨을 모두 통과했다는 연구 결과가 나왔다. 제미나이 3.0 프로는 레벨 1에서 97.6%라는 기록적인 점수를 달성했으며, GPT-5 등도 합격점을 넘겼다. 다만 윤리 문제에서는 여전히 약점을 보였다.",
      "Tag": [
        "Benchmark",
        "Finance",
        "CFA"
      ]
    },
    "PR_Scanner_Log": {
      "Detected_Triggers": [
        "Record",
        "Ace"
      ],
      "Marketing_Jargon_Count": 2,
      "Qualifier_Check": "Clean",
      "Sales_Intent": "Low"
    },
    "Impact_Analysis_IS": {
      "Analysis_Log": {
        "WHO_Primary_Entity": "Researchers (Columbia et al.)",
        "WHO_Primary_Tier_Source": "Academic_Media (Tier 3 - Univ)",
        "WHO_Entity_Tier": 3,
        "WHO_Secondary_Entity": "Google/OpenAI",
        "WHO_Secondary_Tier": 1,
        "Gap_Calculation_Log": "|3 - 1| = 2 -> Score 0.0",
        "WHAT_X_Magnitude": 2,
        "WHAT_Y_Evidence": 3,
        "SOTA_Check_Result": "Benchmark Success"
      },
      "Scores": {
        "IW_Score": 2,
        "Gap_Score": 0,
        "Context_Bonus": 1.5,
        "IE_Breakdown_Total": {
          "Scope_Total": 1.5,
          "Criticality_Total": 0.5
        },
        "Adjustment_Score": 0
      },
      "Reasoning": {
        "Score_Justification": "학계의 Tier 1 모델 성능 평가(Evaluation). 전문직무 대체 가능성을 시사하는 중간(Meso) 규모의 임팩트."
      }
    },
    "Evidence_Analysis_ZES": {
      "ZES_Penalty_Check": {
        "Penalty_Focus_Raw_Sum": 0,
        "Penalty_Clipping_Indicator": false
      },
      "ZES_Score_Vector": {
        "Positive_Scores": [
          {
            "ID": "P_5_Objective_Evidence",
            "Raw_Score": 1,
            "Weight": 1.4,
            "Evidence": "Detailed pass rates/scores per level"
          },
          {
            "ID": "P_1_Verifiable_Source",
            "Raw_Score": 0.75,
            "Weight": 2,
            "Evidence": "Multi-university study"
          }
        ],
        "Negative_Scores": []
      },
      "Analysis_Commentary": {
        "ZES_Summary": "구체적인 시험 점수와 방법론을 제시하여 모델 성능을 입증함. 일부 표현이 강하나 연구 결과에 기반함."
      }
    }
  },
  "source_id": "the_decoder",
  "original_title": "Reasoning models now ace all three CFA exam levels",
  "evidence": {
    "score_vector": {
      "Positive_Scores": [
        {
          "ID": "P_5_Objective_Evidence",
          "Raw_Score": 1,
          "Weight": 1.4,
          "Evidence": "Detailed pass rates/scores per level"
        },
        {
          "ID": "P_1_Verifiable_Source",
          "Raw_Score": 0.75,
          "Weight": 2,
          "Evidence": "Multi-university study"
        }
      ],
      "Negative_Scores": []
    },
    "commentary": {
      "ZES_Summary": "구체적인 시험 점수와 방법론을 제시하여 모델 성능을 입증함. 일부 표현이 강하나 연구 결과에 기반함."
    }
  },
  "impact_evidence": {
    "scores": {
      "IW_Score": 2,
      "Gap_Score": 0,
      "Context_Bonus": 1.5,
      "IE_Breakdown_Total": {
        "Scope_Total": 1.5,
        "Criticality_Total": 0.5
      },
      "Adjustment_Score": 0
    },
    "analysis_log": {
      "WHO_Primary_Entity": "Researchers (Columbia et al.)",
      "WHO_Primary_Tier_Source": "Academic_Media (Tier 3 - Univ)",
      "WHO_Entity_Tier": 3,
      "WHO_Secondary_Entity": "Google/OpenAI",
      "WHO_Secondary_Tier": 1,
      "Gap_Calculation_Log": "|3 - 1| = 2 -> Score 0.0",
      "WHAT_X_Magnitude": 2,
      "WHAT_Y_Evidence": 3,
      "SOTA_Check_Result": "Benchmark Success"
    },
    "reasoning": {
      "Score_Justification": "학계의 Tier 1 모델 성능 평가(Evaluation). 전문직무 대체 가능성을 시사하는 중간(Meso) 규모의 임팩트."
    }
  },
  "crawled_at": "2025-12-16T09:52:40.063965+00:00",
  "edition": "251216_TUE_1",
  "saved": true,
  "saved_at": "2025-12-16T09:52:40.068523+00:00",
  "staged_at": "2025-12-16T10:01:04.389330+00:00",
  "staged": true
}