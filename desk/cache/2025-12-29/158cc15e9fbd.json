{
  "_classification": {
    "classified_at": "2025-12-30T00:49:50.347346+09:00",
    "is_selected": true,
    "classified_by": "desk_user",
    "category": "AI/ML"
  },
  "_original": {
    "source_id": "the_decoder",
    "description": "If I spent more time thinking about a simple task than a complex one—and did worse on it—my boss would have some questions. But that's exactly what's happening with current reasoning models like Deepseek-R1. A team of researchers took a closer look at the problem and proposed theoretical laws describing how AI models should ideally 'think.'",
    "published_at": "2025-12-29T10:02:42+00:00",
    "crawled_at": "2025-12-29T22:50:29.205105+09:00",
    "text": "Ask about this article… Search Large reasoning models often reach illogical conclusions: they think longer on simple tasks than difficult ones and produce worse results. Researchers have now proposed theoretical laws describing how AI models should ideally \"think.\" Reasoning models like OpenAI's o1 or Deepseek-R1 differ from conventional language models by going through an internal \"thought process\" before answering. The model generates a chain of intermediate steps—called a reasoning trace—before reaching a final solution. For the question \"What is 17 × 24?\", a trace might look like this: I break the task down into smaller steps. 17 × 24 = 17 × (20 + 4) = 17 × 20 + 17 × 4. 17 × 20 = 340. 17 × 4 = 68. 340 + 68 = 408. The answer is 408. Ad This technique boosts performance on complex tasks like mathematical proofs or multi-step logic problems. But new research shows it doesn't always work efficiently. Ad DEC_D_Incontent-1 Simple tasks trigger more thinking than complex ones When Deepseek-R1 is asked to square a number, the model generates around 300 more reasoning tokens than for a compound task requiring both summing and squaring. At the same time, accuracy on the more complex task drops by 12.5 percent. A team of researchers from several US universities documented this behavior in a recent study. This example points to a core problem with current large reasoning models: their reasoning behavior often doesn't follow any recognizable logic. Humans typically adjust their thinking effort to match task difficulty. AI models don't do this reliably—sometimes they overthink, sometimes they don't think enough. The researchers trace the problem to training data, since chain-of-thought examples are usually put together without explicit rules for thinking time. Ad New framework proposes laws for optimal AI thinking The proposed \"Laws of Reasoning\" (LoRe) framework lays out two central hypotheses. The first law says required thinking should scale proportionally with task difficulty; tasks twice as hard should need twice as much compute time. The second law says accuracy should drop exponentially as difficulty rises. Since actual task difficulty can't be measured directly, the researchers rely on two testable properties. The first is simple: harder tasks should need more thinking time than easier ones. The second involves compound tasks: if a model requires one minute for task A and two minutes for task B, combining both should take about three minutes. Total effort should equal the sum of individual efforts. Ad DEC_D_Incontent-2 Every tested model fails on compound tasks The researchers built a two-part benchmark for their evaluation. The first part contains 40 tasks across math, science, language, and code, each with 30 variants of increasing difficulty. The second part combines 250 task pairs from the MATH500 dataset into composite questions. Ad Tests on ten large reasoning models paint a mixed picture. Most models handle the first property well, spending more thinking time on harder tasks. One exception is the smallest model tested, Deepseek-R1-Distill-Qwen-1.5B, which shows a negative correlation in language tasks: it actually thinks longer on simpler problems. But every tested model fails on compound tasks. The gaps between expected and actual thinking effort were substantial. Even models with special mechanisms to control reasoning length, like Thinkless-1.5B or AdaptThink-7B, didn't perform any better. Targeted training fixes inefficient thinking The researchers developed a fine-tuning approach designed to promote additive behavior in compound tasks. The method uses groups of three: two subtasks and their combination. From several generated solutions, it picks the combination where thinking effort for the overall task best matches the sum of individual efforts. With a 1.5B model, deviation in reasoning effort dropped by 40.5 percent. Reasoning skills improved across all six benchmarks tested. The 8B model saw an average five percentage point jump in accuracy. One surprise: training on additive thinking times also improved traits that weren't trained directly. The researchers acknowledge some limitations: the benchmark only contains 40 initial tasks, and they didn't test proprietary models due to cost. The code and benchmarks are publicly available. Industry doubles down on reasoning despite questions about its limits In 2025, reasoning models have become central to the LLM landscape: from Deepseek R1 delivering competitive performance with fewer training resources to hybrid models like Claude Sonnet 3.7 that let users set flexible reasoning budgets. At the same time, studies keep showing that while reasoning improves results, it's not the same as human thinking. Models likely just find existing solutions more efficiently within their trained knowledge. They likely can't 'think their way' to fundamentally new ideas the way humans can. Recent science benchmarks from OpenAI and other institutions support this view: models ace existing question-answer tests but struggle with complex, interconnected research tasks that need innovative solutions. Still, the AI industry is betting big that reasoning models can be improved through massive compute increases. OpenAI, for example, used ten times as much reasoning compute for o3 as for its predecessor o1—just four months after its release.",
    "title": "AI reasoning models think harder on easy problems than hard ones, and researchers have a theory for why",
    "image": "https://the-decoder.com/wp-content/uploads/2025/12/Laws-of-Reasoning-Nano-Banana-Pro.jpeg",
    "url": "https://the-decoder.com/ai-reasoning-models-think-harder-on-easy-problems-than-hard-ones-and-researchers-have-a-theory-for-why/"
  },
  "id": "158cc15e9fbd",
  "_publication": {
    "edition_name": "제10호",
    "edition_code": "251230_10",
    "released_at": null,
    "published_at": "2025-12-30T01:11:16.436652+09:00",
    "status": "preview",
    "firestore_synced": true
  },
  "_analysis": {
    "mll_raw": {
      "ZES_Raw_Metrics": {
        "Noise": {
          "Items": {
            "P1": {
              "Rationale": "현상 분석 위주",
              "Score": 2
            },
            "P3": {
              "Rationale": "왜곡 없음",
              "Score": 0
            },
            "P2": {
              "Rationale": "편향 없음",
              "Score": 0
            },
            "P4": {
              "Rationale": "광고 아님",
              "Score": 0
            }
          },
          "Description": "인위적 잡음 (0~10점)"
        },
        "Utility": {
          "Items": {
            "V2": {
              "Rationale": "모델 평가 및 개발에 참고 가능",
              "Score": 6
            },
            "V1": {
              "Rationale": "추론 모델 최적화 방향성 제시",
              "Score": 7
            },
            "V3": {
              "Rationale": "최신 모델(R1)의 맹점을 지적한 시의적절한 정보",
              "Score": 8
            },
            "V4": {
              "Rationale": "AI 아키텍처 연구에 적용",
              "Score": 6
            }
          },
          "Description": "실질적 효용 (0~10점)"
        },
        "Signal": {
          "Items": {
            "T4": {
              "Rationale": "연구 결과의 신뢰성 높음",
              "Score": 6
            },
            "T2": {
              "Rationale": "LoRe(Laws of Reasoning) 프레임워크와 비효율 원인 논리적 설명",
              "Score": 9
            },
            "T3": {
              "Rationale": "대학 연구팀 논문 인용",
              "Score": 7
            },
            "T1": {
              "Rationale": "300 토큰 추가 소모, 정확도 12.5% 하락 등 구체적 실험 수치",
              "Score": 8
            }
          },
          "Description": "정보의 실체 (0~10점)"
        },
        "Fine_Adjustment": {
          "Reason": "AI 모델의 '생각하는 방식'에 대한 심도 있는 분석",
          "Score": 0.5
        }
      },
      "Article_ID": "158cc15e9fbd",
      "IS_Analysis": {
        "Score_Commentary": "최신 추론 모델(o1, R1)의 구조적 결함을 지적하고 이론적 프레임워크를 제시한 학술적 가치가 높은 기사.",
        "Calculations": {
          "IE_Analysis": {
            "Inputs": {
              "SOTA_Check_Result": "True",
              "Criticality_C1_Provenness": 1,
              "Criticality_Total": 1.5,
              "Scope_Matrix_Score": 2.5,
              "Criticality_C2_Societal_Weight": 0.5,
              "Y_Evidence_Code": 3,
              "X_Magnitude_Code": 3
            },
            "IE_Score": 4
          },
          "IW_Analysis": {
            "Inputs": {
              "Se_Tier": 4,
              "Pe_Tier": 2,
              "Selection_Reason": "연구를 수행하고 이론을 제시한 주체(PE). 분석 대상이 된 기업(SE)은 Tier 1/4 혼재하나 기사 맥락상 R1(Tier 4)이 주 예시.",
              "Se_Entity_Name": "DeepSeek / OpenAI",
              "Pe_Selection_Rule": "P4",
              "Pe_Entity_Name": "US Researchers (Academic)"
            },
            "IW_Score": 1.5,
            "Tier_Score": 2,
            "Gap_Score": -0.5
          }
        }
      },
      "Meta": {
        "Tags": [
          "Deep Dive",
          "Model",
          "Research",
          "Math"
        ],
        "Summary": "미국 연구진이 DeepSeek-R1 등 추론 모델이 쉬운 문제에 과도한 연산을 사용하는 비논리적 패턴을 발견. '생각의 법칙(Laws of Reasoning)' 이론 제안.",
        "Headline": "AI 추론 모델, 쉬운 문제에 더 오래 생각하는 비효율성 발견",
        "Specification_Version": "v 1.0.0"
      }
    },
    "tags": [
      "Deep Dive",
      "Model",
      "Research",
      "Math"
    ],
    "summary": "미국 연구진이 DeepSeek-R1 등 추론 모델이 쉬운 문제에 과도한 연산을 사용하는 비논리적 패턴을 발견. '생각의 법칙(Laws of Reasoning)' 이론 제안.",
    "title_ko": "AI 추론 모델, 쉬운 문제에 더 오래 생각하는 비효율성 발견",
    "impact_score": 5.5,
    "analyzed_at": "2025-12-29T23:50:38.084093+09:00",
    "zero_echo_score": 3.8
  },
  "_header": {
    "state": "RELEASED",
    "updated_at": "2025-12-29T16:43:29.645710+00:00",
    "article_id": "158cc15e9fbd",
    "state_history": [
      {
        "by": "crawler",
        "at": "2025-12-29T22:50:29.205105+09:00",
        "state": "COLLECTED"
      },
      {
        "by": "board-send-back",
        "at": "2025-12-29T14:11:34.936860+00:00",
        "state": "COLLECTED"
      },
      {
        "by": "analyzer",
        "at": "2025-12-29T14:12:00.887162+00:00",
        "state": "ANALYZED"
      },
      {
        "state": "COLLECTED",
        "at": "2025-12-29T14:17:10.051818+00:00",
        "by": "board-send-back"
      },
      {
        "by": "analyzer",
        "at": "2025-12-29T14:17:33.758460+00:00",
        "state": "ANALYZED"
      },
      {
        "by": "board-send-back",
        "at": "2025-12-29T14:19:05.718498+00:00",
        "state": "COLLECTED"
      },
      {
        "state": "ANALYZED",
        "at": "2025-12-29T14:23:53.852544+00:00",
        "by": "analyzer"
      },
      {
        "by": "board-send-back",
        "at": "2025-12-29T14:48:16.155241+00:00",
        "state": "COLLECTED"
      },
      {
        "by": "analyzer",
        "at": "2025-12-29T14:50:38.466844+00:00",
        "state": "ANALYZED"
      },
      {
        "by": "desk",
        "at": "2025-12-29T15:49:51.307680+00:00",
        "state": "CLASSIFIED"
      },
      {
        "state": "PUBLISHED",
        "at": "2025-12-29T16:11:16.633292+00:00",
        "by": "publisher"
      },
      {
        "state": "RELEASED",
        "at": "2025-12-29T16:43:29.645710+00:00",
        "by": "publisher"
      }
    ],
    "version": "3.0"
  }
}