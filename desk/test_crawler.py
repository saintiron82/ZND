"""
í…ŒìŠ¤íŠ¸ í¬ë¡¤ëŸ¬ - 5ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°

íŠ¹ì • ì†ŒìŠ¤ IDì—ì„œ 1ê°œ ê¸°ì‚¬ë¥¼ ë‹¨ê³„ë³„ë¡œ ì²˜ë¦¬í•˜ëŠ” í…ŒìŠ¤íŠ¸ìš© ìŠ¤í¬ë¦½íŠ¸
ê° ë‹¨ê³„ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê±°ë‚˜ ALLë¡œ ì „ì²´ ì‹¤í–‰ ê°€ëŠ¥

ì‚¬ìš©ë²•: 
  python test_crawler.py [command] [source_id] [options]

ë‹¨ê³„ë³„ ì‹¤í–‰:
  python test_crawler.py collect aitimes       # 1ï¸âƒ£ ë§í¬ ìˆ˜ì§‘
  python test_crawler.py extract aitimes       # 2ï¸âƒ£ ì½˜í…ì¸  ì¶”ì¶œ
  python test_crawler.py analyze               # 3ï¸âƒ£ MLL ë¶„ì„
  python test_crawler.py stage                 # 4ï¸âƒ£ ì¡°íŒ
  python test_crawler.py publish               # 5ï¸âƒ£ ë°œí–‰

ì „ì²´ ì‹¤í–‰:
  python test_crawler.py all aitimes           # 1~4ë‹¨ê³„ ì—°ì†
  python test_crawler.py full aitimes          # 1~5ë‹¨ê³„ ì „ì²´
  
ë ˆê±°ì‹œ:
  python test_crawler.py aitimes               # ê¸°ì¡´ ë°©ì‹ (ë°”ë¡œ ì²˜ë¦¬)
"""
import os
import sys
import json
import asyncio
from datetime import datetime, timezone
from dotenv import load_dotenv

# í™˜ê²½ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ENV_PATH = os.path.join(BASE_DIR, '.env')
TARGETS_FILE = os.path.join(BASE_DIR, 'config/targets.json')
CACHE_DIR = os.path.join(BASE_DIR, 'cache')
STAGING_DIR = os.path.join(BASE_DIR, 'staging')

load_dotenv(dotenv_path=ENV_PATH)

# Import ê¸°ì¡´ í¬ë¡¤ëŸ¬ í•¨ìˆ˜ë“¤
from crawler import fetch_links, is_recent
from src.mll_client import MLLClient
from src.pipeline import process_article, get_db, save_article
from src.core_logic import (
    load_from_cache, save_to_cache, normalize_field_names, get_config
)
from src.crawler.core import AsyncCrawler


# ==============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ==============================================================================

def load_targets():
    """íƒ€ê²Ÿ ì„¤ì • ë¡œë“œ"""
    with open(TARGETS_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)


def get_target_by_id(source_id: str):
    """ì†ŒìŠ¤ IDë¡œ íƒ€ê²Ÿ ì°¾ê¸°"""
    targets = load_targets()
    for target in targets:
        if target['id'] == source_id:
            return target
    return None


def list_available_sources():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ì†ŒìŠ¤ ëª©ë¡ ì¶œë ¥"""
    targets = load_targets()
    print("\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì†ŒìŠ¤ ID ëª©ë¡:")
    print("-" * 40)
    for t in targets:
        print(f"  â€¢ {t['id']:20} ({t['type']}) - {t['url'][:40]}...")
    print("-" * 40)


# ==============================================================================
# 1ï¸âƒ£ ìˆ˜ì§‘ (Collect)
# ==============================================================================

def step_collect(source_id: str, limit: int = 1):
    """ë§í¬ ìˆ˜ì§‘ ë‹¨ê³„"""
    print(f"\n1ï¸âƒ£ [COLLECT] ë§í¬ ìˆ˜ì§‘ - ì†ŒìŠ¤: {source_id}")
    print("=" * 50)
    
    target = get_target_by_id(source_id)
    if not target:
        print(f"âŒ ì†ŒìŠ¤ ID '{source_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        list_available_sources()
        return None
    
    links = fetch_links(target)
    if not links:
        print("âŒ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # ì¤‘ë³µ í•„í„°ë§
    db = get_db()
    new_links = []
    for link in links[:limit*3]:  # ì—¬ìœ ìˆê²Œ ê°€ì ¸ì˜¤ê¸°
        if not db.check_history(link):
            new_links.append({'url': link, 'source_id': source_id})
            if len(new_links) >= limit:
                break
    
    print(f"ğŸ“‹ ì „ì²´ ë§í¬: {len(links)}ê°œ")
    print(f"âœ… ìƒˆ ë§í¬: {len(new_links)}ê°œ (limit: {limit})")
    
    for i, link in enumerate(new_links):
        print(f"   {i+1}. {link['url'][:60]}...")
    
    return new_links


# ==============================================================================
# 2ï¸âƒ£ ì¶”ì¶œ (Extract)
# ==============================================================================

async def step_extract(links: list):
    """ì½˜í…ì¸  ì¶”ì¶œ ë‹¨ê³„"""
    print(f"\n2ï¸âƒ£ [EXTRACT] ì½˜í…ì¸  ì¶”ì¶œ")
    print("=" * 50)
    
    if not links:
        print("âŒ ì¶”ì¶œí•  ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    extracted = []
    crawler = AsyncCrawler(use_playwright=True)
    
    try:
        await crawler.start()
        
        for item in links:
            url = item['url']
            source_id = item['source_id']
            
            # ìºì‹œ ì²´í¬
            cached = load_from_cache(url)
            if cached and cached.get('text'):
                print(f"ğŸ“¦ [ìºì‹œ] {url[:50]}...")
                extracted.append(cached)
                continue
            
            # í¬ë¡¤ë§
            print(f"ğŸŒ [í¬ë¡¤ë§] {url[:50]}...")
            content = await crawler.process_url(url)
            
            if content and len(content.get('text', '')) >= 200:
                content['source_id'] = source_id
                content['url'] = url
                save_to_cache(url, content)
                extracted.append(content)
                print(f"   âœ… ì €ì¥ ì™„ë£Œ (text: {len(content['text'])}ì)")
            else:
                print(f"   âš ï¸ ë³¸ë¬¸ ë¶€ì¡± ë˜ëŠ” ì‹¤íŒ¨")
    finally:
        await crawler.close()
    
    print(f"\nğŸ“Š ì¶”ì¶œ ì™„ë£Œ: {len(extracted)}ê°œ")
    return extracted


# ==============================================================================
# 3ï¸âƒ£ ë¶„ì„ (Analyze)
# ==============================================================================

def step_analyze(articles: list = None):
    """MLL ë¶„ì„ ë‹¨ê³„"""
    print(f"\n3ï¸âƒ£ [ANALYZE] MLL ë¶„ì„")
    print("=" * 50)
    
    # articlesê°€ ì—†ìœ¼ë©´ ì˜¤ëŠ˜ ìºì‹œì—ì„œ ë¯¸ë¶„ì„ ì°¾ê¸°
    if articles is None:
        today_str = datetime.now().strftime('%Y-%m-%d')
        cache_date_dir = os.path.join(CACHE_DIR, today_str)
        articles = []
        
        if os.path.exists(cache_date_dir):
            for filename in os.listdir(cache_date_dir):
                if not filename.endswith('.json'):
                    continue
                filepath = os.path.join(cache_date_dir, filename)
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if not data.get('mll_status') and not data.get('raw_analysis'):
                        if len(data.get('text', '')) >= 200:
                            articles.append(data)
    
    if not articles:
        print("âŒ ë¶„ì„í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    mll = MLLClient()
    analyzed = []
    
    for article in articles:
        url = article.get('url', 'unknown')
        text = article.get('text', '')
        
        print(f"ğŸ¤– [ë¶„ì„ ì¤‘] {article.get('title', url)[:40]}...")
        
        max_text = get_config('crawler', 'max_text_length_for_analysis', default=3000)
        truncated_text = text[:max_text]
        
        try:
            mll_result = mll.analyze_text(truncated_text)
            
            if mll_result:
                mll_result = normalize_field_names(mll_result)
                article.update(mll_result)
                article['mll_status'] = 'analyzed'
                article['analyzed_at'] = datetime.now(timezone.utc).isoformat()
                
                # ìºì‹œ ì—…ë°ì´íŠ¸
                save_to_cache(url, article)
                analyzed.append(article)
                
                zs = article.get('zero_echo_score', 'N/A')
                is_ = article.get('impact_score', 'N/A')
                print(f"   âœ… ì™„ë£Œ (ZS: {zs}, IS: {is_})")
            else:
                article['mll_status'] = 'failed'
                save_to_cache(url, article)
                print(f"   âš ï¸ MLL ì‘ë‹µ ì—†ìŒ")
        except Exception as e:
            print(f"   âŒ ì—ëŸ¬: {e}")
    
    print(f"\nğŸ“Š ë¶„ì„ ì™„ë£Œ: {len(analyzed)}ê°œ")
    return analyzed


# ==============================================================================
# 4ï¸âƒ£ ì¡°íŒ (Stage)
# ==============================================================================

def step_stage(articles: list = None):
    """ì¡°íŒ ë‹¨ê³„ - staging í´ë”ë¡œ ì´ë™"""
    print(f"\n4ï¸âƒ£ [STAGE] ì¡°íŒ")
    print("=" * 50)
    
    today_str = datetime.now().strftime('%Y-%m-%d')
    staging_date_dir = os.path.join(STAGING_DIR, today_str)
    os.makedirs(staging_date_dir, exist_ok=True)
    
    # articlesê°€ ì—†ìœ¼ë©´ ì˜¤ëŠ˜ ìºì‹œì—ì„œ ë¶„ì„ì™„ë£Œ ì°¾ê¸°
    if articles is None:
        cache_date_dir = os.path.join(CACHE_DIR, today_str)
        articles = []
        
        if os.path.exists(cache_date_dir):
            for filename in os.listdir(cache_date_dir):
                if not filename.endswith('.json'):
                    continue
                filepath = os.path.join(cache_date_dir, filename)
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if data.get('mll_status') == 'analyzed' and not data.get('staged'):
                        articles.append((data, filepath))
    else:
        # ìºì‹œ íŒŒì¼ ê²½ë¡œ ì¶”ê°€
        articles = [(a, None) for a in articles]
    
    if not articles:
        print("âŒ ì¡°íŒí•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    staged = []
    high_noise_threshold = get_config('scoring', 'high_noise_threshold', default=7.0)
    
    for item in articles:
        article = item[0] if isinstance(item, tuple) else item
        cache_path = item[1] if isinstance(item, tuple) else None
        
        url = article.get('url', 'unknown')
        title = article.get('title_ko', article.get('title', 'N/A'))
        zs = float(article.get('zero_echo_score', 5.0))
        
        # ê³ ë…¸ì´ì¦ˆ í•„í„°ë§
        if zs >= high_noise_threshold:
            article['rejected'] = True
            article['reject_reason'] = f'high_noise ({zs})'
            print(f"ï¿½ [ê±°ë¶€] {title[:30]}... (ZS: {zs})")
        else:
            print(f"âœ… [ì¡°íŒ] {title[:30]}... (ZS: {zs})")
        
        # Staging ì €ì¥
        article['staged'] = True
        article['staged_at'] = datetime.now(timezone.utc).isoformat()
        
        from src.core_logic import get_url_hash
        filename = f"{get_url_hash(url)}.json"
        staging_path = os.path.join(staging_date_dir, filename)
        
        with open(staging_path, 'w', encoding='utf-8') as f:
            json.dump(article, f, ensure_ascii=False, indent=2)
        
        staged.append(article)
        
        # ìºì‹œë„ ì—…ë°ì´íŠ¸
        if cache_path and os.path.exists(cache_path):
            article_copy = article.copy()
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(article_copy, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“Š ì¡°íŒ ì™„ë£Œ: {len(staged)}ê°œ (staging/{today_str})")
    return staged


# ==============================================================================
# 5ï¸âƒ£ ë°œí–‰ (Publish)
# ==============================================================================

def step_publish(articles: list = None):
    """ë°œí–‰ ë‹¨ê³„"""
    print(f"\n5ï¸âƒ£ [PUBLISH] ë°œí–‰")
    print("=" * 50)
    
    today_str = datetime.now().strftime('%Y-%m-%d')
    staging_date_dir = os.path.join(STAGING_DIR, today_str)
    
    # articlesê°€ ì—†ìœ¼ë©´ stagingì—ì„œ ì°¾ê¸°
    if articles is None:
        articles = []
        if os.path.exists(staging_date_dir):
            for filename in os.listdir(staging_date_dir):
                if not filename.endswith('.json'):
                    continue
                filepath = os.path.join(staging_date_dir, filename)
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if not data.get('rejected') and not data.get('published'):
                        articles.append((data, filepath))
    else:
        articles = [(a, None) for a in articles if not a.get('rejected')]
    
    if not articles:
        print("âŒ ë°œí–‰í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    published = []
    
    for item in articles:
        article = item[0] if isinstance(item, tuple) else item
        staging_path = item[1] if isinstance(item, tuple) else None
        
        title = article.get('title_ko', article.get('title', 'N/A'))
        
        # í•„ìˆ˜ í•„ë“œ ì²´í¬
        required = ['url', 'title_ko', 'summary', 'zero_echo_score', 'impact_score']
        missing = [f for f in required if f not in article]
        if missing:
            print(f"âš ï¸ [ìŠ¤í‚µ] {title[:30]}... (í•„ë“œ ëˆ„ë½: {missing})")
            continue
        
        # ë°œí–‰
        result = save_article(article, source_id=article.get('source_id'))
        
        if result.get('status') == 'saved':
            article['published'] = True
            article['published_at'] = datetime.now(timezone.utc).isoformat()
            article['data_file'] = result.get('filename')
            
            if staging_path:
                with open(staging_path, 'w', encoding='utf-8') as f:
                    json.dump(article, f, ensure_ascii=False, indent=2)
            
            published.append(article)
            print(f"âœ… [ë°œí–‰] {title[:30]}... â†’ {result.get('filename')}")
        else:
            print(f"âŒ [ì‹¤íŒ¨] {title[:30]}... ({result.get('reason', 'unknown')})")
    
    print(f"\nğŸ“Š ë°œí–‰ ì™„ë£Œ: {len(published)}ê°œ")
    return published


# ==============================================================================
# í†µí•© ì‹¤í–‰
# ==============================================================================

async def run_all(source_id: str, include_publish: bool = False):
    """1~4 ë˜ëŠ” 1~5 ë‹¨ê³„ ì—°ì† ì‹¤í–‰"""
    # 1ï¸âƒ£ ìˆ˜ì§‘
    links = step_collect(source_id, limit=1)
    if not links:
        return
    
    # 2ï¸âƒ£ ì¶”ì¶œ
    articles = await step_extract(links)
    if not articles:
        return
    
    # 3ï¸âƒ£ ë¶„ì„
    analyzed = step_analyze(articles)
    if not analyzed:
        return
    
    # 4ï¸âƒ£ ì¡°íŒ
    staged = step_stage(analyzed)
    
    # 5ï¸âƒ£ ë°œí–‰ (ì„ íƒ)
    if include_publish:
        step_publish(staged)
    else:
        print("\nâ¸ï¸ ë°œí–‰ ëŒ€ê¸° ì¤‘ (stagingì—ì„œ ê²€í†  í›„ publish ì‹¤í–‰)")


# ==============================================================================
# ë ˆê±°ì‹œ í˜¸í™˜
# ==============================================================================

async def test_single_article(source_id: str, skip_mll: bool = False):
    """ê¸°ì¡´ ë°©ì‹ - ë°”ë¡œ ì²˜ë¦¬ (ë ˆê±°ì‹œ í˜¸í™˜)"""
    print(f"\nğŸ§ª [ë ˆê±°ì‹œ ëª¨ë“œ] ì†ŒìŠ¤: {source_id}")
    print("=" * 50)
    
    target = get_target_by_id(source_id)
    if not target:
        print(f"âŒ ì†ŒìŠ¤ ID '{source_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        list_available_sources()
        return
    
    links = fetch_links(target)
    if not links:
        print("âŒ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    db = get_db()
    test_url = links[0]
    
    if db.check_history(test_url):
        print(f"âš ï¸ ì´ URLì€ ì´ë¯¸ ì²˜ë¦¬ë¨: {test_url}")
        for link in links[1:]:
            if not db.check_history(link):
                test_url = link
                print(f"ğŸ”„ ìƒˆ URLë¡œ ë³€ê²½: {test_url}")
                break
        else:
            print("âŒ ëª¨ë“  ë§í¬ê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
    
    mll = MLLClient() if not skip_mll else None
    result = await process_article(
        url=test_url,
        source_id=source_id,
        mll_client=mll,
        skip_mll=skip_mll
    )
    
    print(f"\nğŸ“Š ê²°ê³¼:")
    print(json.dumps(result, indent=2, ensure_ascii=False, default=str))


# ==============================================================================
# ë©”ì¸
# ==============================================================================

def print_usage():
    """ì‚¬ìš©ë²• ì¶œë ¥"""
    print("""
ğŸ§ª í…ŒìŠ¤íŠ¸ í¬ë¡¤ëŸ¬ - 5ë‹¨ê³„ íŒŒì´í”„ë¼ì¸

ì‚¬ìš©ë²•: python test_crawler.py [command] [source_id] [options]

ğŸ“‹ ë‹¨ê³„ë³„ ì‹¤í–‰:
  collect <source_id>     1ï¸âƒ£ ë§í¬ ìˆ˜ì§‘
  extract <source_id>     2ï¸âƒ£ ì½˜í…ì¸  ì¶”ì¶œ
  analyze                 3ï¸âƒ£ MLL ë¶„ì„ (ìºì‹œì—ì„œ ë¯¸ë¶„ì„ ì°¾ê¸°)
  stage                   4ï¸âƒ£ ì¡°íŒ (staging í´ë”ë¡œ)
  publish                 5ï¸âƒ£ ë°œí–‰ (staging â†’ data)

âš¡ í†µí•© ì‹¤í–‰:
  all <source_id>         1~4ë‹¨ê³„ ì—°ì† (ë°œí–‰ ëŒ€ê¸°)
  full <source_id>        1~5ë‹¨ê³„ ì „ì²´ ì‹¤í–‰

ğŸ”§ ë ˆê±°ì‹œ:
  <source_id>             ê¸°ì¡´ ë°©ì‹ (ë°”ë¡œ ì²˜ë¦¬)
  <source_id> --skip-mll  MLL ê±´ë„ˆë›°ê¸°

ğŸ“‹ ê¸°íƒ€:
  list                    ì†ŒìŠ¤ ëª©ë¡ ë³´ê¸°
""")
    list_available_sources()


def main():
    """ë©”ì¸ ì§„ì…ì """
    if len(sys.argv) < 2:
        print_usage()
        return
    
    command = sys.argv[1].lower()
    
    # ì†ŒìŠ¤ ëª©ë¡
    if command == 'list':
        list_available_sources()
        return
    
    # ë‹¨ê³„ë³„ ì‹¤í–‰
    if command == 'collect':
        source_id = sys.argv[2] if len(sys.argv) > 2 else None
        if not source_id:
            print("âŒ source_idë¥¼ ì§€ì •í•˜ì„¸ìš”")
            return
        step_collect(source_id)
        
    elif command == 'extract':
        source_id = sys.argv[2] if len(sys.argv) > 2 else None
        if not source_id:
            print("âŒ source_idë¥¼ ì§€ì •í•˜ì„¸ìš”")
            return
        links = step_collect(source_id)
        if links:
            asyncio.run(step_extract(links))
            
    elif command == 'analyze':
        step_analyze()
        
    elif command == 'stage':
        step_stage()
        
    elif command == 'publish':
        step_publish()
        
    elif command == 'all':
        source_id = sys.argv[2] if len(sys.argv) > 2 else None
        if not source_id:
            print("âŒ source_idë¥¼ ì§€ì •í•˜ì„¸ìš”")
            return
        asyncio.run(run_all(source_id, include_publish=False))
        
    elif command == 'full':
        source_id = sys.argv[2] if len(sys.argv) > 2 else None
        if not source_id:
            print("âŒ source_idë¥¼ ì§€ì •í•˜ì„¸ìš”")
            return
        asyncio.run(run_all(source_id, include_publish=True))
    
    else:
        # ë ˆê±°ì‹œ ëª¨ë“œ
        source_id = command
        skip_mll = '--skip-mll' in sys.argv
        asyncio.run(test_single_article(source_id, skip_mll))


if __name__ == "__main__":
    main()
