{
  "article_id": "707363",
  "cached_at": "2025-12-19T18:10:27.145903+00:00",
  "image": "https://the-decoder.com/wp-content/uploads/2025/12/openai_science_logo.jpeg",
  "published_at": "Thu, 18 Dec 2025 19:04:34 GMT",
  "summary": "OpenAI의 새로운 벤치마크 FrontierScience에서 GPT-5.2가 올림피아드 수준 문제에서는 높은 점수(77%)를 기록했으나, 개방형 박사급 연구 과제에서는 25%의 정답률에 그쳤다. 이는 GPT-4o 등 기존 모델 대비 큰 향상이지만, 여전히 전문가 수준의 독자 연구 수행에는 한계가 있음을 보여준다. 구글, Anthropic 모델들과의 비교 결과도 포함되었다.",
  "text": "Matthias is the co-founder and publisher of THE DECODER, exploring how AI is fundamentally changing the relationship between humans and computers. Content Summary OpenAI's new FrontierScience benchmark tests AI models at Olympiad and research levels. GPT-5.2 comes out on top, but the results also reveal where current systems still fall short. Ad OpenAI says existing scientific benchmarks are running out of headroom. When the company released GPQA—a \"Google-proof\" multiple-choice test for PhD-level science questions—in November 2023, GPT-4 scored 39 percent. Two years later, GPT-5.2 hits 92 percent. That rapid improvement, the company says, calls for tougher evaluation methods. Enter FrontierScience, a two-part benchmark: an Olympiad set with problems at the level of international science competitions, and a research set with open-ended PhD-level challenges. The published Gold set contains 160 questions on physics, chemistry, and biology, filtered down from over 700 original problems. OpenAI is holding back the rest to check for potential contamination. Olympic medalists and researchers designed the questions The 100 Olympiad questions were created by 42 former international medalists or national coaches who have collectively won 108 Olympic medals. The problems draw from the International Physics Olympiad, International Chemistry Olympiad, and International Biology Olympiad. Every answer can be verified as a single number, algebraic expression, or unique term. Ad Ad THE DECODER Newsletter The most important AI news straight to your inbox. ✓ Weekly ✓ Free ✓ Cancel at any time Please leave this field empty The 60 research questions come from 45 scientists with expertise spanning quantum mechanics, molecular biology, and photochemistry. OpenAI says each task should take at least three to five hours to solve. Instead of a single correct answer, research tasks are scored on a ten-point rubric, with GPT-5 handling the automated grading at high reasoning intensity. GPT-5.2 leads, but research tasks remain tough All reasoning models were tested at \"high\" reasoning intensity, with GPT-5.2 also tested at \"xhigh\" - and without browsing enabled. GPT-5.2 scores 77 percent on the Olympiad set and 25 percent on Research. Gemini 3 Pro trails close behind on Olympiad at 76 percent. On Research, GPT-5.2 and GPT-5 tie for first place, and in a \"surprising\" (OpenAI) twist, GPT-5 significantly outperforms the newer GPT-5.1, which manages just about 19 percent. Claude Opus 4.5 hits 71 percent on Olympiad and 18 percent on Research. Grok 4 scores 66.2 percent and 16 percent respectively. The older GPT-4o lags far behind at 12 percent on Olympiad and under one percent on Research. OpenAI's first reasoning model o1, released last September, marked a major leap forward. Share Recommend our article Share More compute means better results Performance scales with compute time. GPT-5.2 jumps from 67.5 percent at low reasoning intensity to 77 percent at the highest setting on the Olympiad set. On Research, scores climb from 18 to 25 percent. OpenAI's o3 model bucks the trend on Research: it actually does slightly worse at high reasoning intensity than at medium. The company calls this \"surprising\" but doesn't explain why. OpenAI says the results show real progress on expert-level questions but leave plenty of room for improvement, especially on open research tasks. Across subjects, models do best in chemistry. Common failure modes include logic errors, trouble with niche concepts, calculation mistakes, and factual errors. Language models are getting better at working with numbers Recent months have brought multiple reports of AI speeding up research. OpenAI released \"GPT-5 Science Acceleration,\" a collection of case studies showing mathematicians getting help with proofs, physicists with symmetry analysis, and immunologists with hypotheses and experimental design. Physicist Steve Hsu published a paper whose central idea came from GPT-5. He sees this as the start of \"hybrid human-AI collaborations\" that could become standard in math, physics, and other formal sciences. The result has also drawn criticism. OpenAI has announced plans to build autonomous research agents by 2028 that could fundamentally speed up scientific discovery. Google DeepMind and OpenAI also showed in 2025 that AI models with advanced reasoning and reinforcement learning can increasingly solve complex math problems on their own for hours without symbolic aids. Mathematician Terence Tao has said AI helped him solve problems too. At the same time, experts warn of risks: uncritical use of AI in science could churn out large volumes of plausible-sounding but wrong results. Ad",
  "title": "GPT-5.2 tops OpenAI's new FrontierScience test but struggles with real research problems",
  "url": "https://the-decoder.com/gpt-5-2-tops-openais-new-frontierscience-test-but-struggles-with-real-research-problems/",
  "title_ko": "GPT-5.2, OpenAI의 새로운 FrontierScience 테스트 1위 차지했으나 실제 연구 문제에선 고전",
  "tags": [],
  "impact_score": 7.5,
  "IS_Analysis": {
    "Score_Commentary": "Tier 1 OpenAI가 자사 최신 모델 GPT-5.2의 성능을 새로운 고난이도 벤치마크를 통해 검증. 과학 연구 분야에서의 AI 활용 가능성과 한계를 동시에 명확히 함.",
    "Calculations": {
      "IW_Analysis": {
        "Inputs": {
          "Pe_Selection_Rule": "P4",
          "Pe_Entity_Name": "OpenAI",
          "Pe_Tier": 1,
          "Se_Entity_Name": "Google",
          "Se_Tier": 1,
          "PE/SE 선정이유": "벤치마크 발표 및 1위 모델 개발사 OpenAI가 PE. 기사 내에서 Gemini 3 Pro(Google), Grok 4 등과 구체적 점수 비교를 수행하므로 Google(Tier 1)을 SE로 선정."
        },
        "Tier_Score": 3,
        "Gap_Score": 2,
        "IW_Score": 5
      },
      "IE_Analysis": {
        "Inputs": {
          "X_Magnitude_Code": 2,
          "Y_Evidence_Code": 4,
          "Scope_Matrix_Score": 1,
          "Criticality_C1_Provenness": 1,
          "Criticality_C2_Societal_Weight": 0.5,
          "Criticality_Total": 1.5,
          "SOTA_Check_Result": "True"
        },
        "IE_Score": 2.5
      }
    }
  },
  "zero_echo_score": 5.0,
  "ZES_Raw_Metrics": {
    "Signal": {
      "T1": 9,
      "T2": 7,
      "T3": 6,
      "Rationale": "각 모델별 점수(77%, 25%, 19% 등)와 벤치마크 구성 방식이 매우 구체적임."
    },
    "Noise": {
      "P1": 2,
      "P2": 4,
      "P3": 1,
      "Rationale": "자사 벤치마크를 통한 자사 모델 우위 입증이라는 한계 존재."
    },
    "Utility": {
      "V1": 7,
      "V2": 5,
      "V3": 8,
      "Rationale": "최신 모델들의 추론 능력 현주소를 파악하는 데 유용한 데이터."
    },
    "Fine_Adjustment": {
      "Score": 0,
      "Reason": "벤치마크 결과 보도이므로 표준적인 점수 부여."
    }
  },
  "schema_version": "V1.0",
  "raw_analysis": {
    "Article_ID": "707363",
    "Meta": {
      "Specification_Version": "v 1.0.0",
      "Headline": "GPT-5.2, OpenAI의 새로운 FrontierScience 테스트 1위 차지했으나 실제 연구 문제에선 고전",
      "Summary": "OpenAI의 새로운 벤치마크 FrontierScience에서 GPT-5.2가 올림피아드 수준 문제에서는 높은 점수(77%)를 기록했으나, 개방형 박사급 연구 과제에서는 25%의 정답률에 그쳤다. 이는 GPT-4o 등 기존 모델 대비 큰 향상이지만, 여전히 전문가 수준의 독자 연구 수행에는 한계가 있음을 보여준다. 구글, Anthropic 모델들과의 비교 결과도 포함되었다.",
      "Tags": [
        "Benchmark",
        "OpenAI",
        "GPT"
      ]
    },
    "IS_Analysis": {
      "Score_Commentary": "Tier 1 OpenAI가 자사 최신 모델 GPT-5.2의 성능을 새로운 고난이도 벤치마크를 통해 검증. 과학 연구 분야에서의 AI 활용 가능성과 한계를 동시에 명확히 함.",
      "Calculations": {
        "IW_Analysis": {
          "Inputs": {
            "Pe_Selection_Rule": "P4",
            "Pe_Entity_Name": "OpenAI",
            "Pe_Tier": 1,
            "Se_Entity_Name": "Google",
            "Se_Tier": 1,
            "PE/SE 선정이유": "벤치마크 발표 및 1위 모델 개발사 OpenAI가 PE. 기사 내에서 Gemini 3 Pro(Google), Grok 4 등과 구체적 점수 비교를 수행하므로 Google(Tier 1)을 SE로 선정."
          },
          "Tier_Score": 3,
          "Gap_Score": 2,
          "IW_Score": 5
        },
        "IE_Analysis": {
          "Inputs": {
            "X_Magnitude_Code": 2,
            "Y_Evidence_Code": 4,
            "Scope_Matrix_Score": 1,
            "Criticality_C1_Provenness": 1,
            "Criticality_C2_Societal_Weight": 0.5,
            "Criticality_Total": 1.5,
            "SOTA_Check_Result": "True"
          },
          "IE_Score": 2.5
        }
      }
    },
    "ZES_Raw_Metrics": {
      "Signal": {
        "T1": 9,
        "T2": 7,
        "T3": 6,
        "Rationale": "각 모델별 점수(77%, 25%, 19% 등)와 벤치마크 구성 방식이 매우 구체적임."
      },
      "Noise": {
        "P1": 2,
        "P2": 4,
        "P3": 1,
        "Rationale": "자사 벤치마크를 통한 자사 모델 우위 입증이라는 한계 존재."
      },
      "Utility": {
        "V1": 7,
        "V2": 5,
        "V3": 8,
        "Rationale": "최신 모델들의 추론 능력 현주소를 파악하는 데 유용한 데이터."
      },
      "Fine_Adjustment": {
        "Score": 0,
        "Reason": "벤치마크 결과 보도이므로 표준적인 점수 부여."
      }
    }
  },
  "source_id": "the_decoder",
  "original_title": "GPT-5.2 tops OpenAI's new FrontierScience test but struggles with real research problems",
  "evidence": {
    "breakdown": {
      "Signal": {
        "T1": 9.0,
        "T2": 7.0,
        "T3": 6.0,
        "S_Avg": 7.33
      },
      "Noise": {
        "P1": 2.0,
        "P2": 4.0,
        "P3": 1.0,
        "N_Avg": 2.33
      },
      "Utility": {
        "V1": 7.0,
        "V2": 5.0,
        "V3": 8.0,
        "U_Avg": 6.67
      },
      "Fine_Adjustment": 0.0,
      "ZS_Raw": 5.0,
      "ZS_Final": 5.0
    },
    "raw_metrics": {
      "Signal": {
        "T1": 9,
        "T2": 7,
        "T3": 6,
        "Rationale": "각 모델별 점수(77%, 25%, 19% 등)와 벤치마크 구성 방식이 매우 구체적임."
      },
      "Noise": {
        "P1": 2,
        "P2": 4,
        "P3": 1,
        "Rationale": "자사 벤치마크를 통한 자사 모델 우위 입증이라는 한계 존재."
      },
      "Utility": {
        "V1": 7,
        "V2": 5,
        "V3": 8,
        "Rationale": "최신 모델들의 추론 능력 현주소를 파악하는 데 유용한 데이터."
      },
      "Fine_Adjustment": {
        "Score": 0,
        "Reason": "벤치마크 결과 보도이므로 표준적인 점수 부여."
      }
    }
  },
  "impact_evidence": {
    "calculations": {
      "IW_Analysis": {
        "Tier_Score": 3.0,
        "Gap_Score": 2.0,
        "IW_Total": 5.0
      },
      "IE_Analysis": {
        "Scope_Total": 1.0,
        "Criticality_Total": 1.5,
        "IE_Total": 2.5
      },
      "IS_Raw": 7.5,
      "IS_Final": 7.5,
      "Score_Commentary": "Tier 1 OpenAI가 자사 최신 모델 GPT-5.2의 성능을 새로운 고난이도 벤치마크를 통해 검증. 과학 연구 분야에서의 AI 활용 가능성과 한계를 동시에 명확히 함."
    },
    "raw_inputs": {
      "Pe_Selection_Rule": "P4",
      "Pe_Entity_Name": "OpenAI",
      "Pe_Tier": 1,
      "Se_Entity_Name": "Google",
      "Se_Tier": 1,
      "PE/SE 선정이유": "벤치마크 발표 및 1위 모델 개발사 OpenAI가 PE. 기사 내에서 Gemini 3 Pro(Google), Grok 4 등과 구체적 점수 비교를 수행하므로 Google(Tier 1)을 SE로 선정."
    },
    "raw_ie_inputs": {
      "X_Magnitude_Code": 2,
      "Y_Evidence_Code": 4,
      "Scope_Matrix_Score": 1,
      "Criticality_C1_Provenness": 1,
      "Criticality_C2_Societal_Weight": 0.5,
      "Criticality_Total": 1.5,
      "SOTA_Check_Result": "True"
    }
  },
  "status": "reviewed",
  "staged": true,
  "staged_at": "2025-12-20T19:05:33.585958+00:00",
  "synced_to_firebase": true,
  "synced_at": "2025-12-21T16:12:13.920788+00:00"
}