{
  "article_id": "6082bd",
  "cached_at": "2025-12-18T15:33:44.878106+00:00",
  "image": "https://the-decoder.com/wp-content/uploads/2025/12/Nvidia-nemotron-3-release.jpg",
  "published_at": "2025-12-18T16:57:06.117421+00:00",
  "summary": "엔비디아가 맘바(Mamba)와 트랜스포머를 결합한 하이브리드 아키텍처 기반의 Nemotron 3 모델군을 공개했다. 메모리 효율성을 극대화하여 긴 문맥 처리에 강점이 있으며, 훈련 데이터와 가중치도 공개했다.",
  "text": "Jonathan writes for THE DECODER about how AI tools can improve both work and creative projects. Content Summary Nvidia's new Nemotron 3 family combines Mamba and Transformer architectures to handle long context windows without burning through resources. Ad The new model generation targets agent-based AI, systems that autonomously handle complex tasks over extended periods. The lineup includes three models: Nano, Super, and Ultra. Nano is available now, while Super and Ultra are scheduled for the first half of 2026. Nvidia breaks from the standard pure Transformer approach. Instead, it uses a hybrid structure combining efficient Mamba 2 layers with Transformer elements and a Mixture of Experts (MoE) approach, similar to systems IBM and Mistral have tested. This setup cuts resource use, especially for long input sequences. While pure Transformers need memory that grows linearly with input length, the Mamba layers here maintain a constant memory state during text generation. Ad Ad THE DECODER Newsletter The most important AI news straight to your inbox. ✓ Weekly ✓ Free ✓ Cancel at any time Please leave this field empty Nemotron 3 supports a one-million-token context window. This matches resource-heavy frontier models from OpenAI and Google, letting agents hold entire code repositories or long conversation histories in memory without spiking hardware demands. Hybrid architecture boosts efficiency The Nano model has 31.6 billion total parameters, but only 3 billion are active per processing step. On the Artificial Analysis Index benchmark, the open-source model rivals gpt-oss-20B and Qwen3-30B in accuracy but delivers significantly higher token throughput. However, according to Artificial Analysis, it requires 160 million tokens for a test run - far more than runner-up Qwen3-VL at 110 million. Share Recommend our article Share Nvidia introduces two architectural changes for the larger Super and Ultra models. The first, LatentMoE, addresses the memory bandwidth cost of routing tokens directly to expert networks in standard MoE models. The new method projects tokens into a compressed, latent representation before processing. Nvidia says this drastically increases expert count and active experts per token without slowing inference. The larger models also use multi-token prediction (MTP), where models predict several future tokens simultaneously during training rather than just the next one. This should improve logical reasoning and speed up text generation. Super and Ultra use the new NVFP4 4-bit floating point format, built for the Blackwell GPU architecture. Nvidia releases training data The scope of this release is unusual for a major AI player. Nvidia provided weights for the Nano version along with training recipes and most of the datasets on Hugging Face. The collection includes Nemotron-CC-v2.1 (2.5 trillion tokens based on Common Crawl), Nemotron-CC-Code-v1 (428 billion code tokens), and synthetic datasets for math, science, and security. The models used reinforcement learning across multiple environments simultaneously. This prevents the model from degrading in one area while improving in another. Developers can plug in their own RL environments through the NeMo Gym open-source library. The release aligns with Nvidia's recent push for smaller language models designed for agent-based tasks. Nemotron 3 follows this strategy by prioritizing speed over raw performance. The version numbering is a bit confusing: Nvidia already released Nemotron-4, which focuses on synthetic training data, in summer 2024. Ad",
  "title": "Nvidia's Nemotron 3 swaps pure Transformers for a Mamba hybrid to run AI agents efficiently",
  "url": "https://the-decoder.com/nvidias-nemotron-3-swaps-pure-transformers-for-a-mamba-hybrid-to-run-ai-agents-efficiently/",
  "title_ko": "엔비디아, AI 에이전트용 하이브리드 모델 'Nemotron 3' 공개",
  "tags": [],
  "impact_score": 7.0,
  "IS_Analysis": {
    "Score_Commentary": "새로운 아키텍처(Mamba Hybrid)를 도입하고 데이터를 공개하는 등 기술적 파급력(Scope)이 크다. PE가 Tier 1 하드웨어 기업에서 모델(SW) 영역으로 확장하는 중요한 움직임이다.",
    "Calculations": {
      "IW_Analysis": {
        "Inputs": {
          "Pe_Selection_Rule": "P4",
          "Pe_Entity_Name": "Nvidia",
          "Pe_Tier": 1,
          "Se_Entity_Name": "None",
          "Se_Tier": 0,
          "PE/SE 선정이유": "PE: 모델 개발 및 출시의 주체. SE: IBM/Mistral 등은 기술적 비교 대상일 뿐 직접적 협력/분쟁 관계 아님."
        },
        "Tier_Score": 3,
        "Gap_Score": 0,
        "IW_Score": 3
      },
      "IE_Analysis": {
        "Inputs": {
          "X_Magnitude_Code": 3,
          "Y_Evidence_Code": 3,
          "Scope_Matrix_Score": 2.5,
          "Criticality_C1_Provenness": 1,
          "Criticality_C2_Societal_Weight": 0.5,
          "Criticality_Total": 1.5,
          "SOTA_Check_Result": "True"
        },
        "IE_Score": 4
      }
    }
  },
  "zero_echo_score": 2.4,
  "ZES_Raw_Metrics": {
    "Signal": {
      "T1": 9,
      "T2": 9,
      "T3": 8,
      "Rationale": "파라미터 수, 토큰 처리량, 아키텍처 원리 등 기술적 상세 정보가 매우 풍부함."
    },
    "Noise": {
      "P1": 1,
      "P2": 0,
      "P3": 1,
      "Rationale": "기술적 사실 위주 서술로 노이즈가 거의 없음."
    },
    "Utility": {
      "V1": 6,
      "V2": 8,
      "V3": 8,
      "Rationale": "오픈소스 공개 및 새로운 아키텍처 제안으로 개발자 효용이 매우 높음."
    },
    "Fine_Adjustment": {
      "Score": 1,
      "Reason": "기술적 깊이와 구체성이 매우 뛰어난 고품질 기사."
    }
  },
  "schema_version": "V1.0",
  "raw_analysis": {
    "Article_ID": "6082bd",
    "Meta": {
      "Specification_Version": "v 1.0.0",
      "Headline": "엔비디아, AI 에이전트용 하이브리드 모델 'Nemotron 3' 공개",
      "Summary": "엔비디아가 맘바(Mamba)와 트랜스포머를 결합한 하이브리드 아키텍처 기반의 Nemotron 3 모델군을 공개했다. 메모리 효율성을 극대화하여 긴 문맥 처리에 강점이 있으며, 훈련 데이터와 가중치도 공개했다."
    },
    "IS_Analysis": {
      "Score_Commentary": "새로운 아키텍처(Mamba Hybrid)를 도입하고 데이터를 공개하는 등 기술적 파급력(Scope)이 크다. PE가 Tier 1 하드웨어 기업에서 모델(SW) 영역으로 확장하는 중요한 움직임이다.",
      "Calculations": {
        "IW_Analysis": {
          "Inputs": {
            "Pe_Selection_Rule": "P4",
            "Pe_Entity_Name": "Nvidia",
            "Pe_Tier": 1,
            "Se_Entity_Name": "None",
            "Se_Tier": 0,
            "PE/SE 선정이유": "PE: 모델 개발 및 출시의 주체. SE: IBM/Mistral 등은 기술적 비교 대상일 뿐 직접적 협력/분쟁 관계 아님."
          },
          "Tier_Score": 3,
          "Gap_Score": 0,
          "IW_Score": 3
        },
        "IE_Analysis": {
          "Inputs": {
            "X_Magnitude_Code": 3,
            "Y_Evidence_Code": 3,
            "Scope_Matrix_Score": 2.5,
            "Criticality_C1_Provenness": 1,
            "Criticality_C2_Societal_Weight": 0.5,
            "Criticality_Total": 1.5,
            "SOTA_Check_Result": "True"
          },
          "IE_Score": 4
        }
      }
    },
    "ZES_Raw_Metrics": {
      "Signal": {
        "T1": 9,
        "T2": 9,
        "T3": 8,
        "Rationale": "파라미터 수, 토큰 처리량, 아키텍처 원리 등 기술적 상세 정보가 매우 풍부함."
      },
      "Noise": {
        "P1": 1,
        "P2": 0,
        "P3": 1,
        "Rationale": "기술적 사실 위주 서술로 노이즈가 거의 없음."
      },
      "Utility": {
        "V1": 6,
        "V2": 8,
        "V3": 8,
        "Rationale": "오픈소스 공개 및 새로운 아키텍처 제안으로 개발자 효용이 매우 높음."
      },
      "Fine_Adjustment": {
        "Score": 1,
        "Reason": "기술적 깊이와 구체성이 매우 뛰어난 고품질 기사."
      }
    }
  },
  "source_id": "the_decoder",
  "original_title": "Nvidia's Nemotron 3 swaps pure Transformers for a Mamba hybrid to run AI agents efficiently",
  "evidence": {
    "breakdown": {
      "schema": "V1.0",
      "Signal": {
        "T1": 9.0,
        "T2": 9.0,
        "T3": 8.0,
        "S_Avg": 8.67,
        "Rationale": "파라미터 수, 토큰 처리량, 아키텍처 원리 등 기술적 상세 정보가 매우 풍부함."
      },
      "Noise": {
        "P1": 1.0,
        "P2": 0.0,
        "P3": 1.0,
        "N_Avg": 0.67,
        "Rationale": "기술적 사실 위주 서술로 노이즈가 거의 없음."
      },
      "Utility": {
        "V1": 6.0,
        "V2": 8.0,
        "V3": 8.0,
        "U_Avg": 7.33,
        "Rationale": "오픈소스 공개 및 새로운 아키텍처 제안으로 개발자 효용이 매우 높음."
      },
      "Fine_Adjustment": 1.0,
      "Fine_Reason": "기술적 깊이와 구체성이 매우 뛰어난 고품질 기사.",
      "ZS_Raw": 2.4,
      "ZS_Final": 2.4
    },
    "raw_metrics": {
      "Signal": {
        "T1": 9,
        "T2": 9,
        "T3": 8,
        "Rationale": "파라미터 수, 토큰 처리량, 아키텍처 원리 등 기술적 상세 정보가 매우 풍부함."
      },
      "Noise": {
        "P1": 1,
        "P2": 0,
        "P3": 1,
        "Rationale": "기술적 사실 위주 서술로 노이즈가 거의 없음."
      },
      "Utility": {
        "V1": 6,
        "V2": 8,
        "V3": 8,
        "Rationale": "오픈소스 공개 및 새로운 아키텍처 제안으로 개발자 효용이 매우 높음."
      },
      "Fine_Adjustment": {
        "Score": 1,
        "Reason": "기술적 깊이와 구체성이 매우 뛰어난 고품질 기사."
      }
    }
  },
  "impact_evidence": {
    "calculations": {
      "schema": "V1.0",
      "IW_Analysis": {
        "Tier_Score": 3.0,
        "Gap_Score": 0.0,
        "IW_Total": 3.0
      },
      "IE_Analysis": {
        "Scope_Total": 2.5,
        "Criticality_Total": 1.5,
        "IE_Total": 4.0
      },
      "IS_Raw": 7.0,
      "IS_Final": 7.0,
      "Score_Commentary": "새로운 아키텍처(Mamba Hybrid)를 도입하고 데이터를 공개하는 등 기술적 파급력(Scope)이 크다. PE가 Tier 1 하드웨어 기업에서 모델(SW) 영역으로 확장하는 중요한 움직임이다."
    },
    "raw_inputs": {
      "Pe_Selection_Rule": "P4",
      "Pe_Entity_Name": "Nvidia",
      "Pe_Tier": 1,
      "Se_Entity_Name": "None",
      "Se_Tier": 0,
      "PE/SE 선정이유": "PE: 모델 개발 및 출시의 주체. SE: IBM/Mistral 등은 기술적 비교 대상일 뿐 직접적 협력/분쟁 관계 아님."
    },
    "raw_ie_inputs": {
      "X_Magnitude_Code": 3,
      "Y_Evidence_Code": 3,
      "Scope_Matrix_Score": 2.5,
      "Criticality_C1_Provenness": 1,
      "Criticality_C2_Societal_Weight": 0.5,
      "Criticality_Total": 1.5,
      "SOTA_Check_Result": "True"
    },
    "schema_version": "V1.0"
  },
  "crawled_at": "2025-12-18T15:41:11.583743+00:00",
  "edition": "251218_THU_1",
  "status": "ACCEPTED",
  "saved": true,
  "saved_at": "2025-12-18T15:41:11.589841+00:00",
  "version": "V1.0",
  "staged_at": "2025-12-18T16:28:02.168381+00:00",
  "staged": true,
  "category": "AI/ML",
  "dedup_status": "selected",
  "published": true,
  "data_file": "the_decoder_6082bd.json"
}