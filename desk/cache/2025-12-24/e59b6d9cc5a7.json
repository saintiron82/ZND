{
  "title": "Ex-Tesla AI chief Andrej Karpathy shares four tips for AI startups competing with OpenAI",
  "description": "Karpathy sees Cursor as proof that a new category of AI applications is emerging. Startups shouldn't try to compete with the large language model labs head-on. Instead, they should focus on becoming specialists for vertical markets.",
  "image": "https://the-decoder.com/wp-content/uploads/2025/12/ai_startup_cliche-3.jpeg",
  "text": "Ask about this article… Search Karpathy sees Cursor as proof that a new category of AI applications is emerging. Startups shouldn't view themselves as competitors to the large language model labs but as specialists serving vertical markets. Ever since apps started building on top of language models from the major AI labs, there's been an ongoing debate about so-called \"AI wrappers\" - applications optimized for specific tasks or audiences, but whose core capabilities come from underlying language models. The question: can these apps differentiate enough to survive? Former Tesla AI chief Andrej Karpathy recently shared his take on the future of these AI startups. He finds the rise of Cursor, the AI-powered code editor, particularly striking. According to Karpathy, the tool \"convincingly revealed a new layer of an 'LLM app.'\" People now talk about \"Cursor for X,\" a sign that a new paradigm is taking hold. Ad What makes the new app layer work Karpathy says LLM apps like Cursor bundle and orchestrate LLM calls for specific verticals. He identifies four core functions: first, these apps perform what's known as \"context engineering.\" They prepare and structure the context fed to the language model, taking this strenuous but important work from the user. Ad DEC_D_Incontent-1 Second, they orchestrate multiple LLM calls under the hood, \"strung into increasingly more complex DAGs, carefully balancing performance and cost tradeoffs.\" Third, LLM apps provide an \"application-specific GUI for the human in the loop.\" Fourth, they offer an \"autonomy slider\": users can decide how much control to hand over and how independently the AI should act. Ad Startups vs. the major labs: who wins? The AI industry has been intensely debating how \"thick\" this new app layer really is ever since \"wrappers\" first appeared on the market. The central question: will the large language model labs like OpenAI, Anthropic, or Google cover all applications themselves, or is there room for specialized providers? Karpathy suspects the LLM labs will tend to train what he calls the \"generally capable college student\" - versatile but not specialized models. LLM apps, on the other hand, would organize and fine-tune teams of these models, turning them into deployed professionals in specific industries. Ad DEC_D_Incontent-2 The key lies in private data, tools for action, and real-world feedback. According to Karpathy, anyone who can feed information to an AI and let it do things—like trigger orders, send messages, or control machines—has a good chance of holding their own against the major labs. Ad But the labs won't back down easily: OpenAI has made crystal clear the company wants to cover the entire AI value chain, from chips to apps. Anthropic and Google are also continuously developing their AI chatbots to handle more and more everyday tasks.",
  "published_at": "2025-12-23T13:36:46+00:00",
  "url": "https://the-decoder.com/ex-tesla-ai-chief-andrej-karpathy-shares-four-tips-for-ai-startups-competing-with-openai/",
  "source_id": "the_decoder",
  "article_id": "e59b6d9cc5a7",
  "cached_at": "2025-12-23T15:48:57.789694+00:00"
}