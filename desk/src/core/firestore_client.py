# -*- coding: utf-8 -*-
"""
Firestore Client for Article Management
Firestore ì—°ë™ í´ë˜ìŠ¤ - ëª¨ë“  ë°ì´í„°ì˜ SSOT(Single Source of Truth)
"""
import os
from datetime import datetime, timezone
from typing import Optional, List, Dict, Any

import firebase_admin
from firebase_admin import credentials, firestore
from src.core_logic import get_kst_now # [IMPORTS]


class FirestoreClient:
    """Firestore ë°ì´í„°ë² ì´ìŠ¤ í´ë¼ì´ì–¸íŠ¸"""
    
    _instance = None
    _usage_stats = {
        'reads': 0,
        'writes': 0,
        'deletes': 0,
        'session_start': None
    }
    
    def __new__(cls):
        """ì‹±ê¸€í†¤ íŒ¨í„´"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        self.db = self._initialize_firebase()
        self._initialized = True
        
        # History Setup
        self.history = self._load_history()  # Local: URL -> timestamp
        self._remote_hashes = set()          # Remote: Hash set
        self._load_remote_history_hashes()   # Load remote hashes
        
        # Initialize usage stats
        self.reset_usage_stats()
        FirestoreClient._usage_stats['session_start'] = get_kst_now()
    
    def _initialize_firebase(self):
        """Firebase ì´ˆê¸°í™”"""
        if not firebase_admin._apps:
            # ì„œë¹„ìŠ¤ ê³„ì • í‚¤ ê²½ë¡œ íƒìƒ‰ (ì—¬ëŸ¬ ìœ„ì¹˜ í™•ì¸)
            base_dir = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
            project_root = os.path.dirname(base_dir)  # ZND ë£¨íŠ¸
            
            key_paths = [
                os.path.join(base_dir, 'zeroechodaily-serviceAccountKey.json'),
                os.path.join(base_dir, 'serviceAccountKey.json'),
                os.path.join(project_root, 'desk_arcive', 'zeroechodaily-serviceAccountKey.json'),
                os.path.join(project_root, 'zeroechodaily-serviceAccountKey.json'),
            ]
            
            key_path = None
            for path in key_paths:
                if os.path.exists(path):
                    key_path = path
                    print(f"âœ… Firebase key found: {path}")
                    break
            
            if key_path:
                cred = credentials.Certificate(key_path)
                firebase_admin.initialize_app(cred)
            else:
                raise FileNotFoundError(f"Firebase service account key not found. Searched: {key_paths}")
        
        return firestore.client()
    
    def get_env_name(self) -> str:
        """í™˜ê²½ ì„¤ì • ë°˜í™˜ (dev ë˜ëŠ” release)"""
        return os.getenv('ZND_ENV', 'dev')
    
    @staticmethod
    def get_schema_version() -> str:
        """ìŠ¤í‚¤ë§ˆ ë²„ì „ ë°˜í™˜ (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ìŒ)"""
        return os.getenv('SCHEMA_VERSION', '3.0')

    def _get_env(self) -> str:
        return self.get_env_name()
    
    def _get_collection(self, collection_name: str):
        """í™˜ê²½ë³„ ì»¬ë ‰ì…˜ ì°¸ì¡° ë°˜í™˜"""
        env = self._get_env()
        return self.db.collection(env).document('data').collection(collection_name)
    
    # =========================================================================
    # Helpers
    # =========================================================================



    def _get_cache_dir(self):
        """ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ ë°˜í™˜ (í™˜ê²½ë³„ ë¶„ë¦¬)"""
        base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        env = os.getenv('ZND_ENV', 'dev')
        return os.path.join(base_dir, 'cache', env)

    def _load_history(self):
        """crawling_history.json ë¡œë“œ (ë¡œì»¬ ì „ìš©)"""
        import json
        file_path = os.path.join(self._get_cache_dir(), 'crawling_history.json')
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return {}
        return {}
    
    def _load_remote_history_hashes(self):
        """Firestore íˆìŠ¤í† ë¦¬ ì¸ë±ìŠ¤ ë¡œë“œ (Hash Set)"""
        try:
            doc_ref = self._get_collection('history').document('_index')
            doc = doc_ref.get()
            self._track_read()

            if doc.exists:
                data = doc.to_dict()

                # Case 1: ì¤‘ì²© ê°ì²´ í˜•íƒœ {'urls': {'hash1': {...}, 'hash2': {...}}}
                urls_map = data.get('urls', {})
                if urls_map:
                    self._remote_hashes = set(urls_map.keys())
                else:
                    # Case 2: í”Œë« í‚¤ í˜•íƒœ {'urls.hash1': {...}, 'urls.hash2': {...}}
                    self._remote_hashes = set()
                    for key in data.keys():
                        if key.startswith('urls.'):
                            hash_part = key[5:]  # 'urls.' ì œê±°
                            self._remote_hashes.add(hash_part)

                print(f"ğŸ“¥ [History] Loaded {len(self._remote_hashes)} remote hashes")
        except Exception as e:
            print(f"âš ï¸ [History] Remote hash load failed: {e}")

    def _save_history_file(self):
        """crawling_history.json ì €ì¥ (ìµœê·¼ 5000ê°œ ìœ ì§€)"""
        import json
        file_path = os.path.join(self._get_cache_dir(), 'crawling_history.json')
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Limit to last 5000 entries
        if len(self.history) > 5000:
            keys_to_keep = list(self.history.keys())[-5000:]
            self.history = {k: self.history[k] for k in keys_to_keep}

        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(self.history, f, ensure_ascii=False, indent=2)

    # =========================================================================
    # Usage Tracking
    # =========================================================================
    
    @classmethod
    def _track_read(cls, count: int = 1):
        cls._usage_stats['reads'] += count
    
    @classmethod
    def _track_write(cls, count: int = 1):
        cls._usage_stats['writes'] += count
    
    @classmethod
    def _track_delete(cls, count: int = 1):
        cls._usage_stats['deletes'] += count
    
    @classmethod
    def get_usage_stats(cls) -> Dict[str, Any]:
        return cls._usage_stats.copy()
    def reset_usage_stats(cls):
        cls._usage_stats = {
            'reads': 0,
            'writes': 0,
            'deletes': 0,
            'session_start': get_kst_now() # [FIX] Use KST
        }
    
    # =========================================================================
    # Articles Collection CRUD
    # =========================================================================
    
    def get_article(self, article_id: str) -> Optional[Dict[str, Any]]:
        """
        ê¸°ì‚¬ ì¡°íšŒ (updated_at ê¸°ì¤€ ìµœì‹  ë°ì´í„°)
        - ë¡œì»¬/Firestore ë‘˜ ë‹¤ ì¡°íšŒ í›„ updated_at ë¹„êµ
        - ìµœì‹  ë°ì´í„°ê°€ ì •ë³¸
        """
        import glob
        import json
        
        local_data = None
        remote_data = None
        
        # 1. Local Cache ì¡°íšŒ
        try:
            base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            env = os.getenv('ZND_ENV', 'dev')
            cache_root = os.path.join(base_dir, 'cache', env)
            
            search_pattern = os.path.join(cache_root, '*', f'{article_id}.json')
            found_paths = glob.glob(search_pattern)
            
            # DEBUG
            if not found_paths:
                print(f"ğŸ” [DEBUG get_article] pattern='{search_pattern}', found={len(found_paths)}")
            
            if found_paths:
                found_paths.sort(key=os.path.getmtime, reverse=True)
                target_path = found_paths[0]
                
                with open(target_path, 'r', encoding='utf-8') as f:
                    local_data = json.load(f)
        except Exception as e:
            print(f"âš ï¸ [FirestoreClient] Local cache lookup failed: {e}")

        # 2. Firestore ì¡°íšŒ
        try:
            doc_ref = self._get_collection('articles').document(article_id)
            doc = doc_ref.get()
            self._track_read()
            
            if doc.exists:
                remote_data = doc.to_dict()
                remote_data['id'] = doc.id
        except Exception as e:
            print(f"âš ï¸ [FirestoreClient] Firestore lookup failed: {e}")
        
        # 3. Smart Merge (ì§€ëŠ¥í˜• ë³‘í•©)
        # ë‹¨ìˆœíˆ ìµœì‹  ê²ƒì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, "ì •ë³´ì˜ ì´ëŸ‰"ì„ ë³´ì¡´í•˜ë©° ìµœì‹  ìƒíƒœë¥¼ ë°˜ì˜
        
        if local_data and remote_data:
            local_header = local_data.get('_header', {})
            remote_header = remote_data.get('_header', {})
            
            local_time = local_header.get('updated_at', '')
            remote_time = remote_header.get('updated_at', '')
            
            remote_is_newer = remote_time >= local_time
            
            # ë°ì´í„° ì™„ì „ì„± ì²´í¬ (_original í•„ìˆ˜)
            local_complete = bool(local_data.get('_original'))
            remote_complete = bool(remote_data.get('_original'))
            
            # =========================================================
            # Smart Sync: ë’¤ì³ì§„ ìª½ë§Œ ì—…ë°ì´íŠ¸ (Optimization)
            # =========================================================
            
            if remote_is_newer:
                if remote_complete:
                    # Case 1: Remoteê°€ ì •ë³¸ -> Localë§Œ ì—…ë°ì´íŠ¸ (Cache Refresh)
                    try:
                        if target_path:
                            with open(target_path, 'w', encoding='utf-8') as f:
                                json.dump(remote_data, f, ensure_ascii=False, indent=2)
                            # print(f"ğŸ“¥ [Sync] Local cache updated from Firestore: {article_id}")
                    except Exception as e:
                        print(f"âš ï¸ [Sync] Local update failed: {e}")
                    return remote_data
                    
                elif local_complete:
                    # Case 2: Remoteê°€ ìµœì‹ ì´ë‚˜ ë¶ˆì™„ì „ -> Merge -> ë‘˜ ë‹¤ ì—…ë°ì´íŠ¸ (Repair & Sync)
                    print(f"ğŸ› ï¸ [Sync] Reconstructing sparse data for {article_id}")
                    merged = local_data.copy()
                    
                    if '_header' not in merged: merged['_header'] = {}
                    merged['_header'].update(remote_header)
                    
                    for key, val in remote_data.items():
                        if key not in ['_header', '_original'] and val:
                            merged[key] = val
                            
                    # 1. Fix Firestore
                    try:
                        self.save_article(article_id, merged)
                    except Exception as e:
                        print(f"âš ï¸ [Sync] Firestore repair failed: {e}")
                        
                    # 2. Update Local
                    try:
                        if target_path:
                            with open(target_path, 'w', encoding='utf-8') as f:
                                json.dump(merged, f, ensure_ascii=False, indent=2)
                    except Exception as e:
                        print(f"âš ï¸ [Sync] Local update failed: {e}")
                            
                    return merged
                else:
                    # Case 3: ë‘˜ ë‹¤ ë¶ˆì™„ì „ -> Remote ì‚¬ìš© (ë³µêµ¬ ë¶ˆê°€)
                    return remote_data
            else:
                # Case 4: Localì´ ì •ë³¸ -> Firestoreë§Œ ì—…ë°ì´íŠ¸ (Server Sync)
                # [ìµœì í™”] ì‹¤ì œë¡œ ë°ì´í„°ê°€ ë‹¤ë¥¼ ë•Œë§Œ ì“°ê¸° ìˆ˜í–‰
                # updated_at ì°¨ì´ê°€ ë¯¸ë¯¸í•˜ê±°ë‚˜ ìƒíƒœê°€ ê°™ìœ¼ë©´ ìŠ¤í‚µ

                local_state = local_header.get('state', '')
                remote_state = remote_header.get('state', '')

                # [FIX] ìƒíƒœ ì—­ì „ ë°©ì§€: PUBLISHED/RELEASEDë¥¼ ë‚®ì€ ìƒíƒœë¡œ ë®ì–´ì“°ì§€ ì•ŠìŒ
                # ìë™ ë™ê¸°í™”ì—ì„œë§Œ ì°¨ë‹¨, ìˆ˜ë™ UI ë³€ê²½(update_state)ì€ ë³„ë„ ê²½ë¡œë¡œ í—ˆìš©ë¨
                protected_states = {'PUBLISHED', 'RELEASED'}
                lower_states = {'COLLECTED', 'ANALYZED', 'CLASSIFIED', 'REJECTED'}

                if remote_state in protected_states and local_state in lower_states:
                    print(f"ğŸ›¡ï¸ [Sync] State downgrade blocked: {article_id} (Remote={remote_state}, Local={local_state})")
                    # Remote ë°ì´í„° ìœ ì§€, Local ìºì‹œë§Œ ì—…ë°ì´íŠ¸
                    try:
                        if target_path:
                            with open(target_path, 'w', encoding='utf-8') as f:
                                json.dump(remote_data, f, ensure_ascii=False, indent=2)
                            print(f"   ğŸ“¥ Local cache corrected to {remote_state}")
                    except Exception as e:
                        print(f"âš ï¸ [Sync] Local cache correction failed: {e}")
                    return remote_data

                # ìƒíƒœê°€ ê°™ê³  ì‹œê°„ ì°¨ì´ê°€ 1ì´ˆ ë¯¸ë§Œì´ë©´ ì“°ê¸° ìŠ¤í‚µ (ë¶ˆí•„ìš”í•œ ë™ê¸°í™” ë°©ì§€)
                time_diff_negligible = abs(len(local_time) - len(remote_time)) < 2 if local_time and remote_time else False
                same_state = local_state == remote_state

                if same_state and (local_time == remote_time or time_diff_negligible):
                    # ì´ë¯¸ ë™ê¸°í™”ë¨ - ì“°ê¸° ìŠ¤í‚µ
                    pass
                else:
                    try:
                        print(f"ğŸ“¤ [Sync] Pushing local changes to Firestore: {article_id} ({remote_state} -> {local_state})")
                        self.save_article(article_id, local_data)
                    except Exception as e:
                        print(f"âš ï¸ [Sync] Firestore update failed: {e}")

                return local_data
                
        elif local_data:
            return local_data
        elif remote_data:
            return remote_data
        
        return None


    def list_recent_articles(self, limit: int = 1000) -> List[Dict[str, Any]]:
        """ìµœê·¼ ê¸°ì‚¬ ëª©ë¡ ì¡°íšŒ (ì¤‘ë³µ ê²€ì‚¬ìš©)"""
        # Firestore Query
        query = self._get_collection('articles')\
            .order_by('_header.created_at', direction=firestore.Query.DESCENDING)\
            .limit(limit)
            
        docs = query.stream()
        # self._track_read(limit) # Stream reads count individually? No, batch approx.
        # Actually stream counts as 1 read per doc.
        
        results = []
        count = 0
        for doc in docs:
            data = doc.to_dict()
            data['id'] = doc.id
            results.append(data)
            count += 1
            
        self._track_read(count)
        return results
    
    def upsert_article_state(self, article_id: str, updates: Dict[str, Any]) -> tuple[bool, str]:
        """
        ê¸°ì‚¬ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì—†ìœ¼ë©´ ìƒì„±, ë¡œì»¬ íŒŒì¼ë„ ë™ê¸°í™” ì‹œë„)
        Args:
            article_id: ë¬¸ì„œ ID
            updates: ì—…ë°ì´íŠ¸í•  í•„ë“œ ë”•ì…”ë„ˆë¦¬
        """
        # 1. Local Cache Update
        try:
            import glob
            import json
            
            base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            env = os.getenv('ZND_ENV', 'dev')
            cache_root = os.path.join(base_dir, 'cache', env)
            
            # Robust search pattern
            search_pattern = os.path.join(cache_root, '**', f'*{article_id}.json')
            files = glob.glob(search_pattern, recursive=True)
            
            if files:
                target_file = files[0]
                with open(target_file, 'r', encoding='utf-8') as f:
                    content = json.load(f)
                
                # Apply dot-notation updates
                for key, value in updates.items():
                    parts = key.split('.')
                    target = content
                    for part in parts[:-1]:
                        if part not in target:
                             if isinstance(target, dict):
                                 target[part] = {}
                             target = target[part]
                    if isinstance(target, dict):
                        target[parts[-1]] = value
                
                with open(target_file, 'w', encoding='utf-8') as f:
                    json.dump(content, f, ensure_ascii=False, indent=2)
                print(f"âœ… [FirestoreClient] Local file updated during upsert: {target_file}")
        except Exception as e:
            print(f"âš ï¸ [FirestoreClient] Local upsert failed: {e}")

        # 2. Firestore Upsert
        try:
            doc_ref = self._get_collection('articles').document(article_id)
            doc_ref.set(updates, merge=True)
            self._track_write()
            print(f"âœï¸ [FirestoreClient] Firestore Upserted: {article_id}")
            return True, f"Upsert successful: {article_id}"
        except Exception as e:
            print(f"âŒ [FirestoreClient] Firestore Upsert Failed: {e}")
            return False, str(e)

    def _expand_dot_notation(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Convert dot.notation keys to nested dictionaries"""
        expanded = {}
        for key, value in data.items():
            parts = key.split('.')
            target = expanded
            for part in parts[:-1]:
                if part not in target:
                    target[part] = {}
                target = target[part]
                if not isinstance(target, dict): # Safety check
                     # If conflicting structure exists (e.g. valid string value replaced by dict), 
                     # we can't easily resolve without data loss. Overwrite.
                     target = {} 
            target[parts[-1]] = value
            
        # Merge logic to handle deep merges if needed? 
        # For 'set' on new doc, simplest expansion is defined above.
        # But wait, if multiple keys share path? e.g. 'a.b':1, 'a.c':2
        # My loop handles this because 'target' points to the same inner dict.
        return expanded

    def upsert_article_state(self, article_id: str, updates: Dict[str, Any]) -> tuple[bool, str]:
        """
        ê¸°ì‚¬ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì—†ìœ¼ë©´ ìƒì„± - Upsert)
        Args:
            article_id: ë¬¸ì„œ ID
            updates: ì—…ë°ì´íŠ¸í•  í•„ë“œ ë”•ì…”ë„ˆë¦¬
        """
        # 1. Local Cache Update
        try:
            import glob
            import json
            
            base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            env = os.getenv('ZND_ENV', 'dev')
            cache_root = os.path.join(base_dir, 'cache', env)
            
            # Robust search pattern (With or without underscore prefix)
            search_pattern = os.path.join(cache_root, '**', f'*{article_id}.json')
            files = glob.glob(search_pattern, recursive=True)
            
            if files:
                target_file = files[0]
                with open(target_file, 'r', encoding='utf-8') as f:
                    content = json.load(f)
                
                # Apply dot-notation locally
                # We can reuse _expand_dot_notation logic concept, but we need to merge INTO content.
                # Let's keep existing explicit merge logic for file safety.
                for key, value in updates.items():
                    parts = key.split('.')
                    target = content
                    for part in parts[:-1]:
                        if part not in target:
                            if isinstance(target, dict):
                                target[part] = {}
                            target = target[part]
                        elif isinstance(target[part], dict):
                            target = target[part]
                        else:
                            # Conflict: Trying to traverse non-dict
                            pass 
                    if isinstance(target, dict):
                         target[parts[-1]] = value
                
                # Ensure updated_at exists
                if '_header' in content:
                    content['_header']['updated_at'] = get_kst_now()

                with open(target_file, 'w', encoding='utf-8') as f:
                    json.dump(content, f, ensure_ascii=False, indent=2)
                print(f"âœ… [FirestoreClient] Local file updated during upsert: {target_file}")
        except Exception as e:
            print(f"âš ï¸ [FirestoreClient] Local upsert failed: {e}")

        # 2. Firestore Upsert
        # Critical Fix: 'set(merge=True)' does NOT support dot-notation for nesting.
        # MUST use 'update()' for dots, or expand dict for 'set()'.
        try:
            doc_ref = self._get_collection('articles').document(article_id)
            
            try:
                # Attempt UPDATE (Supports dot notation natively)
                doc_ref.update(updates)
                self._track_write()
                print(f"âœï¸ [FirestoreClient] Firestore Updated: {article_id}")
                return True, f"Update successful: {article_id}"
                
            except Exception as e:
                # Use string check for NotFound as commonly import might imply extra dependency
                if "404" in str(e) or "Not Found" in str(e) or "not found" in str(e):
                    # Document doesn't exist -> CREATE (Set)
                    # Must expand dots manually because set() interprets "a.b" as literal key.
                    expanded_data = self._expand_dot_notation(updates)
                    doc_ref.set(expanded_data)
                    self._track_write()
                    print(f"âœ¨ [FirestoreClient] Firestore Created (Set): {article_id}")
                    return True, f"Created successful: {article_id}"
                else:
                    raise e
                    
        except Exception as e:
            print(f"âŒ [FirestoreClient] Firestore Upsert Failed: {e}")
            return False, str(e)

    def save_article(self, article_id: str, data: Dict[str, Any]) -> bool:
        """ê¸°ì‚¬ ì €ì¥ (ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸)"""
        doc_ref = self._get_collection('articles').document(article_id)
        doc_ref.set(data, merge=True)
        self._track_write()
        return True
    
    def update_article(self, article_id: str, updates: Dict[str, Any]) -> bool:
        """ê¸°ì‚¬ ë¶€ë¶„ ì—…ë°ì´íŠ¸ (Firestore + Local Cache) - ë‘˜ ë‹¤ ì—…ë°ì´íŠ¸"""
        
        local_success = False
        firestore_success = False
        
        # 1. Try Local Cache Update first
        try:
            import glob
            import json
            from datetime import datetime
            
            base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            env = os.getenv('ZND_ENV', 'dev')
            cache_root = os.path.join(base_dir, 'cache', env)
            
            search_pattern = os.path.join(cache_root, '*', f'{article_id}.json')
            files = glob.glob(search_pattern)
            
            if files:
                target_file = files[0]
                print(f"ğŸ“‚ [FirestoreClient] Updating local file: {target_file}")
                
                with open(target_file, 'r', encoding='utf-8') as f:
                    content = json.load(f)
                
                # Apply dot-notation updates
                for key, value in updates.items():
                    parts = key.split('.')
                    target = content
                    for i, part in enumerate(parts[:-1]):
                        if part not in target:
                            target[part] = {}
                        target = target[part]
                    target[parts[-1]] = value
                
                with open(target_file, 'w', encoding='utf-8') as f:
                    json.dump(content, f, ensure_ascii=False, indent=2)
                
                print(f"âœ… [FirestoreClient] Local file updated: {article_id}")
                local_success = True
                
        except Exception as e:
            print(f"âš ï¸ [FirestoreClient] Local update failed: {e}")

        # 2. Always try Firestore update (not just fallback)
        try:
            doc_ref = self._get_collection('articles').document(article_id)
            doc_ref.update(updates)
            self._track_write()
            firestore_success = True
            print(f"âœ… [FirestoreClient] Firestore updated: {article_id}")
        except Exception as e:
            print(f"âš ï¸ [FirestoreClient] Firestore update failed: {e}")
        
        return local_success or firestore_success
    
    def delete_article(self, article_id: str) -> bool:
        """ê¸°ì‚¬ ì‚­ì œ"""
        doc_ref = self._get_collection('articles').document(article_id)
        doc_ref.delete()
        self._track_delete()
        return True
    
    def list_articles_by_state(self, state: str, limit: int = 100) -> List[Dict[str, Any]]:
        """ìƒíƒœë³„ ê¸°ì‚¬ ëª©ë¡ ì¡°íšŒ (ë¡œì»¬ ì½ê¸° + Firestore, updated_at ê¸°ì¤€ ìµœì‹  ìš°ì„ )"""
        local_articles = {}  # article_id -> article
        firestore_articles = {}
        
        # 1. Local Cache ê²€ìƒ‰ (ì½ê¸° ì „ìš©)
        if state in ['COLLECTED', 'ANALYZED', 'CLASSIFIED', 'PUBLISHED', 'REJECTED']:
            try:
                import glob
                import json
                
                base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
                env = os.getenv('ZND_ENV', 'dev')
                cache_root = os.path.join(base_dir, 'cache', env)
                
                if os.path.exists(cache_root):
                    files = glob.glob(os.path.join(cache_root, '*', '*.json'))
                    files.sort(key=os.path.getmtime, reverse=True)
                    
                    for fpath in files[:limit * 2]:  # ì—¬ìœ ìˆê²Œ ë¡œë“œ
                        try:
                            with open(fpath, 'r', encoding='utf-8') as f:
                                content = json.load(f)
                                
                            if '_header' in content:
                                file_state = content['_header'].get('state')
                                article_id = content['_header'].get('article_id')
                                if file_state == state and article_id:
                                    local_articles[article_id] = content
                        except Exception:
                            continue
                    print(f"ğŸ“‚ [Local] Loaded {len(local_articles)} articles for state {state}")
            except Exception as e:
                print(f"âš ï¸ Local cache search failed: {e}")

        # 2. Firestore ê²€ìƒ‰
        try:
            query = self._get_collection('articles').where(
                '_header.state', '==', state
            ).limit(limit * 2)
            
            docs = query.stream()
            self._track_read()
            
            for doc in docs:
                data = doc.to_dict()
                data['id'] = doc.id
                article_id = data.get('_header', {}).get('article_id') or doc.id
                firestore_articles[article_id] = data
            print(f"â˜ï¸ [Firestore] Loaded {len(firestore_articles)} articles for state {state}")
        except Exception as e:
            print(f"âš ï¸ Firestore search failed: {e}")
        
        # 3. ë³‘í•©: updated_at ê¸°ì¤€ ìµœì‹  ë°ì´í„° ìš°ì„  + ë°ì´í„° ì™„ì „ì„± ê²€ì‚¬
        merged = {}
        all_ids = set(local_articles.keys()) | set(firestore_articles.keys())
        
        def is_complete(article):
            """ë°ì´í„° ì™„ì „ì„± ê²€ì‚¬: _original.url í•„ìˆ˜"""
            if not article:
                return False
            original = article.get('_original', {})
            return bool(original.get('url'))
        
        for aid in all_ids:
            local = local_articles.get(aid)
            remote = firestore_articles.get(aid)
            
            if local and remote:
                # ë‘˜ ë‹¤ ìˆìœ¼ë©´: ì™„ì „ì„±ê³¼ updated_at í•¨ê»˜ ê³ ë ¤
                local_complete = is_complete(local)
                remote_complete = is_complete(remote)
                
                if local_complete and not remote_complete:
                    # Localë§Œ ì™„ì „ -> Local ì‚¬ìš©
                    merged[aid] = local
                elif remote_complete and not local_complete:
                    # Remoteë§Œ ì™„ì „ -> Remote ì‚¬ìš©
                    merged[aid] = remote
                else:
                    # ë‘˜ ë‹¤ ì™„ì „í•˜ê±°ë‚˜ ë‘˜ ë‹¤ ë¶ˆì™„ì „ -> updated_at ë¹„êµ
                    local_time = local.get('_header', {}).get('updated_at', '')
                    remote_time = remote.get('_header', {}).get('updated_at', '')
                    merged[aid] = remote if remote_time >= local_time else local
            elif remote:
                merged[aid] = remote
            elif local:
                merged[aid] = local

        
        # 4. ì •ë ¬ ë° ì œí•œ
        result = list(merged.values())
        result.sort(key=lambda x: x.get('_header', {}).get('updated_at', ''), reverse=True)
        
        return result[:limit]
    
    def list_recent_articles(self, limit: int = 100) -> List[Dict[str, Any]]:
        """ìµœê·¼ ê¸°ì‚¬ ëª©ë¡ ì¡°íšŒ"""
        query = self._get_collection('articles').order_by(
            '_header.updated_at', direction=firestore.Query.DESCENDING
        ).limit(limit)
        
        docs = query.stream()
        self._track_read()
        
        articles = []
        for doc in docs:
            data = doc.to_dict()
            data['id'] = doc.id
            articles.append(data)
        
        return articles
    
    # =========================================================================
    # History Collection (URL â†’ article_id ë§¤í•‘)
    # =========================================================================
    
    def get_history_index(self) -> Dict[str, Any]:
        """íˆìŠ¤í† ë¦¬ ì¸ë±ìŠ¤ ì¡°íšŒ"""
        doc_ref = self._get_collection('history').document('_index')
        doc = doc_ref.get()
        self._track_read()
        
        if doc.exists:
            return doc.to_dict()
        return {'urls': {}}
    
    def update_history(self, url: str, article_id: str, status: str):
        """íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ (Firestore + ëŸ°íƒ€ì„ ìºì‹œ)"""
        url_hash = self._url_to_key(url)

        # 1. Firestore ì—…ë°ì´íŠ¸
        doc_ref = self._get_collection('history').document('_index')
        doc_ref.set({
            f'urls.{url_hash}': {
                'article_id': article_id,
                'status': status,
                'updated_at': get_kst_now()
            }
        }, merge=True)
        self._track_write()

        # 2. ëŸ°íƒ€ì„ í•´ì‹œ ìºì‹œì—ë„ ì¶”ê°€ (ì¤‘ë³µ ìˆ˜ì§‘ ë°©ì§€)
        self._remote_hashes.add(url_hash)

        # 3. ë¡œì»¬ íˆìŠ¤í† ë¦¬ì—ë„ ì¶”ê°€ (ì„¸ì…˜ ê°„ ì¤‘ë³µ ë°©ì§€)
        self.history[url] = get_kst_now()
        self._save_history_file()
    
    def check_url_exists(self, url: str) -> Optional[Dict[str, Any]]:
        """URLì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        history = self.get_history_index()
        url_key = self._url_to_key(url)
        return history.get('urls', {}).get(url_key)
    
    def _url_to_key(self, url: str) -> str:
        """URLì„ Firestore í‚¤ë¡œ ë³€í™˜ (ì •ê·œí™” í›„ í•´ì‹œ)"""
        import hashlib
        # URL ì •ê·œí™”: ë ìŠ¬ë˜ì‹œ ì œê±°í•˜ì—¬ ì¼ê´€ëœ í‚¤ ìƒì„±
        normalized_url = url.rstrip('/')
        return hashlib.md5(normalized_url.encode()).hexdigest()[:12]
    
    # =========================================================================
    # Local History Management (Ported from DBClient)
    # =========================================================================

    def check_history(self, url: str) -> bool:
        """
        URL ì²˜ë¦¬ ì—¬ë¶€ í™•ì¸ (ë¡œì»¬ + ì›ê²© í•´ì‹œ ì²´í¬)
        """
        # 1. Local Check (Frequency: High, Cost: Low)
        if url in self.history:
            return True
            
        # 2. Remote Hash Check (Frequency: Low, Cost: Low - InMemory Set)
        url_hash = self._url_to_key(url)
        if url_hash in self._remote_hashes:
            return True
            
        return False

    def get_history_status(self, url: str) -> Optional[str]:
        """(Deprecated) íˆìŠ¤í† ë¦¬ ìƒíƒœ ë°˜í™˜ - í˜¸í™˜ì„± ìœ ì§€ìš©"""
        if url in self.history:
            return "VISITED"
        return None

    def save_history(self, url: str, status: str = None, reason: str = None, article_id: str = None):
        """íˆìŠ¤í† ë¦¬ ì €ì¥ (URL ë°©ë¬¸ ê¸°ë¡) - ë¡œì»¬ + Firestore ë‘˜ ë‹¤"""
        import hashlib
        
        # ë¡œì»¬ íˆìŠ¤í† ë¦¬ ì €ì¥
        self.history[url] = get_kst_now()
        self._save_history_file()
        
        # [FIX] article_id ì—†ìœ¼ë©´ ìë™ ìƒì„±
        if not article_id:
            article_id = hashlib.md5(url.encode()).hexdigest()[:12]
        
        # Firestore íˆìŠ¤í† ë¦¬ í•­ìƒ ë™ê¸°í™” (ì¡°ê±´ ì œê±°)
        try:
            self.update_history(url, article_id, status or 'VISITED')
            # ëŸ°íƒ€ì„ í•´ì‹œì…‹ë„ ê°±ì‹ 
            url_hash = self._url_to_key(url)
            self._remote_hashes.add(url_hash)
        except Exception as e:
            print(f"âš ï¸ [History] Firestore sync failed: {e}")

    def refresh_remote_hashes(self):
        """ì›ê²© íˆìŠ¤í† ë¦¬ í•´ì‹œ ê°•ì œ ìƒˆë¡œê³ ì¹¨ (ì‚¬ì´íŠ¸ ì¬ì˜¤í”ˆ ì‹œ)"""
        self._load_remote_history_hashes()
        print(f"ğŸ”„ [History] Refreshed: {len(self._remote_hashes)} remote hashes")

    def remove_from_history(self, url: str):
        """íˆìŠ¤í† ë¦¬ì—ì„œ ì œê±° (ì¬ì²˜ë¦¬ìš©)"""
        if url in self.history:
            del self.history[url]
            self._save_history_file()
            print(f"ğŸ—‘ï¸ [History] Removed from history: {url[:50]}...")
        else:
            print(f"âš ï¸ [History] URL not found in history: {url[:50]}...")

    # =========================================================================
    # Crawler Support (Ported from DBClient)
    # =========================================================================

    def save_crawled_article(self, article_data: Dict[str, Any]):
        """
        í¬ë¡¤ëŸ¬ ìˆ˜ì§‘ ë°ì´í„° ì €ì¥ (V2 Schema ë³€í™˜ ë° ì €ì¥)
        DBClient.save_article ë¡œì§ ì´ì‹
        """
        import hashlib
        
        # Ensure crawled_at
        crawled_at = article_data.get('crawled_at')
        now = get_kst_now()
        if not crawled_at:
             crawled_at = now
             article_data['crawled_at'] = now

        # Generate ID
        url = article_data.get('url', '')
        article_id = article_data.get('article_id') or hashlib.md5(url.encode()).hexdigest()[:12]
        
        # V2 Schema Construction
        v2_article = {
            '_header': {
                'version': self.get_schema_version(),
                'article_id': article_id,
                'state': 'ANALYZED',  # ì €ì¥ ì‹œ ANALYZED ìƒíƒœ (pipeline íë¦„ìƒ)
                'created_at': crawled_at,
                'updated_at': now,
                'state_history': [
                    {'state': 'COLLECTED', 'at': crawled_at, 'by': 'crawler'},
                    {'state': 'ANALYZED', 'at': now, 'by': 'pipeline'}
                ]
            },
            '_original': {
                'url': url,
                'title': article_data.get('original_title') or article_data.get('title', ''),
                'text': article_data.get('text', '')[:5000],
                'image': article_data.get('image'),
                'source_id': article_data.get('source_id', 'unknown'),
                'crawled_at': crawled_at,
                'published_at': article_data.get('published_at')
            },
            '_analysis': {
                'title_ko': article_data.get('title_ko') or article_data.get('title', ''),
                'summary': article_data.get('summary', ''),
                'tags': article_data.get('tags', []),
                'impact_score': float(article_data.get('impact_score', 0) or 0),
                'zero_echo_score': float(article_data.get('zero_echo_score', 0) or 0),
                'analyzed_at': now,
                'mll_raw': article_data.get('raw_analysis')
            },
            '_classification': None,
            '_publication': None
        }
        
        # Save to Firestore (via existing upsert logic for consistency)
        # Using upsert_article_state might be tricky for full replace/create of structure.
        # Direct set is better for new articles.
        try:
            self._get_collection('articles').document(article_id).set(v2_article, merge=True)
            self._track_write()
            print(f"âœ… [FirestoreClient] Saved crawled article: {article_id}")
            
            # Update History
            if url:
                self.save_history(url, 'ANALYZED', reason='mll_complete')
                
        except Exception as e:
            print(f"âŒ [FirestoreClient] Save crawled article failed: {e}")
    
    # =========================================================================
    # Publications Collection
    # =========================================================================
    
    def get_publication(self, edition_code: str) -> Optional[Dict[str, Any]]:
        """ë°œí–‰ ì •ë³´ ì¡°íšŒ"""
        doc_ref = self._get_collection('publications').document(edition_code)
        doc = doc_ref.get()
        self._track_read()
        
        if doc.exists:
            return doc.to_dict()
        return None
    
    def save_publication(self, edition_code: str, data: Dict[str, Any]) -> bool:
        """ë°œí–‰ ì •ë³´ ì €ì¥"""
        doc_ref = self._get_collection('publications').document(edition_code)
        doc_ref.set(data, merge=True)
        self._track_write()
        return True
    
    def get_publications_meta(self) -> Optional[Dict[str, Any]]:
        """ë°œí–‰ ë©”íƒ€ ì •ë³´ ì¡°íšŒ"""
        doc_ref = self._get_collection('publications').document('_meta')
        doc = doc_ref.get()
        self._track_read()
        
        if doc.exists:
            return doc.to_dict()
        return None

    def get_issues_from_meta(self, status_filter=None) -> List[Dict[str, Any]]:
        """
        _meta ë¬¸ì„œì—ì„œ íšŒì°¨ ëª©ë¡ ì¡°íšŒ (1 READë¡œ ìµœì í™”)
        Args:
            status_filter: 'preview' ë˜ëŠ” 'released' (Noneì´ë©´ ì „ì²´)
        Returns: list of issue dicts
        """
        meta = self.get_publications_meta()
        if not meta:
            return []
        
        issues = meta.get('issues', [])
        
        # status í•„í„° ì ìš©
        if status_filter:
            issues = [i for i in issues if i.get('status') == status_filter]
        
        # ì‹œìŠ¤í…œ ë¬¸ì„œ í•„í„°ë§ (edition_codeê°€ '_'ë¡œ ì‹œì‘í•˜ëŠ” í•­ëª© ì œì™¸)
        issues = [i for i in issues if not i.get('edition_code', '').startswith('_')]
        
        # edition_code ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ë°œí–‰ìˆœ ìœ ì§€)
        issues.sort(key=lambda x: x.get('edition_code', ''), reverse=True)
        
        # API ì‘ë‹µ í˜•ì‹ì— ë§ê²Œ ë³€í™˜ (ë ˆê±°ì‹œ í•„ë“œ ì œê±°ë¨)
        result = []
        for iss in issues:
            result.append({
                'edition_code': iss.get('edition_code'),
                'edition_name': iss.get('edition_name'),
                'index': iss.get('index', 1),
                'article_count': iss.get('article_count', 0),
                'published_at': iss.get('published_at'),
                'updated_at': iss.get('updated_at'),
                'status': iss.get('status', 'preview'),
                'schema_version': iss.get('schema_version', '3.1')
            })
        
        print(f"ğŸ“‹ [Firestore] Loaded {len(result)} issues from _meta (1 READ)")
        return result
    
    def update_publications_meta(self, data: Dict[str, Any]) -> bool:
        """ë°œí–‰ ë©”íƒ€ ì •ë³´ ì—…ë°ì´íŠ¸"""
        doc_ref = self._get_collection('publications').document('_meta')
        doc_ref.set(data, merge=True)
        self._track_write()
        return True

    def list_publications(self, limit: int = 20) -> List[Dict[str, Any]]:
        """ë°œí–‰ íšŒì°¨ ëª©ë¡ ì¡°íšŒ (ìµœì‹ ìˆœ)"""
        query = self._get_collection('publications')\
            .order_by('published_at', direction=firestore.Query.DESCENDING)\
            .limit(limit)
        
        docs = query.stream()
        self._track_read()
        
        return [doc.to_dict() for doc in docs]

    def list_articles_by_edition(self, edition_code: str) -> List[Dict[str, Any]]:
        """íšŒì°¨ë³„ ê¸°ì‚¬ ëª©ë¡ ì¡°íšŒ (publications/{edition_code} ë¬¸ì„œì˜ articles í•„ë“œ)"""
        # publications/{edition_code} ë¬¸ì„œì—ì„œ articles ë°°ì—´ ì½ê¸°
        pub_doc = self.get_publication(edition_code)
        if pub_doc and 'articles' in pub_doc:
            self._track_read()
            return pub_doc['articles']
        
        # Fallback: articles ì»¬ë ‰ì…˜ì—ì„œ ì¿¼ë¦¬ (êµ¬ë²„ì „ í˜¸í™˜)
        query = self._get_collection('articles').where(
            '_publication.edition_code', '==', edition_code
        )
        
        docs = query.stream()
        self._track_read()
        
        articles = []
        for doc in docs:
            data = doc.to_dict()
            data['id'] = doc.id
            articles.append(data)
        return articles

    # =========================================================================
    # Trend Reports Collection
    # =========================================================================
    
    def save_trend_report(self, report_id: str, data: Dict[str, Any]) -> bool:
        """íŠ¸ë Œë“œ ë¦¬í¬íŠ¸ ì €ì¥"""
        doc_ref = self._get_collection('trend_reports').document(report_id)
        doc_ref.set(data, merge=True)
        self._track_write()
        print(f"âœ… [FirestoreClient] Trend report saved: {report_id}")
        return True
    
    def get_trend_report(self, report_id: str) -> Optional[Dict[str, Any]]:
        """íŠ¸ë Œë“œ ë¦¬í¬íŠ¸ ì¡°íšŒ"""
        doc_ref = self._get_collection('trend_reports').document(report_id)
        doc = doc_ref.get()
        self._track_read()
        
        if doc.exists:
            data = doc.to_dict()
            data['id'] = doc.id
            return data
        return None
    
    def list_trend_reports(self, limit: int = 20) -> List[Dict[str, Any]]:
        """íŠ¸ë Œë“œ ë¦¬í¬íŠ¸ ëª©ë¡ ì¡°íšŒ (ìµœì‹ ìˆœ)"""
        query = self._get_collection('trend_reports')\
            .order_by('created_at', direction=firestore.Query.DESCENDING)\
            .limit(limit)
        
        docs = query.stream()
        self._track_read()
        
        reports = []
        for doc in docs:
            data = doc.to_dict()
            data['id'] = doc.id
            reports.append({
                'id': data.get('id'),
                'period': data.get('period', {}),
                'created_at': data.get('created_at')
            })
        return reports
    
    def delete_trend_report(self, report_id: str) -> bool:
        """íŠ¸ë Œë“œ ë¦¬í¬íŠ¸ ì‚­ì œ"""
        try:
            doc_ref = self._get_collection('trend_reports').document(report_id)
            doc_ref.delete()
            self._track_delete()
            print(f"ğŸ—‘ï¸ [FirestoreClient] Trend report deleted: {report_id}")
            return True
        except Exception as e:
            print(f"âŒ [FirestoreClient] Failed to delete report: {e}")
            return False

