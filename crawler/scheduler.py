# -*- coding: utf-8 -*-
"""
ZND Crawler Scheduler - PM2 Entry Point
APScheduler ê¸°ë°˜ ë…ë¦½ ìŠ¤ì¼€ì¤„ëŸ¬

ì‹¤í–‰: python scheduler.py
PM2: pm2 start ecosystem.config.js

[V2] desk ì½”ì–´ ê¸°ë°˜ í†µí•© íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
"""
import os
import sys
import json
import signal
import time
from datetime import datetime

# ê²½ë¡œ ì„¤ì •
CRAWLER_DIR = os.path.dirname(os.path.abspath(__file__))
ZND_ROOT = os.path.dirname(CRAWLER_DIR)
DESK_DIR = os.path.join(ZND_ROOT, 'desk')

# Add paths for imports
sys.path.insert(0, CRAWLER_DIR)  # for core.xxx
sys.path.insert(0, ZND_ROOT)     # for crawler.xxx (when called from outside)
sys.path.insert(0, DESK_DIR)     # for desk modules

from apscheduler.schedulers.blocking import BlockingScheduler
from apscheduler.triggers.cron import CronTrigger

# [V2] desk í†µí•© íŒŒì´í”„ë¼ì¸ ì‚¬ìš© (ê¸°ì¡´ extractor ëŒ€ì²´)
from src.scheduler_pipeline import (
    run_pipeline, 
    PipelinePhase,
    PHASES_COLLECT_ONLY,
    PHASES_UNTIL_PUBLISH
)
from core.logger import log_crawl_event

# ì„¤ì • íŒŒì¼ ê²½ë¡œ
CONFIG_DIR = os.path.join(CRAWLER_DIR, 'config')
SCHEDULES_FILE = os.path.join(CONFIG_DIR, 'schedules.json')


def load_schedules() -> list:
    """ìŠ¤ì¼€ì¤„ ì„¤ì • ë¡œë“œ"""
    if not os.path.exists(SCHEDULES_FILE):
        return []
    try:
        with open(SCHEDULES_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('schedules', [])
    except Exception as e:
        print(f"âŒ Failed to load schedules: {e}")
        return []


def save_schedules(schedules: list):
    """ìŠ¤ì¼€ì¤„ ì„¤ì • ì €ì¥"""
    os.makedirs(CONFIG_DIR, exist_ok=True)
    with open(SCHEDULES_FILE, 'w', encoding='utf-8') as f:
        json.dump({'schedules': schedules}, f, indent=2, ensure_ascii=False)


def run_scheduled_crawl(schedule_name: str = "Scheduled", phases: list = None):
    """
    ìŠ¤ì¼€ì¤„ì— ì˜í•´ í˜¸ì¶œë˜ëŠ” í¬ë¡¤ë§ ì‘ì—…
    
    [V2] desk í†µí•© íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
    """
    print(f"\n{'='*50}")
    print(f"â° [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Scheduled crawl triggered")
    print(f"ğŸ“‹ Schedule: {schedule_name}")
    print(f"{'='*50}\n")
    
    try:
        # [V2] ìƒˆ íŒŒì´í”„ë¼ì¸ ì‚¬ìš© (desk ì½”ì–´ ì§ì ‘ í˜¸ì¶œ)
        if phases is None:
            phases = PHASES_COLLECT_ONLY  # ê¸°ë³¸: ìˆ˜ì§‘ë§Œ
        
        result = run_pipeline(phases=phases, schedule_name=schedule_name)
        log_crawl_event("Scheduled", f"Pipeline completed: {result.message}", 0, success=result.success)
        
        # ë””ìŠ¤ì½”ë“œ ì•Œë¦¼
        if result.collected > 0 or result.extracted > 0:
            try:
                from core.discord_notifier import send_simple_message
                send_simple_message(
                    f"ğŸ“¡ {schedule_name} ì™„ë£Œ\n{result.message}",
                    "ğŸ• ìŠ¤ì¼€ì¤„ ìˆ˜ì§‘"
                )
            except Exception as e:
                print(f"âš ï¸ [Discord] Notification failed: {e}")
                
    except Exception as e:
        log_crawl_event("Scheduled", f"Pipeline failed: {str(e)}", 0, success=False)
        print(f"âŒ Scheduled crawl error: {e}")


def run_scheduled_sync():
    """ìŠ¤ì¼€ì¤„ì— ì˜í•´ í˜¸ì¶œë˜ëŠ” ìºì‹œ ë™ê¸°í™” ì‘ì—…"""
    print(f"\n{'='*50}")
    print(f"â˜ï¸ [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Scheduled sync triggered")
    print(f"{'='*50}\n")
    
    try:
        # desk ëª¨ë“ˆì˜ db_client ì‚¬ìš©
        from src.db_client import DBClient
        import os as sync_os
        
        db = DBClient()
        if not db.db:
            print("âŒ [Sync] Firestore ì—°ê²° ì‹¤íŒ¨")
            return
        
        CACHE_DIR = sync_os.path.join(ZND_ROOT, 'desk', 'cache')
        synced_count = 0
        skipped_count = 0
        
        # ëª¨ë“  ë‚ ì§œ í´ë” ë™ê¸°í™”
        if sync_os.path.exists(CACHE_DIR):
            for date_folder in sync_os.listdir(CACHE_DIR):
                if not sync_os.path.isdir(sync_os.path.join(CACHE_DIR, date_folder)):
                    continue
                if len(date_folder) != 10:  # YYYY-MM-DD í˜•ì‹ë§Œ
                    continue
                
                cache_date_dir = sync_os.path.join(CACHE_DIR, date_folder)
                cache_list = []
                
                import json as sync_json
                from datetime import timezone
                
                for filename in sync_os.listdir(cache_date_dir):
                    if not filename.endswith('.json'):
                        continue
                    
                    filepath = sync_os.path.join(cache_date_dir, filename)
                    try:
                        with open(filepath, 'r', encoding='utf-8-sig') as f:
                            cache_data = sync_json.load(f)
                        
                        # synced_atì´ ìˆìœ¼ë©´ ìŠ¤í‚µ
                        synced_at = cache_data.get('synced_at')
                        updated_at = cache_data.get('updated_at') or cache_data.get('staged_at')
                        
                        if synced_at:
                            if updated_at and updated_at > synced_at:
                                pass  # ë³€ê²½ë¨ -> ì—…ë¡œë“œ
                            else:
                                skipped_count += 1
                                continue
                        
                        article_id = cache_data.get('article_id')
                        if not article_id:
                            article_id = filename.replace('.json', '')
                            cache_data['article_id'] = article_id
                        
                        cache_list.append(cache_data)
                    except:
                        pass
                
                # ë°°ì¹˜ ì—…ë¡œë“œ
                if cache_list:
                    now = datetime.now(timezone.utc).isoformat()
                    for cache_data in cache_list:
                        cache_data['synced_at'] = now
                    
                    result = db.upload_cache_batch(date_folder, cache_list)
                    synced_count += result.get('success', 0)
                    
                    # ë¡œì»¬ íŒŒì¼ì— synced_at ë§ˆí‚¹
                    for cache_data in cache_list:
                        article_id = cache_data.get('article_id')
                        filepath = sync_os.path.join(cache_date_dir, f"{article_id}.json")
                        if sync_os.path.exists(filepath):
                            try:
                                with open(filepath, 'r', encoding='utf-8-sig') as f:
                                    file_data = sync_json.load(f)
                                file_data['synced_at'] = now
                                with open(filepath, 'w', encoding='utf-8') as f:
                                    sync_json.dump(file_data, f, ensure_ascii=False, indent=2)
                            except:
                                pass
        
        print(f"â˜ï¸ [Sync] ë™ê¸°í™” ì™„ë£Œ: {synced_count}ê°œ, ìŠ¤í‚µ: {skipped_count}ê°œ")
        log_crawl_event("Sync", f"Synced {synced_count} items", 0, success=True)
        
        # Discord ì•Œë¦¼ ì „ì†¡
        if synced_count > 0:
            try:
                from core.discord_notifier import send_simple_message
                send_simple_message(
                    f"ìºì‹œ {synced_count}ê°œ ë™ê¸°í™” ì™„ë£Œ (ìŠ¤í‚µ: {skipped_count}ê°œ)",
                    "â˜ï¸ ìë™ ë™ê¸°í™”"
                )
            except Exception as e:
                print(f"âš ï¸ [Discord] ì•Œë¦¼ ì‹¤íŒ¨: {e}")
        
    except Exception as e:
        log_crawl_event("Sync", f"Sync failed: {str(e)}", 0, success=False)
        print(f"âŒ Scheduled sync error: {e}")


def create_scheduler() -> BlockingScheduler:
    """ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„± ë° ì‘ì—… ë“±ë¡"""
    scheduler = BlockingScheduler(timezone='Asia/Seoul')
    
    schedules = load_schedules()
    
    if not schedules:
        # ê¸°ë³¸ ìŠ¤ì¼€ì¤„: ë§¤ 6ì‹œê°„ë§ˆë‹¤
        print("ğŸ“‹ No schedules found. Using default: every 6 hours")
        scheduler.add_job(
            run_scheduled_crawl,
            CronTrigger(hour='*/6', minute=0),
            id='default_crawl',
            name='Default 6-hour Crawl'
        )
    else:
        for sched in schedules:
            if not sched.get('enabled', True):
                continue
            
            cron = sched.get('cron', '0 8 * * *')  # ê¸°ë³¸: ë§¤ì¼ 8ì‹œ
            parts = cron.split()
            
            # [V2] phases íŒŒì‹± (schedules.jsonì—ì„œ ì§€ì • ê°€ëŠ¥)
            phases_config = sched.get('phases', ['collect', 'extract'])
            phases = []
            for phase_name in phases_config:
                try:
                    phases.append(PipelinePhase(phase_name.lower()))
                except ValueError:
                    print(f"âš ï¸ Unknown phase: {phase_name}")
            
            if not phases:
                phases = PHASES_COLLECT_ONLY
            
            try:
                trigger = CronTrigger(
                    minute=parts[0] if len(parts) > 0 else '0',
                    hour=parts[1] if len(parts) > 1 else '*',
                    day=parts[2] if len(parts) > 2 else '*',
                    month=parts[3] if len(parts) > 3 else '*',
                    day_of_week=parts[4] if len(parts) > 4 else '*'
                )
                
                # [V2] phases ì¸ì ì¶”ê°€
                scheduler.add_job(
                    run_scheduled_crawl,
                    trigger,
                    args=[sched.get('name', 'Unnamed'), phases],
                    id=sched.get('id', f"job_{sched.get('name', 'unknown')}"),
                    name=sched.get('name', 'Unnamed')
                )
                print(f"âœ… Registered: {sched.get('name')} ({cron}) - phases: {[p.value for p in phases]}")
            except Exception as e:
                print(f"âš ï¸ Failed to register schedule '{sched.get('name')}': {e}")
    
    # ìë™ ë™ê¸°í™” ìŠ¤ì¼€ì¤„ ì¶”ê°€ (ë§¤ì¼ 23ì‹œ)
    scheduler.add_job(
        run_scheduled_sync,
        CronTrigger(hour=23, minute=0),
        id='auto_sync',
        name='ìë™ ë™ê¸°í™” (23ì‹œ)'
    )
    print("âœ… Registered: ìë™ ë™ê¸°í™” (23:00)")
    
    return scheduler


def signal_handler(signum, frame):
    """ì¢…ë£Œ ì‹œê·¸ë„ ì²˜ë¦¬"""
    print("\nğŸ›‘ Shutdown signal received. Stopping scheduler...")
    sys.exit(0)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘       ğŸ• ZND Crawler Scheduler                   â•‘
â•‘       Independent Background Service             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Config Dir: {CONFIG_DIR}
    """)
    
    # ì¢…ë£Œ ì‹œê·¸ë„ ë“±ë¡
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„± ë° ì‹œì‘
    scheduler = create_scheduler()
    
    print("\nğŸ“… Registered Jobs:")
    for job in scheduler.get_jobs():
        print(f"   - {job.name}: {job.trigger}")
    
    print("\nğŸš€ Scheduler is running. Press Ctrl+C to stop.\n")
    
    try:
        scheduler.start()
    except (KeyboardInterrupt, SystemExit):
        print("\nğŸ‘‹ Scheduler stopped.")


if __name__ == '__main__':
    main()
