#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
sync.py - ìºì‹œ ë™ê¸°í™” CLI ë„êµ¬

Usage:
    python sync.py push [--date YYYY-MM-DD] [--all]  # ë¡œì»¬ ìºì‹œ + íˆìŠ¤í† ë¦¬ â†’ Firestore
    python sync.py pull [--date YYYY-MM-DD] [--all]  # Firestore â†’ ë¡œì»¬ ìºì‹œ + íˆìŠ¤í† ë¦¬
    python sync.py status                             # ë™ê¸°í™” ìƒíƒœ í™•ì¸
    
ë™ê¸°í™” ëŒ€ìƒ:
    1. desk/cache/{ë‚ ì§œ}/*.json - ëª¨ë“  ìºì‹œ íŒŒì¼ (ë™ê¸°í™” ì•ˆëœ ê²ƒë§Œ)
    2. desk/data/crawling_history.json - í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ (ì¤‘ë³µ ë°©ì§€ìš©)
"""

import os
import sys
import json
import argparse
from datetime import datetime, timezone, timedelta

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)  # desk/
sys.path.insert(0, PROJECT_ROOT)

from src.db_client import DBClient

# ìºì‹œ ë° ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
CACHE_DIR = os.path.join(PROJECT_ROOT, 'cache')
DATA_DIR = os.path.join(PROJECT_ROOT, 'data')
HISTORY_FILE = os.path.join(DATA_DIR, 'crawling_history.json')


def get_local_cache_files(date_str: str, synced_ids: set = None) -> list:
    """
    ë¡œì»¬ ìºì‹œ í´ë”ì—ì„œ ì§€ì • ë‚ ì§œì˜ ìºì‹œ íŒŒì¼ë“¤ì„ ì½ìŒ
    
    Args:
        date_str: 'YYYY-MM-DD' í˜•ì‹
        synced_ids: ì´ë¯¸ ë™ê¸°í™”ëœ article_id set (ìŠ¤í‚µìš©)
    
    Returns:
        ìºì‹œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ë™ê¸°í™” ì•ˆëœ ê²ƒë§Œ)
    """
    cache_date_dir = os.path.join(CACHE_DIR, date_str)
    cache_list = []
    skipped = 0
    
    if not os.path.exists(cache_date_dir):
        print(f"âš ï¸ ë¡œì»¬ ìºì‹œ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {cache_date_dir}")
        return cache_list
    
    synced_ids = synced_ids or set()
    
    for filename in os.listdir(cache_date_dir):
        if not filename.endswith('.json'):
            continue
        
        filepath = os.path.join(cache_date_dir, filename)
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                # article_idê°€ ì—†ìœ¼ë©´ íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ
                article_id = data.get('article_id')
                if not article_id:
                    article_id = filename.replace('.json', '')
                    data['article_id'] = article_id
                
                # ì´ë¯¸ ë™ê¸°í™”ëœ í•­ëª© ìŠ¤í‚µ
                if article_id in synced_ids:
                    skipped += 1
                    continue
                
                # synced_at í•„ë“œê°€ ìˆìœ¼ë©´ ìŠ¤í‚µ (ë¡œì»¬ì—ì„œ í‘œì‹œ)
                if data.get('synced_at'):
                    skipped += 1
                    continue
                
                cache_list.append(data)
                
        except Exception as e:
            print(f"âš ï¸ ì½ê¸° ì‹¤íŒ¨: {filename} - {e}")
    
    if skipped > 0:
        print(f"   â­ï¸ ì´ë¯¸ ë™ê¸°í™”ë¨: {skipped}ê°œ ìŠ¤í‚µ")
    
    return cache_list


def mark_cache_as_synced(date_str: str, article_ids: list):
    """
    ë¡œì»¬ ìºì‹œ íŒŒì¼ì— synced_at í•„ë“œ ì¶”ê°€
    """
    cache_date_dir = os.path.join(CACHE_DIR, date_str)
    if not os.path.exists(cache_date_dir):
        return
    
    synced_at = datetime.now(timezone.utc).isoformat()
    
    for filename in os.listdir(cache_date_dir):
        if not filename.endswith('.json'):
            continue
        
        filepath = os.path.join(cache_date_dir, filename)
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            article_id = data.get('article_id', filename.replace('.json', ''))
            if article_id in article_ids:
                data['synced_at'] = synced_at
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
        except:
            pass


def save_cache_to_local(cache_list: list, date_str: str) -> int:
    """
    ìºì‹œ ë°ì´í„°ë¥¼ ë¡œì»¬ í´ë”ì— ì €ì¥
    """
    cache_date_dir = os.path.join(CACHE_DIR, date_str)
    os.makedirs(cache_date_dir, exist_ok=True)
    
    saved_count = 0
    for cache_data in cache_list:
        article_id = cache_data.get('article_id')
        if not article_id:
            continue
        
        filepath = os.path.join(cache_date_dir, f"{article_id}.json")
        
        # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë³‘í•©
        if os.path.exists(filepath):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    existing = json.load(f)
                    # Firestore ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸ (ë¡œì»¬ ìš°ì„  ì•„ë‹˜)
                    existing.update(cache_data)
                    cache_data = existing
            except:
                pass
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            saved_count += 1
        except Exception as e:
            print(f"âš ï¸ ì €ì¥ ì‹¤íŒ¨: {article_id} - {e}")
    
    return saved_count


def get_local_cache_dates() -> list:
    """ë¡œì»¬ ìºì‹œ í´ë”ì˜ ë‚ ì§œ ëª©ë¡ ì¡°íšŒ"""
    if not os.path.exists(CACHE_DIR):
        return []
    
    dates = []
    for name in os.listdir(CACHE_DIR):
        date_path = os.path.join(CACHE_DIR, name)
        if os.path.isdir(date_path) and len(name) == 10:  # YYYY-MM-DD í˜•ì‹
            json_files = [f for f in os.listdir(date_path) if f.endswith('.json')]
            if json_files:
                dates.append(name)
    
    dates.sort(reverse=True)
    return dates


def load_local_history() -> dict:
    """ë¡œì»¬ í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ ë¡œë“œ"""
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass
    return {}


def save_local_history(history: dict):
    """ë¡œì»¬ í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ ì €ì¥"""
    os.makedirs(DATA_DIR, exist_ok=True)
    with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)


def cmd_push(args):
    """push ëª…ë ¹: ë¡œì»¬ ìºì‹œ + íˆìŠ¤í† ë¦¬ â†’ Firestore"""
    print(f"\nğŸš€ Push: ë¡œì»¬ â†’ Firestore")
    print("=" * 50)
    
    db = DBClient()
    if not db.db:
        print("âŒ Firestore ì—°ê²° ì‹¤íŒ¨. serviceAccountKey.jsonì„ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    # ë‚ ì§œ ê²°ì •
    if args.all:
        dates = get_local_cache_dates()
        print(f"ğŸ“… ì „ì²´ ë‚ ì§œ ë™ê¸°í™”: {len(dates)}ê°œ ë‚ ì§œ")
    else:
        date_str = args.date or datetime.now().strftime('%Y-%m-%d')
        dates = [date_str]
        print(f"ğŸ“… ëŒ€ìƒ ë‚ ì§œ: {date_str}")
    
    total_success = 0
    total_failed = 0
    
    # ë‚ ì§œë³„ ìºì‹œ ì—…ë¡œë“œ (ë¡œì»¬ synced_at í•„ë“œë¡œë§Œ íŒë‹¨ â†’ Firestore ì¡°íšŒ ë¹„ìš© 0)
    for date_str in dates:
        print(f"\nğŸ“¦ [{date_str}] ìºì‹œ ì²˜ë¦¬ ì¤‘...")
        
        # ë¡œì»¬ synced_at í•„ë“œë§Œ í™•ì¸ (Firestore ì¡°íšŒ ì•ˆí•¨ = ë¹„ìš© ì ˆê°)
        cache_list = get_local_cache_files(date_str)
        
        if not cache_list:
            print(f"   âœ… ìƒˆë¡œ ë™ê¸°í™”í•  ìºì‹œ ì—†ìŒ")
            continue
        
        print(f"   ğŸ“¤ ì—…ë¡œë“œ ëŒ€ìƒ: {len(cache_list)}ê°œ")
        
        result = db.upload_cache_batch(date_str, cache_list)
        total_success += result['success']
        total_failed += result['failed']
        
        # ë¡œì»¬ íŒŒì¼ì— synced_at ë§ˆí‚¹ (ë‹¤ìŒ push ì‹œ ìŠ¤í‚µë¨)
        uploaded_ids = [c.get('article_id') for c in cache_list if c.get('article_id')]
        mark_cache_as_synced(date_str, uploaded_ids)
    
    # 4. í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ ë™ê¸°í™”
    print(f"\nğŸ“œ í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ ë™ê¸°í™” ì¤‘...")
    local_history = load_local_history()
    
    if local_history:
        history_result = db.upload_crawling_history(local_history)
        print(f"   âœ… íˆìŠ¤í† ë¦¬ {history_result.get('count', 0)}ê°œ ì—…ë¡œë“œ")
    else:
        print("   âš ï¸ ë¡œì»¬ íˆìŠ¤í† ë¦¬ê°€ ë¹„ì–´ìˆìŒ")
    
    # 5. ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 50)
    print("ğŸ“Š Push ì™„ë£Œ:")
    print(f"   âœ… ìºì‹œ ì„±ê³µ: {total_success}ê°œ")
    print(f"   âŒ ìºì‹œ ì‹¤íŒ¨: {total_failed}ê°œ")


def cmd_pull(args):
    """pull ëª…ë ¹: Firestore â†’ ë¡œì»¬ ìºì‹œ + íˆìŠ¤í† ë¦¬"""
    print(f"\nâ¬‡ï¸ Pull: Firestore â†’ ë¡œì»¬")
    print("=" * 50)
    
    db = DBClient()
    if not db.db:
        print("âŒ Firestore ì—°ê²° ì‹¤íŒ¨. serviceAccountKey.jsonì„ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    # 1. ë‚ ì§œ ê²°ì •
    if args.all:
        dates = db.get_cache_sync_dates()
        print(f"ğŸ“… Firestore ë‚ ì§œ: {len(dates)}ê°œ")
    else:
        date_str = args.date or datetime.now().strftime('%Y-%m-%d')
        dates = [date_str]
        print(f"ğŸ“… ëŒ€ìƒ ë‚ ì§œ: {date_str}")
    
    total_saved = 0
    
    # 2. ë‚ ì§œë³„ ìºì‹œ ë‹¤ìš´ë¡œë“œ
    for date_str in dates:
        print(f"\nğŸ“¦ [{date_str}] ìºì‹œ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        
        cache_list = db.download_cache_batch(date_str)
        
        if not cache_list:
            print(f"   âš ï¸ Firestoreì— ìºì‹œ ì—†ìŒ")
            continue
        
        print(f"   â˜ï¸ Firestore: {len(cache_list)}ê°œ")
        
        saved_count = save_cache_to_local(cache_list, date_str)
        total_saved += saved_count
        print(f"   ğŸ’¾ ì €ì¥: {saved_count}ê°œ")
    
    # 3. í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ ë™ê¸°í™”
    print(f"\nğŸ“œ í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ ë‹¤ìš´ë¡œë“œ ì¤‘...")
    remote_history = db.download_crawling_history()
    
    if remote_history:
        local_history = load_local_history()
        # ì›ê²© íˆìŠ¤í† ë¦¬ë¥¼ ë¡œì»¬ì— ë³‘í•© (ì›ê²© ìš°ì„ )
        local_history.update(remote_history)
        save_local_history(local_history)
        print(f"   âœ… íˆìŠ¤í† ë¦¬ {len(remote_history)}ê°œ ë³‘í•© (ë¡œì»¬ ì´ {len(local_history)}ê°œ)")
    else:
        print("   âš ï¸ Firestore íˆìŠ¤í† ë¦¬ê°€ ë¹„ì–´ìˆìŒ")
    
    # 4. ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 50)
    print("ğŸ“Š Pull ì™„ë£Œ:")
    print(f"   âœ… ìºì‹œ ì €ì¥: {total_saved}ê°œ")
    print(f"   ğŸ“ ê²½ë¡œ: {CACHE_DIR}")


def cmd_status(args):
    """status ëª…ë ¹: ë™ê¸°í™” ìƒíƒœ í™•ì¸"""
    print("\nğŸ“Š ìºì‹œ ë™ê¸°í™” ìƒíƒœ")
    print("=" * 50)
    
    # 1. ë¡œì»¬ ìºì‹œ ìƒíƒœ
    local_dates = get_local_cache_dates()
    print(f"\nğŸ“ ë¡œì»¬ ìºì‹œ:")
    
    local_total = 0
    local_unsynced = 0
    
    if local_dates:
        for date_str in local_dates[:5]:
            cache_date_dir = os.path.join(CACHE_DIR, date_str)
            total = 0
            unsynced = 0
            
            for filename in os.listdir(cache_date_dir):
                if not filename.endswith('.json'):
                    continue
                total += 1
                
                filepath = os.path.join(cache_date_dir, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if not data.get('synced_at'):
                            unsynced += 1
                except:
                    pass
            
            local_total += total
            local_unsynced += unsynced
            
            sync_status = "âœ…" if unsynced == 0 else f"âš ï¸ {unsynced}ê°œ ë¯¸ë™ê¸°í™”"
            print(f"   {date_str}: {total}ê°œ ({sync_status})")
            
        if len(local_dates) > 5:
            print(f"   ... ì™¸ {len(local_dates) - 5}ì¼")
    else:
        print("   (ì—†ìŒ)")
    
    # 2. ë¡œì»¬ íˆìŠ¤í† ë¦¬ ìƒíƒœ
    local_history = load_local_history()
    print(f"\nğŸ“œ ë¡œì»¬ íˆìŠ¤í† ë¦¬: {len(local_history)}ê°œ URL")
    
    # 3. Firestore ìƒíƒœ
    db = DBClient()
    if not db.db:
        print("\nâ˜ï¸ Firestore: âš ï¸ ì—°ê²° ì•ˆë¨")
        return
    
    firestore_dates = db.get_cache_sync_dates()
    print(f"\nâ˜ï¸ Firestore ìºì‹œ:")
    if firestore_dates:
        for date_str in firestore_dates[:5]:
            print(f"   {date_str}")
        if len(firestore_dates) > 5:
            print(f"   ... ì™¸ {len(firestore_dates) - 5}ì¼")
    else:
        print("   (ì—†ìŒ)")
    
    # 4. ë©”íƒ€ë°ì´í„°
    meta = db.get_sync_metadata()
    if meta:
        print(f"\nğŸ• ë§ˆì§€ë§‰ ë™ê¸°í™”:")
        if 'last_push' in meta:
            print(f"   Push: {meta.get('last_push_date', '?')} ({meta.get('last_push_count', '?')}ê°œ)")
        if 'last_pull' in meta:
            print(f"   Pull: {meta.get('last_pull_date', '?')} ({meta.get('last_pull_count', '?')}ê°œ)")
    
    # 5. ìš”ì•½
    print(f"\nğŸ” ìš”ì•½:")
    print(f"   ğŸ“¦ ë¡œì»¬ ìºì‹œ: {local_total}ê°œ (ë¯¸ë™ê¸°í™” {local_unsynced}ê°œ)")
    print(f"   ğŸ“œ ë¡œì»¬ íˆìŠ¤í† ë¦¬: {len(local_history)}ê°œ")
    
    if local_unsynced > 0:
        print(f"\nğŸ’¡ Tip: 'python sync.py push --all' ë¡œ ì „ì²´ ë™ê¸°í™”")


def main():
    parser = argparse.ArgumentParser(
        description='ZND ìºì‹œ ë™ê¸°í™” ë„êµ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ë™ê¸°í™” ëŒ€ìƒ:
  1. desk/cache/{ë‚ ì§œ}/*.json - ëª¨ë“  ìºì‹œ íŒŒì¼
  2. desk/data/crawling_history.json - í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬

ì˜ˆì‹œ:
  python sync.py push                    # ì˜¤ëŠ˜ ìºì‹œ + íˆìŠ¤í† ë¦¬ ì—…ë¡œë“œ
  python sync.py push --date 2025-12-24  # íŠ¹ì • ë‚ ì§œ ì—…ë¡œë“œ
  python sync.py push --all              # ì „ì²´ ë‚ ì§œ ì—…ë¡œë“œ
  python sync.py pull                    # ì˜¤ëŠ˜ ìºì‹œ ë‹¤ìš´ë¡œë“œ
  python sync.py pull --all              # ì „ì²´ ë‚ ì§œ ë‹¤ìš´ë¡œë“œ
  python sync.py status                  # ë™ê¸°í™” ìƒíƒœ í™•ì¸
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='ëª…ë ¹ì–´')
    
    # push ëª…ë ¹
    push_parser = subparsers.add_parser('push', help='ë¡œì»¬ ìºì‹œ + íˆìŠ¤í† ë¦¬ â†’ Firestore')
    push_parser.add_argument('--date', '-d', help='ë‚ ì§œ (YYYY-MM-DD, ê¸°ë³¸: ì˜¤ëŠ˜)')
    push_parser.add_argument('--all', '-a', action='store_true', help='ì „ì²´ ë‚ ì§œ ë™ê¸°í™”')
    push_parser.set_defaults(func=cmd_push)
    
    # pull ëª…ë ¹
    pull_parser = subparsers.add_parser('pull', help='Firestore â†’ ë¡œì»¬ ìºì‹œ + íˆìŠ¤í† ë¦¬')
    pull_parser.add_argument('--date', '-d', help='ë‚ ì§œ (YYYY-MM-DD, ê¸°ë³¸: ì˜¤ëŠ˜)')
    pull_parser.add_argument('--all', '-a', action='store_true', help='ì „ì²´ ë‚ ì§œ ë™ê¸°í™”')
    pull_parser.set_defaults(func=cmd_pull)
    
    # status ëª…ë ¹
    status_parser = subparsers.add_parser('status', help='ë™ê¸°í™” ìƒíƒœ í™•ì¸')
    status_parser.set_defaults(func=cmd_status)
    
    args = parser.parse_args()
    
    if args.command is None:
        parser.print_help()
        return
    
    args.func(args)


if __name__ == '__main__':
    main()
