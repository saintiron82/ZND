# src/crawler_state.py
# í¬ë¡¤ë§ ìƒíƒœ ê´€ë¦¬ - ë…ë¦½ í¬ë¡¤ëŸ¬ ëª¨ë“ˆê³¼ í†µí•©
import sys
import os

# Add crawler module to path
CRAWLER_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'crawler')
if CRAWLER_DIR not in sys.path:
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

# ìƒíƒœ ë³€ìˆ˜ (ë©”ëª¨ë¦¬ ë‚´)
_is_crawling = False
_current_task = ""
_progress = {
    "current_target": "",
    "current_index": 0,
    "total_targets": 0,
    "collected_count": 0,
    "message": ""
}


def set_crawling(status: bool, task: str = ""):
    """í¬ë¡¤ë§ ìƒíƒœ ì„¤ì •"""
    global _is_crawling, _current_task, _progress
    _is_crawling = status
    _current_task = task if status else ""
    if not status:
        # ì™„ë£Œ ì‹œ ì§„í–‰ ìƒí™© ì´ˆê¸°í™”
        _progress = {"current_target": "", "current_index": 0, "total_targets": 0, "collected_count": 0, "message": ""}


def update_progress(target: str = "", index: int = 0, total: int = 0, count: int = 0, message: str = ""):
    """ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸"""
    global _progress
    if target:
        _progress["current_target"] = target
    if index > 0:
        _progress["current_index"] = index
    if total > 0:
        _progress["total_targets"] = total
    if count > 0:
        _progress["collected_count"] += count
    if message:
        _progress["message"] = message


def get_crawling_status():
    """í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ (ì§„í–‰ ìƒí™© í¬í•¨)"""
    global _is_crawling, _current_task, _progress
    return {
        "is_crawling": _is_crawling,
        "current_task": _current_task,
        "progress": _progress
    }


# ë¡œê¹… í•¨ìˆ˜ - crawler/logs/crawler_history.jsonlì— ê¸°ë¡
def log_crawl_event(action: str, result: str, duration: float, success: bool = True):
    """ì‹¤í–‰ ì´ë ¥ ê¸°ë¡ (crawler/logs/crawler_history.jsonl)"""
    import json
    from datetime import datetime
    
    # ë…ë¦½ ìŠ¤ì¼€ì¤„ëŸ¬ì˜ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    ZND_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
    LOG_FILE = os.path.join(ZND_ROOT, 'crawler', 'logs', 'crawler_history.jsonl')
    
    try:
        os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)
        entry = {
            "timestamp": datetime.now().isoformat(),
            "action": action, 
            "result": result,
            "duration": round(duration, 2),
            "success": success
        }
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')
        print(f"ğŸ“ [Log] {action}: {result} ({duration:.2f}s)")
    except Exception as e:
        print(f"âŒ Log save failed: {e}")


def get_crawl_logs(limit: int = 50, offset: int = 0):
    """ìµœê·¼ ë¡œê·¸ ì¡°íšŒ (crawler/logs/crawler_history.jsonl) - í˜ì´ì§€ë„¤ì´ì…˜ ì§€ì›"""
    import json
    
    # ë…ë¦½ ìŠ¤ì¼€ì¤„ëŸ¬ì˜ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    ZND_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
    LOG_FILE = os.path.join(ZND_ROOT, 'crawler', 'logs', 'crawler_history.jsonl')
    
    if not os.path.exists(LOG_FILE):
        return []
    
    logs = []
    try:
        with open(LOG_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    logs.append(json.loads(line))
    except:
        return []
    
    # ìµœì‹ ìˆœ ì •ë ¬ í›„ offsetë¶€í„° limitë§Œí¼ ë°˜í™˜
    sorted_logs = sorted(logs, key=lambda x: x.get('timestamp', ''), reverse=True)
    return sorted_logs[offset:offset + limit]


