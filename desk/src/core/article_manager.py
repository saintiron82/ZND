# -*- coding: utf-8 -*-
"""
Article Manager - ê¸°ì‚¬ ì¤‘ì•™ ê´€ë¦¬ ì‹œìŠ¤í…œ
ëª¨ë“  ê¸°ì‚¬ CRUD ë° ìƒíƒœ ì „ì´ì˜ ë‹¨ì¼ ì§„ì…ì 
"""
import hashlib
from datetime import datetime, timezone
from typing import Optional, List, Dict, Any

from .article_state import ArticleState, can_transition
from .firestore_client import FirestoreClient


class ArticleManager:
    """
    ê¸°ì‚¬ ì¤‘ì•™ ê´€ë¦¬ ì‹œìŠ¤í…œ
    
    ëª¨ë“  ê¸°ì‚¬ ë°ì´í„°ëŠ” 5ê°œ ì„¹ì…˜ìœ¼ë¡œ êµ¬ì„±:
    - _header: ë©”íƒ€ ì •ë³´ (ìƒíƒœ, ë²„ì „, íˆìŠ¤í† ë¦¬)
    - _original: ì›ë³¸ ì •ë³´ (í¬ë¡¤ëŸ¬ê°€ ì‘ì„±, ë¶ˆë³€)
    - _analysis: ë¶„ì„ ì •ë³´ (AI Analyzerê°€ ì‘ì„±)
    - _classification: ë¶„ë¥˜ ì •ë³´ (Desk UIì—ì„œ ì‘ì„±)
    - _publication: ë°œí–‰ ì •ë³´ (Publisherê°€ ì‘ì„±)
    """
    
    SCHEMA_VERSION = "2.0"
    
    def __init__(self):
        self.db = FirestoreClient()
    
    # =========================================================================
    # Article ID Generation
    # =========================================================================
    
    @staticmethod
    def generate_article_id(url: str) -> str:
        """URLì—ì„œ article_id ìƒì„± (12ìë¦¬ MD5 í•´ì‹œ)"""
        return hashlib.md5(url.encode()).hexdigest()[:12]
    
    # =========================================================================
    # CRUD Operations
    # =========================================================================
    
    def get(self, article_id: str) -> Optional[Dict[str, Any]]:
        """ê¸°ì‚¬ ì¡°íšŒ"""
        """ê¸°ì‚¬ ì¡°íšŒ"""
        article = self.db.get_article(article_id)
        if article:
            return self._flatten_article(article)
        return None
    
    def get_by_url(self, url: str) -> Optional[Dict[str, Any]]:
        """URLë¡œ ê¸°ì‚¬ ì¡°íšŒ"""
        article_id = self.generate_article_id(url)
        return self.get(article_id)
    
    def create(self, url: str, original_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ìƒˆ ê¸°ì‚¬ ìƒì„± (Collectorìš©)
        
        Args:
            url: ê¸°ì‚¬ URL
            original_data: ì›ë³¸ ë°ì´í„° (title, text, image, source_id ë“±)
        
        Returns:
            ìƒì„±ëœ ê¸°ì‚¬ ë°ì´í„°
        """
        article_id = self.generate_article_id(url)
        now = datetime.now(timezone.utc).isoformat()
        
        article = {
            '_header': {
                'version': self.SCHEMA_VERSION,
                'article_id': article_id,
                'state': ArticleState.COLLECTED.value,
                'created_at': now,
                'updated_at': now,
                'state_history': [
                    {
                        'state': ArticleState.COLLECTED.value,
                        'at': now,
                        'by': 'collector'
                    }
                ]
            },
            '_original': {
                'url': url,
                'title': original_data.get('title', ''),
                'text': original_data.get('text', ''),
                'image': original_data.get('image'),
                'source_id': original_data.get('source_id', 'unknown'),
                'crawled_at': now
            },
            '_analysis': None,
            '_classification': None,
            '_publication': None
        }
        
        # Firestoreì— ì €ì¥
        self.db.save_article(article_id, article)
        
        # íˆìŠ¤í† ë¦¬ì— URL ë“±ë¡
        self.db.update_history(url, article_id, ArticleState.COLLECTED.value)
        
        return article
    
    def update_state(
        self, 
        article_id: str, 
        new_state: ArticleState, 
        by: str = 'system',
        section_data: Optional[Dict[str, Any]] = None
    ) -> bool:
        """
        ê¸°ì‚¬ ìƒíƒœ ë³€ê²½
        
        Args:
            article_id: ê¸°ì‚¬ ID
            new_state: ìƒˆ ìƒíƒœ
            by: ë³€ê²½ ì£¼ì²´ (collector, analyzer, desk, publisher)
            section_data: í•´ë‹¹ ì„¹ì…˜ì— ì €ì¥í•  ë°ì´í„°
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        article = self.get(article_id)
        if not article:
            return False
        
        current_state = ArticleState(article['_header']['state'])
        
        # ìƒíƒœ ì „ì´ ìœ íš¨ì„± ê²€ì‚¬
        if not can_transition(current_state, new_state):
            print(f"âš ï¸ Invalid state transition: {current_state} â†’ {new_state}")
            return False
        
        now = datetime.now(timezone.utc).isoformat()
        
        # í—¤ë” ì—…ë°ì´íŠ¸
        updates = {
            '_header.state': new_state.value,
            '_header.updated_at': now,
        }
        
        # ìƒíƒœ íˆìŠ¤í† ë¦¬ ì¶”ê°€
        new_history_entry = {
            'state': new_state.value,
            'at': now,
            'by': by
        }
        
        # ì„¹ì…˜ ë°ì´í„° ì—…ë°ì´íŠ¸
        if section_data:
            section_map = {
                ArticleState.ANALYZED: '_analysis',
                ArticleState.CLASSIFIED: '_classification',
                ArticleState.REJECTED: '_rejection',  # íê¸° ì •ë³´ëŠ” ë³„ë„ ì„¹ì…˜
                ArticleState.PUBLISHED: '_publication',
                ArticleState.RELEASED: '_publication',
            }
            if new_state in section_map:
                section_name = section_map[new_state]
                for key, value in section_data.items():
                    updates[f'{section_name}.{key}'] = value
        
        # Firestore ì—…ë°ì´íŠ¸ (state_historyëŠ” ë°°ì—´ì´ë¼ ë³„ë„ ì²˜ë¦¬ í•„ìš”)
        # [Log] Start
        try:
             with open('debug_manager.log', 'a', encoding='utf-8') as f:
                 f.write(f"{datetime.now(timezone.utc)}: [UpdateState] {article_id} -> {new_state.value}\n")
        except: pass

        # Firestore ì—…ë°ì´íŠ¸ (state_historyëŠ” ë°°ì—´ì´ë¼ ë³„ë„ ì²˜ë¦¬ í•„ìš”)
        # [Fix] Update -> Upsert (ë¬¸ì„œ ì—†ìœ¼ë©´ ìƒì„±)
        # ê¸°ì¡´: self.db.update_article(article_id, updates)
        success, msg = self.db.upsert_article_state(article_id, updates)
        
        # [Log] Result
        try:
             with open('debug_manager.log', 'a', encoding='utf-8') as f:
                 f.write(f"  Firestore Upsert: {success} ({msg})\n")
        except: pass
        
        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        url = article['_original']['url']
        self.db.update_history(url, article_id, new_state.value)
        
        # Registry ì¸ë©”ëª¨ë¦¬ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ (SSOT ë™ê¸°í™”)
        try:
            from .article_registry import get_registry
            registry = get_registry()
            
            info = registry.get(article_id)
            if not info:
                # [Lazy Load] Registryì— ì—†ìœ¼ë©´ ë””ìŠ¤í¬ì—ì„œ ë¡œë“œ
                info = registry.find_and_register(article_id)
            
            if info:
                registry._update_article_state(info, new_state.value)
                
                # ë¡œì»¬ ìºì‹œ íŒŒì¼ë„ ìƒíƒœ ë™ê¸°í™”
                current_ts = datetime.now(timezone.utc).isoformat()
                registry._update_local_cache(article_id, new_state.value, by, current_ts)
                
                print(f"âœ… [Registry] State synced: {article_id} â†’ {new_state.value}")
            else:
                print(f"âš ï¸ [Registry] Article not found even after lazy load: {article_id}")
                
        except Exception as e:
            print(f"âš ï¸ [Registry] Sync failed: {e}")
            import traceback
            traceback.print_exc()

        # [Note] Direct DB update removed to maintain centralized management via Registry.

        
        return True
    
    def _flatten_article(self, article: Dict[str, Any]) -> Dict[str, Any]:
        """
        ê³„ì¸µí˜• ë°ì´í„°ë¥¼ UI/ë ˆê±°ì‹œ í˜¸í™˜ì„ ìœ„í•´ í‰íƒ„í™” (Flatten)
        Priorities: _publication > _classification > _analysis > _original > _header
        """
        if not article:
            return article
            
        # Start with a shallow copy to preserve original structure (backward compatibility)
        flat = article.copy()
        
        # Merge sections into the top level for frontend convenience
        # 1. Header (Meta)
        if '_header' in article and article['_header']:
            flat.update(article['_header'])
            
        # 2. Original
        if '_original' in article and article['_original']:
            flat.update(article['_original'])
            
        # 3. Analysis
        if '_analysis' in article and article['_analysis']:
            flat.update(article['_analysis'])
            
        # 4. Classification
        if '_classification' in article and article['_classification']:
            flat.update(article['_classification'])
            
        # 5. Publication
        if '_publication' in article and article['_publication']:
            flat.update(article['_publication'])
            
        # _raw is now strictly redundancy since we started with copy, 
        # but kept if explicitly needed by name.
        flat['_raw'] = article
        
        return flat
    
    def update_analysis(self, article_id: str, analysis_data: Dict[str, Any]) -> bool:
        """
        AI ë¶„ì„ ê²°ê³¼ ì €ì¥ (Analyzerìš©)
        
        Args:
            article_id: ê¸°ì‚¬ ID
            analysis_data: ë¶„ì„ ê²°ê³¼ (title_ko, summary, tags, scores, mll_raw)
        """
        now = datetime.now(timezone.utc).isoformat()
        
        section_data = {
            'title_ko': analysis_data.get('title_ko', ''),
            'summary': analysis_data.get('summary', ''),
            'tags': analysis_data.get('tags', []),
            'impact_score': analysis_data.get('impact_score', 0),
            'zero_echo_score': analysis_data.get('zero_echo_score', 0),
            'analyzed_at': now,
            'mll_raw': analysis_data.get('mll_raw')
        }
        
        return self.update_state(
            article_id, 
            ArticleState.ANALYZED, 
            by='analyzer',
            section_data=section_data
        )
    
    def update_classification(self, article_id: str, category: str, is_selected: bool = True) -> bool:
        """
        ë¶„ë¥˜ ì •ë³´ ì €ì¥ (Desk UIìš©)
        
        Args:
            article_id: ê¸°ì‚¬ ID
            category: ì¹´í…Œê³ ë¦¬
            is_selected: ì„ íƒ ì—¬ë¶€ (ì¤‘ë³µ ì œê±°ìš©)
        """
        now = datetime.now(timezone.utc).isoformat()
        
        section_data = {
            'category': category,
            'is_selected': is_selected,
            'classified_at': now,
            'classified_by': 'desk_user'
        }
        
        return self.update_state(
            article_id,
            ArticleState.CLASSIFIED,
            by='desk',
            section_data=section_data
        )
    
    def publish(self, article_id: str, edition_code: str, edition_name: str) -> bool:
        """
        ë°œí–‰ ì²˜ë¦¬ (Publisherìš©)
        
        Args:
            article_id: ê¸°ì‚¬ ID
            edition_code: íšŒì°¨ ì½”ë“œ (ì˜ˆ: 251226_5)
            edition_name: íšŒì°¨ ì´ë¦„ (ì˜ˆ: 5í˜¸)
        """
        now = datetime.now(timezone.utc).isoformat()
        
        section_data = {
            'edition_code': edition_code,
            'edition_name': edition_name,
            'published_at': now,
            'released_at': None,
            'status': 'preview',  # P1 Fix: publication status
            'firestore_synced': True
        }
        
        success = self.update_state(
            article_id,
            ArticleState.PUBLISHED,
            by='publisher',
            section_data=section_data
        )

        if success:
            # 1. Fetch full article data for snapshot
            full_article = self.get(article_id)
            if full_article:
                formatted_article = _format_article_for_snapshot(full_article)
            else:
                # Fallback if somehow missing
                 formatted_article = {'id': article_id, 'title': 'Unknown'}

            # 2. Update Publications Collection (Document)
            # Load existing to append
            pub_doc = self.db.get_publication(edition_code)
            
            if not pub_doc:
                # Initialize new publication document
                pub_doc = {
                    'edition_code': edition_code,
                    'edition_name': edition_name,
                    'published_at': now,
                    'updated_at': now,
                    'status': 'preview',
                    'schema_version': '2.0.0',
                    'article_count': 0,
                    'article_ids': [],
                    'articles': [],
                    'date': now[:10]
                }
            
            # Append new article if not exists
            if article_id not in pub_doc.get('article_ids', []):
                pub_doc['article_ids'] = pub_doc.get('article_ids', []) + [article_id]
                pub_doc['articles'] = pub_doc.get('articles', []) + [formatted_article]
                pub_doc['article_count'] = len(pub_doc['article_ids'])
                pub_doc['updated_at'] = now
            
            # Save SSOT
            print(f"ğŸ“ [Publish] Saving publication document: {edition_code}")
            try:
                self.db.save_publication(edition_code, pub_doc)
                print(f"âœ… [Publish] Publication document saved successfully")
            except Exception as e:
                print(f"âŒ [Publish] Failed to save publication: {e}")
                import traceback
                traceback.print_exc()
                # Rollback article state on failure
                self.update_state(article_id, ArticleState.CLASSIFIED, by='publish_rollback')
                return False
            
            # 3. Update _meta (Summary)
            print(f"ğŸ“ [Publish] Updating publications meta...")
            try:
                meta = self.db.get_publications_meta() or {'issues': []}
                issues = meta.get('issues', [])
                
                # Check existing
                existing_idx = next((i for i, x in enumerate(issues) if x.get('edition_code') == edition_code), -1)
                
                issue_summary = {
                    'edition_code': edition_code,
                    'edition_name': edition_name,
                    'published_at': pub_doc['published_at'],
                    'updated_at': now,
                    'article_count': pub_doc['article_count'],
                    'status': pub_doc.get('status', 'preview'),
                    'schema_version': '2.0.0',
                    # Legacy fields for compatibility if needed, but user emphasized NEW schema structure
                    'code': edition_code,
                    'name': edition_name,
                    'count': pub_doc['article_count']
                }
                
                if existing_idx >= 0:
                    issues[existing_idx] = issue_summary
                else:
                    issues.insert(0, issue_summary)
                
                meta['issues'] = issues
                meta['latest_updated_at'] = now
                self.db.update_publications_meta(meta)
                print(f"âœ… [Publish] Publications meta updated successfully")
            except Exception as e:
                print(f"âŒ [Publish] Failed to update publications meta: {e}")
                import traceback
                traceback.print_exc()
                # Rollback article state on failure
                self.update_state(article_id, ArticleState.CLASSIFIED, by='publish_rollback')
                return False

            # Cache Warmup
            self._warmup_cache()
        
        return success

    def find_duplicates(self, article_id: str, threshold: float = 0.6) -> List[Dict[str, Any]]:
        """
        ì¤‘ë³µ ì˜ì‹¬ ê¸°ì‚¬ ê²€ìƒ‰ (Title similarity based)
        """
        from difflib import SequenceMatcher
        
        target = self.get_article(article_id)
        if not target:
            return []
            
        target_title = target.get('_analysis', {}).get('title_ko') or target.get('_original', {}).get('title', '')
        if not target_title:
            return []
            
        # Recent articles (e.g., last 1000)
        candidates = self.db.list_recent_articles(limit=1000)
        
        results = []
        for cand in candidates:
            if cand['id'] == article_id:
                continue
                
            cand_title = cand.get('_analysis', {}).get('title_ko') or cand.get('_original', {}).get('title', '')
            if not cand_title:
                continue
                
            ratio = SequenceMatcher(None, target_title, cand_title).ratio()
            if ratio >= threshold:
                results.append({
                    'id': cand['id'],
                    'title': cand_title,
                    'similarity': round(ratio * 100, 1),
                    'state': cand.get('_header', {}).get('state', 'unknown'),
                    'published_at': cand.get('_original', {}).get('published_at')
                })
        
        # Sort by similarity desc
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results
    
    def reject(self, article_id: str, reason: str = 'cutline', by: str = 'system') -> bool:
        """
        ê¸°ì‚¬ íê¸°
        
        Args:
            article_id: ê¸°ì‚¬ ID
            reason: íê¸° ì‚¬ìœ  (cutline: ì»¤íŠ¸ë¼ì¸, duplicate: ì¤‘ë³µ, manual: ìˆ˜ë™)
            by: íê¸° ì£¼ì²´
        """
        now = datetime.now(timezone.utc).isoformat()
        
        return self.update_state(
            article_id,
            ArticleState.REJECTED,
            by=by,
            section_data={
                'reason': reason,
                'rejected_at': now,
                'rejected_by': by
            }
        )
    

    
    # =========================================================================
    # Query Operations
    # =========================================================================
    
    def find_by_state(self, state: ArticleState, limit: int = 100) -> List[Dict[str, Any]]:
        """ìƒíƒœë³„ ê¸°ì‚¬ ëª©ë¡ ì¡°íšŒ"""
        return self.db.list_articles_by_state(state.value, limit)
    
    def find_collected(self, limit: int = 100) -> List[Dict[str, Any]]:
        """ìˆ˜ì§‘ëœ ê¸°ì‚¬ ëª©ë¡ (AI ë¶„ì„ ëŒ€ê¸°)"""
        return self.find_by_state(ArticleState.COLLECTED, limit)
    
    def find_analyzed(self, limit: int = 100) -> List[Dict[str, Any]]:
        """ë¶„ì„ ì™„ë£Œ ê¸°ì‚¬ ëª©ë¡ (ë¶„ë¥˜ ëŒ€ê¸°)"""
        return self.find_by_state(ArticleState.ANALYZED, limit)
    
    def find_classified(self, limit: int = 100) -> List[Dict[str, Any]]:
        """ë¶„ë¥˜ ì™„ë£Œ ê¸°ì‚¬ ëª©ë¡ (ë°œí–‰ ëŒ€ê¸°)"""
        return self.find_by_state(ArticleState.CLASSIFIED, limit)
    
    def find_recent(self, limit: int = 100) -> List[Dict[str, Any]]:
        """ìµœê·¼ ê¸°ì‚¬ ëª©ë¡"""
        return self.db.list_recent_articles(limit)
    
    # =========================================================================
    # URL Check
    # =========================================================================
    
    def is_url_processed(self, url: str) -> bool:
        """URLì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        result = self.db.check_url_exists(url)
        return result is not None
    
    def get_url_status(self, url: str) -> Optional[str]:
        """URLì˜ ì²˜ë¦¬ ìƒíƒœ ì¡°íšŒ"""
        result = self.db.check_url_exists(url)
        if result:
            return result.get('status')
        return None

    # =========================================================================
    # Publication / Edition Operations (In-Memory Cache)
    # =========================================================================
    
    _local_cache = {
        'meta': None,
        'articles': {}  # edition_code -> list[dict]
    }
    
    def _warmup_cache(self):
        """ìµœê·¼ 2íšŒì°¨ ë°ì´í„° ë©”ëª¨ë¦¬ ë¡œë“œ"""
        try:
            # 1. Meta ë¡œë“œ
            meta = self.db.get_publications_meta()
            if not meta:
                print("[ArticleManager] No publications meta found.")
                return
            
            self._local_cache['meta'] = meta
            
            # 2. ìµœê·¼ 2íšŒì°¨ ê¸°ì‚¬ ë¡œë“œ
            issues = meta.get('issues', [])
            # published_at ì—­ìˆœ ì •ë ¬ ë³´ì¥
            issues.sort(key=lambda x: x.get('published_at', ''), reverse=True)
            
            recent_codes = [issue.get('edition_code') or issue.get('code') for issue in issues[:2]]
            recent_codes = [c for c in recent_codes if c]  # None ì œê±°
            
            for code in recent_codes:
                articles = self.db.list_articles_by_edition(code)
                self._local_cache['articles'][code] = articles
                
            print(f"[ArticleManager] Cached {len(recent_codes)} recent editions: {recent_codes}")
            
        except Exception as e:
            print(f"[ArticleManager] Cache warmup failed: {e}")

    def get_editions(self, limit: int = 20) -> List[Dict[str, Any]]:
        """ë°œí–‰ëœ íšŒì°¨ ëª©ë¡ ì¡°íšŒ (_meta ê¸°ë°˜ + Cache, í˜¸í™˜ì„± ë³´ì¥)"""
        # 1. Cache Hit Check - use get_issues_from_meta for proper formatting
        if self._local_cache.get('meta'):
            # Already have meta, use the optimized method
            issues = self.db.get_issues_from_meta()
            return issues[:limit]

        # 2. Fallback to DB (get_issues_from_meta handles everything)
        issues = self.db.get_issues_from_meta()
        if issues:
            # Cache the meta for future use
            self._local_cache['meta'] = self.db.get_publications_meta()
        return issues[:limit]


    def get_edition_articles(self, edition_code: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • íšŒì°¨ì˜ ê¸°ì‚¬ ëª©ë¡ ì¡°íšŒ (Cache ìš°ì„ )"""
        # 1. Cache Hit Check
        if edition_code in self._local_cache.get('articles', {}):
            return self._local_cache['articles'][edition_code]

        # 2. DB Query
        articles = self.db.list_articles_by_edition(edition_code)
        return articles

    def release_edition(self, edition_code: str) -> Dict[str, Any]:
        """
        íšŒì°¨ ì •ì‹ ë°œí–‰ (Preview -> Released)
        1. ë©”íƒ€ ë°ì´í„° ìƒíƒœ ë³€ê²½
        2. ì†Œì† ê¸°ì‚¬ ìƒíƒœ ì¼ê´„ ë³€ê²½ (PUBLISHED -> RELEASED)
        """
        now = datetime.now(timezone.utc).isoformat()
        
        # 1. ë©”íƒ€ ë°ì´í„° í™•ì¸ ë° ì—…ë°ì´íŠ¸
        meta = self.db.get_publications_meta()
        if not meta:
            return {'success': False, 'error': 'Meta not found'}
            
        issues = meta.get('issues', [])
        target_issue = None
        target_idx = -1
        
        for idx, iss in enumerate(issues):
            if iss.get('edition_code') == edition_code or iss.get('code') == edition_code:
                target_issue = iss
                target_idx = idx
                break
                
        if not target_issue:
            return {'success': False, 'error': 'Edition not found'}
            
        if target_issue.get('status') == 'released':
            return {'success': True, 'message': 'Already released', 'released_count': 0}
            
        # Update Meta Status
        target_issue['status'] = 'released'
        target_issue['released_at'] = now
        issues[target_idx] = target_issue
        
        self.db.update_publications_meta(meta)
        
        # Update Individual Publication Doc
        pub_doc = self.db.get_publication(edition_code) or {}
        pub_doc['status'] = 'released'
        pub_doc['released_at'] = now
        self.db.save_publication(edition_code, pub_doc)
        
        # 2. Update Articles
        articles = self.get_edition_articles(edition_code)
        updated_count = 0
        
        for art in articles:
            # article_id, header.state check
            art_id = art.get('article_id') or art.get('id')
            current_state = art.get('_header', {}).get('state')  # P2 Fix: Use only _header.state
            
            if art_id and current_state != 'RELEASED':
                # Update State to RELEASED
                # Note: 'publisher' is the actor
                self.update_state(art_id, ArticleState.RELEASED, by='publisher')
                
                # Update _publication section explicitly if needed?
                # update_state mainly handles header. 
                # Ideally, we should add 'released_at' to _publication section too.
                # But for now, let's trust state change is enough.
                updated_count += 1
                
        # 3. Warmup Cache
        self._warmup_cache()
        
        return {
            'success': True,
            'edition_code': edition_code,
            'released_count': updated_count,
            'released_at': now
        }

    def delete_edition(self, edition_code: str) -> Dict[str, Any]:
        """
        íšŒì°¨ íŒŒê¸° (Unpublish/Rollback)
        1. ë©”íƒ€ ë°ì´í„°ì—ì„œ í•´ë‹¹ íšŒì°¨ ì œê±°
        2. publications ì»¬ë ‰ì…˜ì—ì„œ ë¬¸ì„œ ì‚­ì œ
        3. ì†Œì† ê¸°ì‚¬ë“¤ì˜ ìƒíƒœë¥¼ CLASSIFIEDë¡œ ì›ë³µ (Draft ëª©ë¡ìœ¼ë¡œ ë³µê·€)
        """
        # 1. ë©”íƒ€ ë°ì´í„° í™•ì¸
        meta = self.db.get_publications_meta()
        if not meta:
            return {'success': False, 'error': 'Meta not found'}
            
        issues = meta.get('issues', [])
        
        # í•´ë‹¹ íšŒì°¨ í•„í„°ë§ (ì œê±°)
        initial_len = len(issues)
        issues = [i for i in issues if not (i.get('edition_code') == edition_code or i.get('code') == edition_code)]
        
        if len(issues) == initial_len:
             return {'success': False, 'error': 'Edition not found in meta'}

        # Meta ì—…ë°ì´íŠ¸
        meta['issues'] = issues
        meta['latest_updated_at'] = datetime.now(timezone.utc).isoformat()
        self.db.update_publications_meta(meta)
        
        # 2. ê¸°ì‚¬ ëª©ë¡ í™•ë³´ (ë¬¸ì„œ ì‚­ì œ ì „)
        # ì‚­ì œ ëŒ€ìƒ ê¸°ì‚¬ë“¤ì„ ì°¾ê¸° ìœ„í•´ publication ë¬¸ì„œ ì¡°íšŒ
        # ë§Œì•½ ì´ë¯¸ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ index ê²€ìƒ‰ ì‹œë„
        pub_doc = self.db.get_publication(edition_code)
        target_article_ids = []
        
        if pub_doc:
            target_article_ids = pub_doc.get('article_ids', [])
        
        # í˜¹ì‹œ pub_docì— ì—†ë”ë¼ë„ indexë¡œ ì°¾ì•„ë³¸ë‹¤ (Safety)
        if not target_article_ids:
             query = self.db._get_collection('articles').where('_publication.edition_code', '==', edition_code)
             docs = query.stream()
             target_article_ids = [doc.id for doc in docs]

        # 3. ê°œë³„ íšŒì°¨ ë¬¸ì„œ ì‚­ì œ
        doc_ref = self.db._get_collection('publications').document(edition_code)
        doc_ref.delete()
        
        # 4. ê¸°ì‚¬ ìƒíƒœ ì›ë³µ (Revert Articles)
        reverted_count = 0
        empty_pub_data = {
            'edition_code': None,
            'edition_name': None,
            'published_at': None,
            'released_at': None
        }
            
        for art_id in target_article_ids:
            # ê°•ì œë¡œ CLASSIFIED (ë¶„ë¥˜ë¨/ë°œí–‰ëŒ€ê¸°)ë¡œ ë³€ê²½
            # _publication ì •ë³´ ì´ˆê¸°í™”
            self.update_state(
                art_id, 
                ArticleState.CLASSIFIED, 
                by='publisher',
                section_data=empty_pub_data
            )
            reverted_count += 1
            
        # Cache Warmup
        self._warmup_cache()
        
        return {
            'success': True,
            'edition_code': edition_code,
            'reverted_count': reverted_count
        }


# =============================================================================
# Helper Functions (Module Level)
# =============================================================================

def _format_article_for_snapshot(article: dict) -> dict:
    """ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë°œí–‰ ìŠ¤ëƒ…ìƒ·ìš©ìœ¼ë¡œ ë³€í™˜ (User Schema ì¤€ìˆ˜)"""
    header = article.get('_header', {})
    original = article.get('_original', {})
    analysis = article.get('_analysis', {})
    classification = article.get('_classification', {})
    
    return {
        'id': header.get('article_id'),
        'source_id': original.get('source_id'),
        'title': original.get('title'),
        'title_ko': analysis.get('title_ko'),
        'title_en': analysis.get('title_en', ''),
        'url': original.get('url'),
        'published_at': original.get('published_at') or header.get('created_at'),
        'category': classification.get('category'),
        'impact_score': analysis.get('impact_score'),
        'zero_echo_score': analysis.get('zero_echo_score'),
        'summary': analysis.get('summary'),
        'tags': analysis.get('tags', []),
        'layout_type': classification.get('layout_type', 'Standard'),
        'date': (original.get('published_at') or '')[:10],
        'filename': f"{original.get('source_id')}_{header.get('article_id')}.json"
    }
