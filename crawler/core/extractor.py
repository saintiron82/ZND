# -*- coding: utf-8 -*-
"""
Crawler Extractor - ì½˜í…ì¸  ì¶”ì¶œ ëª¨ë“ˆ
AsyncCrawlerë¥¼ ì‚¬ìš©í•œ ë³¸ë¬¸ ì¶”ì¶œ ë° ìºì‹œ ì €ì¥
"""
import os
import sys
import time
import asyncio

# Path setup - must be done before imports
CORE_DIR = os.path.dirname(os.path.abspath(__file__))
CRAWLER_DIR = os.path.dirname(CORE_DIR)
ZND_ROOT = os.path.dirname(CRAWLER_DIR)
DESK_DIR = os.path.join(ZND_ROOT, 'desk')

# Add to sys.path
if CRAWLER_DIR not in sys.path:
    sys.path.insert(0, CRAWLER_DIR)
if DESK_DIR not in sys.path:
    sys.path.insert(0, DESK_DIR)

from core.logger import log_crawl_event
from core.collector import collect_links
from src.db_client import DBClient
from src.crawler.core import AsyncCrawler
from src.core_logic import (
    load_from_cache as _core_load_from_cache,
    save_to_cache as _core_save_to_cache
)


def load_from_cache(url):
    return _core_load_from_cache(url)

def save_to_cache(url, content):
    return _core_save_to_cache(url, content)


def extract_content(links: list = None) -> dict:
    """
    ìˆ˜ì§‘ëœ ë§í¬ì—ì„œ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    linksê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ collect_links() í˜¸ì¶œ
    
    Returns:
        dict: {success: bool, extracted: int, skipped: int, failed: int}
    """
    start_time = time.time()
    db = DBClient()  # íˆìŠ¤í† ë¦¬ ì €ì¥ìš©
    
    # ë§í¬ê°€ ì—†ìœ¼ë©´ ìë™ ìˆ˜ì§‘
    if not links:
        result = collect_links()
        if not result['success']:
            return result
        links = result['links']
    
    extracted_count = 0
    skipped_count = 0
    failed_count = 0
    
    async def extract_all():
        nonlocal extracted_count, skipped_count, failed_count
        crawler = AsyncCrawler(use_playwright=True)
        try:
            await crawler.start()
            for item in links:
                url = item['url'] if isinstance(item, dict) else item
                source_id = item.get('source_id', 'unknown') if isinstance(item, dict) else 'unknown'
                
                # ìºì‹œ ì²´í¬
                cached = load_from_cache(url)
                if cached and cached.get('text'):
                    skipped_count += 1
                    continue
                
                try:
                    content = await crawler.process_url(url)
                    if content and len(content.get('text', '')) >= 200:
                        content['source_id'] = source_id
                        save_to_cache(url, content)
                        # [FIX] Save to history to prevent re-crawling
                        db.save_history(url, 'ACCEPTED', reason='crawled')
                        extracted_count += 1
                    else:
                        # ì¶”ì¶œ ì‹¤íŒ¨ - íˆìŠ¤í† ë¦¬ì— EXTRACT_FAILED ì €ì¥ (24ì‹œê°„ í›„ ì¬ì‹œë„ ê°€ëŠ¥)
                        db.save_history(url, 'EXTRACT_FAILED', reason='short_content')
                        print(f"âš ï¸ [Extract] Failed (short content): {url[:50]}...")
                        failed_count += 1
                except Exception as e:
                    # ì¶”ì¶œ ì˜ˆì™¸ - íˆìŠ¤í† ë¦¬ì— EXTRACT_FAILED ì €ì¥
                    db.save_history(url, 'EXTRACT_FAILED', reason=str(e)[:100])
                    print(f"âš ï¸ [Extract] Failed: {url[:50]}... - {e}")
                    failed_count += 1
        finally:
            await crawler.close()
    
    try:
        asyncio.run(extract_all())
        
        duration = time.time() - start_time
        msg = f"Extracted {extracted_count} (Skip:{skipped_count}, Fail:{failed_count})"
        log_crawl_event("Extract", msg, duration, success=True)
        
        print(f"ğŸ“¥ [Extract] ì¶”ì¶œ: {extracted_count}, ìŠ¤í‚µ: {skipped_count}, ì‹¤íŒ¨: {failed_count}")
        return {
            'success': True,
            'extracted': extracted_count,
            'skipped': skipped_count,
            'failed': failed_count,
            'message': msg
        }
        
    except Exception as e:
        duration = time.time() - start_time
        log_crawl_event("Extract", f"Error: {str(e)}", duration, success=False)
        print(f"âŒ [Extract] Error: {e}")
        return {'success': False, 'error': str(e)}


def run_full_pipeline(schedule_name: str = "Scheduled"):
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: Collect -> Extract -> Discord ì•Œë¦¼
    ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ í˜¸ì¶œìš©
    
    Args:
        schedule_name: ìŠ¤ì¼€ì¤„ ì´ë¦„ (ë””ìŠ¤ì½”ë“œ ì•Œë¦¼ìš©)
    """
    print("ğŸš€ [Pipeline] Starting full crawl pipeline...")
    
    # ê²°ê³¼ ìˆ˜ì§‘ìš©
    final_result = {
        'success': True,
        'collected': 0,
        'extracted': 0,
        'analyzed': 0,
        'cached': 0,
        'failed': 0,
        'message': ''
    }
    
    # 1. Collect
    collect_result = collect_links()
    final_result['collected'] = collect_result.get('total', 0)
    
    if not collect_result['success'] or collect_result['total'] == 0:
        print(f"ğŸ“­ [Pipeline] No new links to process")
        final_result['message'] = 'No new links'
        
        # ìˆ˜ì§‘ ê²°ê³¼ê°€ 0ì´ì–´ë„ ì•Œë¦¼ ì „ì†¡ (ì„ íƒì )
        try:
            from core.discord_notifier import send_crawl_notification
            send_crawl_notification(final_result, schedule_name)
        except Exception as e:
            print(f"âš ï¸ [Discord] Notification failed: {e}")
        
        return collect_result
    
    # 2. Extract
    extract_result = extract_content(collect_result['links'])
    final_result['extracted'] = extract_result.get('extracted', 0)
    final_result['cached'] = extract_result.get('extracted', 0)  # ì¶”ì¶œëœ ê²ƒ = ìºì‹œë¨
    final_result['failed'] = extract_result.get('failed', 0)
    final_result['success'] = extract_result.get('success', True)
    final_result['message'] = extract_result.get('message', '')
    
    # 3. ë””ìŠ¤ì½”ë“œ ì•Œë¦¼ ì „ì†¡
    try:
        from core.discord_notifier import send_crawl_notification
        send_crawl_notification(final_result, schedule_name)
    except Exception as e:
        print(f"âš ï¸ [Discord] Notification failed: {e}")
    
    return final_result
