import os
import json
import time
import requests
import feedparser
from bs4 import BeautifulSoup
from newspaper import Article
from datetime import datetime, timezone, timedelta
from dotenv import load_dotenv

# Import custom clients
from src.mll_client import MLLClient
from src.db_client import DBClient
from src.crawler.utils import RobotsChecker

# Import shared core logic (source of truth for all crawlers)
from src.core_logic import (
    get_url_hash,
    get_article_id,
    load_from_cache,
    save_to_cache,
    normalize_field_names,
    update_manifest,
    HistoryStatus,
    get_data_filename,
)

# Load environment variables
# Configuration
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ENV_PATH = os.path.join(BASE_DIR, '.env')

# Load environment variables
load_dotenv(dotenv_path=ENV_PATH)

print(f"DEBUG: Loaded MLL_API_URL: {os.getenv('MLL_API_URL')}")

# í¬ë¡¤ë§ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
TARGETS_FILE = os.path.join(BASE_DIR, os.getenv('TARGETS_FILE', 'config/targets.json'))
CRAWL_LIMIT = int(os.getenv('CRAWL_LIMIT', 999))  # ê¸€ë¡œë²Œ í¬ë¡¤ë§ ìˆ˜ëŸ‰ ì œí•œ

def load_targets():
    """
    Loads settings and targets from JSON.
    Returns: (settings_dict, targets_list) tuple
    """
    with open(TARGETS_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ìƒˆ êµ¬ì¡°: {settings: {...}, targets: [...]} ë˜ëŠ” ë ˆê±°ì‹œ: [...]
    if isinstance(data, dict):
        return data.get('settings', {}), data.get('targets', [])
    else:
        return {}, data  # ë ˆê±°ì‹œ ë°°ì—´ í˜•ì‹

def is_recent(date_obj, max_age_hours=48):
    """Checks if a date is within the specified hours."""
    if not date_obj:
        return True # Keep if no date found (benefit of the doubt) but log warning? Actually standard usually is keep.
    
    now = datetime.now(timezone.utc)
    
    # Handle struct_time (from feedparser)
    if isinstance(date_obj, time.struct_time):
        dt = datetime.fromtimestamp(time.mktime(date_obj), tz=timezone.utc)
    elif isinstance(date_obj, datetime):
        if date_obj.tzinfo is None:
            date_obj = date_obj.replace(tzinfo=timezone.utc)
        dt = date_obj
    else:
        return True # Unknown format, keep it
        
    delta = now - dt
    return delta <= timedelta(hours=max_age_hours)

def fetch_links(target, max_age_hours=48):
    """Fetches links based on target type (rss or html)."""
    links = []
    print(f"Fetching from {target['id']}...")
    
    try:
        if target['type'] == 'rss':
            feed = feedparser.parse(target['url'])
            for entry in feed.entries:
                # Check date for RSS
                date_parsed = entry.get('published_parsed') or entry.get('updated_parsed')
                if date_parsed:
                     if is_recent(date_parsed, max_age_hours):
                        links.append(entry.link)
                     else:
                        # Optional: Print skipped? Might be too noisy for RSS
                        pass
                else:
                    links.append(entry.link)
        elif target['type'] == 'html':
            response = requests.get(target['url'])
            soup = BeautifulSoup(response.text, 'html.parser')
            elements = soup.select(target['selector'])
            for el in elements:
                link = el.get('href')
                if link:
                    # Handle relative URLs if necessary
                    if link.startswith('/'):
                        base_url = '/'.join(target['url'].split('/')[:3])
                        link = base_url + link
                    links.append(link)
    except Exception as e:
        print(f"Error fetching {target['id']}: {e}")
        
    return list(dict.fromkeys(links)) # Deduplicate preserving order

def extract_content(url):
    """Extracts article content using newspaper3k."""
    try:
        article = Article(url)
        article.download()
        article.parse()
        return {'text': article.text, 'title': article.title}
    except Exception as e:
        print(f"Error extracting {url}: {e}")
        return None

import asyncio
from src.crawler.core import AsyncCrawler

def run_crawler():
    asyncio.run(main())

async def main():
    """
    Auto Crawler - Orchestrates the unified pipeline.
    Only difference from manual: loops through targets automatically.
    """
    from src.pipeline import process_article, get_db
    from src.core_logic import get_config
    
    # 1. í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    mll = MLLClient()
    db = get_db()
    
    settings, targets = load_targets()
    max_consecutive_duplicates = settings.get('max_consecutive_duplicates', 3)
    max_article_age_hours = settings.get('max_article_age_hours', 48)
    
    for target in targets:
        print(f"\nğŸ¯ [Target] Processing target: {target['id']}")
        
        # 1. Fetch Links (with age filter)
        article_links = fetch_links(target, max_article_age_hours)
        
        # Apply limit (íƒ€ê²Ÿ ì„¤ì •ê³¼ ENV ì¤‘ ë‚®ì€ ê°’ ì ìš©)
        target_limit = target.get('limit', 5)
        effective_limit = min(target_limit, CRAWL_LIMIT)
        
        # [NEW] í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ: limitê¹Œì§€ëŠ” ì•ˆì „, ì´í›„ëŠ” ì—°ì† ì¤‘ë³µ ê°ì§€
        # max_consecutive_duplicatesëŠ” targets.jsonì˜ settingsì—ì„œ ë¡œë“œë¨
        
        print(f"ğŸ”— [Links] Found {len(article_links)} links (Limit: {effective_limit}, Extended mode after limit).")
        
        # 2. Filter duplicates with hybrid strategy
        new_links = []
        consecutive_duplicates = 0
        processed_count = 0
        
        for link in article_links:
            processed_count += 1
            
            if db.check_history(link):
                # Limit ì´ë‚´: ìŠ¤í‚µë§Œ í•˜ê³  ê³„ì† ì§„í–‰
                if processed_count <= effective_limit:
                    print(f"â­ï¸ [Skip] Duplicate (within limit): {link[:50]}...")
                    consecutive_duplicates = 0  # limit ì´ë‚´ì—ì„œëŠ” ë¦¬ì…‹
                else:
                    # Limit ì´ˆê³¼: ì—°ì† ì¤‘ë³µ ì¹´ìš´íŠ¸
                    consecutive_duplicates += 1
                    print(f"â­ï¸ [Skip] Duplicate ({consecutive_duplicates}/{max_consecutive_duplicates}): {link[:50]}...")
                    
                    if consecutive_duplicates >= max_consecutive_duplicates:
                        print(f"ğŸ›‘ [Stop] {target['id']}: ì—°ì† {max_consecutive_duplicates}íšŒ ì¤‘ë³µ ê°ì§€ - í•´ë‹¹ ì†ŒìŠ¤ ì •ì§€")
                        break
            else:
                consecutive_duplicates = 0  # ìƒˆ ê¸°ì‚¬ ë°œê²¬ ì‹œ ë¦¬ì…‹
                new_links.append(link)
        
        if not new_links:
            print("âœ¨ No new links to process.")
            continue

        # 3. Process each article using unified pipeline
        print(f"ğŸš€ [Process] Processing {len(new_links)} new links...")
        
        for url in new_links:
            try:
                result = await process_article(
                    url=url,
                    source_id=target['id'],
                    mll_client=mll,
                    skip_mll=False  # Auto mode uses MLL
                )
                
                status = result.get('status', 'unknown')
                if status == 'saved':
                    print(f"âœ… [Success] Saved: {result.get('article_id')}")
                elif status == 'worthless':
                    print(f"ğŸš« [Worthless] {result.get('reason')}")
                elif status == 'mll_failed':
                    print(f"âš ï¸ [MLL Failed] {result.get('reason')}")
                elif status == 'already_processed':
                    print(f"â­ï¸ [Skip] Already: {result.get('history_status')}")
                else:
                    print(f"â“ [Unknown] {status}: {result}")
                    
            except Exception as e:
                print(f"âŒ [Error] Failed to process {url}: {e}")


if __name__ == "__main__":
    run_crawler()
