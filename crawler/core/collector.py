# -*- coding: utf-8 -*-
"""
Crawler Collector - ë§í¬ ìˆ˜ì§‘ ëª¨ë“ˆ
targets.json ê¸°ë°˜ ìƒˆ ë§í¬ ìˆ˜ì§‘
"""
import os
import sys
import json
import time

# Path setup - must be done before imports
CORE_DIR = os.path.dirname(os.path.abspath(__file__))
CRAWLER_DIR = os.path.dirname(CORE_DIR)
ZND_ROOT = os.path.dirname(CRAWLER_DIR)
DESK_DIR = os.path.join(ZND_ROOT, 'desk')
DESK_SRC_CORE_DIR = os.path.join(DESK_DIR, 'src', 'core')  # firestore_client.py ìœ„ì¹˜

# Add to sys.path
if CRAWLER_DIR not in sys.path:
    sys.path.insert(0, CRAWLER_DIR)
if DESK_DIR not in sys.path:
    sys.path.insert(0, DESK_DIR)
if DESK_SRC_CORE_DIR not in sys.path:
    sys.path.insert(0, DESK_SRC_CORE_DIR)  # firestore_client ì„í¬íŠ¸ìš©

from core.logger import log_crawl_event
from firestore_client import FirestoreClient

# Import dependencies directly
import requests
import feedparser
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from dateutil import parser as date_parser

def normalize_url(url: str) -> str:
    """
    URL ì •ê·œí™”: UTM íŒŒë¼ë¯¸í„°ë§Œ ì œê±°, í•„ìˆ˜ íŒŒë¼ë¯¸í„°ëŠ” ìœ ì§€
    """
    if not url:
        return ""

    # 1. ê³µë°± ì œê±°
    url = url.strip()

    # 2. UTM/ì¶”ì  íŒŒë¼ë¯¸í„°ë§Œ ì„ íƒì ìœ¼ë¡œ ì œê±° (í•„ìˆ˜ íŒŒë¼ë¯¸í„°ëŠ” ìœ ì§€)
    if '?' in url:
        from urllib.parse import urlparse, parse_qs, urlencode
        parsed = urlparse(url)
        params = parse_qs(parsed.query)

        # ì œê±°í•  ì¶”ì  íŒŒë¼ë¯¸í„° ëª©ë¡
        tracking_params = {'utm_source', 'utm_medium', 'utm_campaign', 'utm_term', 'utm_content',
                          'fbclid', 'gclid', 'ref', 'source', 'mc_cid', 'mc_eid'}

        # ì¶”ì  íŒŒë¼ë¯¸í„°ë§Œ ì œê±°, ë‚˜ë¨¸ì§€ëŠ” ìœ ì§€
        filtered_params = {k: v[0] for k, v in params.items() if k.lower() not in tracking_params}

        if filtered_params:
            new_query = urlencode(filtered_params)
            url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}?{new_query}"
        else:
            url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"

    # 3. ë ìŠ¬ë˜ì‹œ ì œê±° (ì„ íƒì )
    if url.endswith('/'):
        url = url[:-1]

    return url

def load_targets():
    """íƒ€ê²Ÿ ì„¤ì • ë¡œë“œ (desk/config/targets.json)"""
    config_path = os.path.join(DESK_DIR, 'config', 'targets.json')
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('settings', {}), data.get('targets', [])
    except Exception as e:
        print(f"âŒ [Collector] Failed to load targets: {e}")
        return {}, []

def is_recent(pub_date, days=2):
    """ìµœì‹  ê¸°ì‚¬ ì—¬ë¶€ í™•ì¸"""
    if not pub_date:
        return True # ë‚ ì§œ ì—†ìœ¼ë©´ ì¼ë‹¨ ìˆ˜ì§‘
    
    try:
        if isinstance(pub_date, str):
            dt = date_parser.parse(pub_date)
        else:
            dt = pub_date
            
        # Timezone unaware -> aware (assuming KST if local)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=datetime.now().astimezone().tzinfo)
            
        now = datetime.now(dt.tzinfo)
        diff = now - dt
        return diff.days <= days
    except:
        return True # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ ìˆ˜ì§‘

def fetch_links(target):
    """ë§í¬ ìˆ˜ì§‘ (RSS/HTML) - URL ì •ê·œí™” ì ìš©"""
    t_type = target.get('type', 'rss')
    url = target.get('url')
    links = []
    
    if not url:
        return []
        
    try:
        if t_type == 'rss':
            feed = feedparser.parse(url)
            for entry in feed.entries:
                link = entry.get('link')
                if not link: continue
                
                # ë‚ ì§œ í•„í„°ë§ (ì˜µì…˜)
                pub_date = entry.get('published') or entry.get('updated')
                if not is_recent(pub_date):
                    continue
                    
                norm_link = normalize_url(link)
                if norm_link:
                    links.append(norm_link)
                    
        elif t_type == 'html':
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            resp = requests.get(url, headers=headers, timeout=10)
            resp.encoding = resp.apparent_encoding
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            selector = target.get('selector', 'a')
            
            for tag in soup.select(selector):
                href = tag.get('href')
                if not href: continue
                
                # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
                if href.startswith('/'):
                    from urllib.parse import urljoin
                    href = urljoin(url, href)
                elif not href.startswith('http'):
                    continue
                    
                norm_link = normalize_url(href)
                if norm_link:
                    links.append(norm_link)
                    
    except Exception as e:
        print(f"âš ï¸ [Fetch] Error fetching {url}: {e}")
        
    return list(set(links)) # ì¤‘ë³µ ì œê±°

def collect_links(progress_callback=None) -> dict:
    """
    ëª¨ë“  í™œì„± íƒ€ê²Ÿì—ì„œ ìƒˆ ë§í¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    Returns:
        dict: {success: bool, links: list, message: str}
    """
    start_time = time.time()
    db = FirestoreClient()
    
    try:
        # Desk config/targets.json ë¡œë“œ
        settings, targets = load_targets()
        time_condition = settings.get('hours', 24)
        all_links = []
        
        # ìºì‹œ ì²´í¬ìš© í•¨ìˆ˜
        from src.core_logic import load_from_cache
        
        total_found = 0
        total_added = 0
        total_skipped = 0
        
        for idx, target in enumerate(targets):
            target_id = target.get('id')
            target_name = target.get('name', target_id) 
            
            if progress_callback:
                progress_callback({
                    'status': 'collecting',
                    'message': f"ğŸ” [{idx+1}/{len(targets)}] '{target_name}' ê²€ìƒ‰ ì¤‘..."
                })
            
            time.sleep(0.3)

            print(f"ğŸ“¡ [Collect] Fetching from target: {target_id} ({target.get('url')})")
            
            # [Refactored] Use internal fetch_links
            links = fetch_links(target)
            
            found_count = len(links)
            total_found += found_count
            print(f"   found {found_count} raw links")
            
            limit = target.get('limit', 5)
            links = links[:limit]
            
            skipped_history = 0
            skipped_cache = 0
            added_count = 0
            
            for link in links:
                # 1. íˆìŠ¤í† ë¦¬ ì²´í¬ (ì´ë¯¸ ì²˜ë¦¬ëœ ê²ƒ ì œì™¸: ACCEPTED, REJECTED ë“±)
                is_in_history = db.check_history(link)
                if is_in_history:
                    skipped_history += 1
                    continue
                
                # 2. ìºì‹œ ì²´í¬
                cached = load_from_cache(link)
                if cached and cached.get('text'):
                    skipped_cache += 1
                    continue
                
                print(f"   âœ… [New] Adding link: {link}")
                all_links.append({
                    'url': link,
                    'source_id': target['id'],
                    'target_name': target.get('name', target['id'])
                })
                added_count += 1
            
            total_added += added_count
            total_skipped += skipped_history + skipped_cache
            
            print(f"   â­ï¸ [{target['id']}] Result: Added={added_count}, SkipHistory={skipped_history}, SkipCache={skipped_cache}")
            
            if progress_callback:
                progress_callback({
                    'status': 'collecting',
                    'message': f"âœ… [{idx+1}/{len(targets)}] '{target_name}': {found_count}ê°œ ë°œê²¬ â†’ {added_count}ê°œ ì‹ ê·œ"
                })
        
        # ì¤‘ë³µ ì œê±° (ì „ì²´)
        seen = set()
        unique_links = []
        for item in all_links:
            if item['url'] not in seen:
                seen.add(item['url'])
                unique_links.append(item)
        
        duration = time.time() - start_time
        msg = f"Collected {len(unique_links)} new links"
        log_crawl_event("Collect", msg, duration, success=True)
        
        print(f"ğŸ“¡ [Collect] ìˆ˜ì§‘ ì™„ë£Œ: {len(unique_links)} ìƒˆ ë§í¬")
        
        if progress_callback:
            progress_callback({
                'status': 'collecting',
                'message': f"ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ: {len(unique_links)}ê°œ ì‹ ê·œ í™•ë³´"
            })
        
        return {
            'success': True,
            'links': unique_links,
            'total': len(unique_links),
            'total_found': total_found,
            'total_skipped': total_skipped,
            'message': msg
        }
        
    except Exception as e:
        duration = time.time() - start_time
        log_crawl_event("Collect", f"Error: {str(e)}", duration, success=False)
        print(f"âŒ [Collect] Error: {e}")
        return {'success': False, 'error': str(e), 'links': []}
